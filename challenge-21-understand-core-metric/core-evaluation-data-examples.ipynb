{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0161594c-f4d4-4a1d-b506-a976684e92ee",
   "metadata": {},
   "source": [
    "## CORE evaluation data examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3ded23-37a2-4569-8e0e-f8bc54405e5d",
   "metadata": {},
   "source": [
    "I realized after creating `challenge-26-understand-midtrain/midtrain-data-examples.ipynb` and doing midtraining that I was forgetting / getting confused about the CORE evaluation data. This notebook with some ugly code shows a few examples for each task type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "93424fbe-b130-4d7c-b2f7-e51f62bb3bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= hellaswag_zeroshot =============\n",
      "\n",
      "This is multiple choice so each item will be scored correct if the choice with the highest probabiliy\n",
      "matches the correct choice. To get into the mechanics a bit more, it's really only the probabilities \n",
      "of the \"choice part\" that are looked at for each of the n \"prompts\". The \"choice part\" is what comes after\n",
      "the query. The query is repeated in each prompt, forming a common prefix. Think about it as which choice\n",
      "has the highest probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Query: Roof shingle removal: A man is sitting on a roof. He\n",
      "Correct prompt: 3\n",
      "\n",
      "prompt 0: Roof shingle removal: A man is sitting on a roof. He is using wrap to wrap a pair of skis.\n",
      "\n",
      "prompt 1: Roof shingle removal: A man is sitting on a roof. He is ripping level tiles off.\n",
      "\n",
      "prompt 2: Roof shingle removal: A man is sitting on a roof. He is holding a rubik's cube.\n",
      "\n",
      "prompt 3: Roof shingle removal: A man is sitting on a roof. He starts pulling up roofing on a roof.\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Query: Clean and jerk: A lady walks to a barbell. She bends down and grabs the pole. The lady\n",
      "Correct prompt: 3\n",
      "\n",
      "prompt 0: Clean and jerk: A lady walks to a barbell. She bends down and grabs the pole. The lady swings and lands in her arms.\n",
      "\n",
      "prompt 1: Clean and jerk: A lady walks to a barbell. She bends down and grabs the pole. The lady pulls the barbell forward.\n",
      "\n",
      "prompt 2: Clean and jerk: A lady walks to a barbell. She bends down and grabs the pole. The lady pulls a rope attached to the barbell.\n",
      "\n",
      "prompt 3: Clean and jerk: A lady walks to a barbell. She bends down and grabs the pole. The lady stands and lifts the weight over her head.\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Query: Canoeing: Two women in a child are shown in a canoe while a man pulls the canoe while standing in the water, with other individuals visible in the background. The child and a different man\n",
      "Correct prompt: 2\n",
      "\n",
      "prompt 0: Canoeing: Two women in a child are shown in a canoe while a man pulls the canoe while standing in the water, with other individuals visible in the background. The child and a different man are then shown paddling down a river in a boat while a woman talks.\n",
      "\n",
      "prompt 1: Canoeing: Two women in a child are shown in a canoe while a man pulls the canoe while standing in the water, with other individuals visible in the background. The child and a different man are driving the canoe, they go down the river flowing side to side.\n",
      "\n",
      "prompt 2: Canoeing: Two women in a child are shown in a canoe while a man pulls the canoe while standing in the water, with other individuals visible in the background. The child and a different man sit in a canoe while the man paddles.\n",
      "\n",
      "prompt 3: Canoeing: Two women in a child are shown in a canoe while a man pulls the canoe while standing in the water, with other individuals visible in the background. The child and a different man walking go down the rapids, while the man in his helicopter almost falls and goes out of canoehood.\n",
      "\n",
      "============= jeopardy =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored correct if the model generates\n",
      "the expected continuation.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Expected continuation: Admiral Richard Byrd\n",
      "\n",
      "prompt 0: WORLD HISTORY: This Navy commander flew from a base at Little America to the South Pole & back Nov. 28-29, 1929\n",
      "Answer:\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Expected continuation: Tower of London\n",
      "\n",
      "prompt 0: WORLD HISTORY: Accused of accepting bribes, Francis Bacon was imprisoned in this forbidding complex in 1621\n",
      "Answer:\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Expected continuation: Algeria\n",
      "\n",
      "prompt 0: WORLD HISTORY: More than 250,000 died in fighting before France granted this African nation independence July 3, 1962\n",
      "Answer:\n",
      "\n",
      "============= bigbench_qa_wikidata =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored correct if the model generates\n",
      "the expected continuation.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Expected continuation: French\n",
      "\n",
      "prompt 0: The native language of Daniel Schneidermann is\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Expected continuation: Russia\n",
      "\n",
      "prompt 0: The country of Novosibirsk is\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Expected continuation: Argentina\n",
      "\n",
      "prompt 0: The country of citizenship of Isabel Martínez de Perón is\n",
      "\n",
      "============= arc_easy =============\n",
      "\n",
      "This is multiple choice so each item will be scored correct if the choice with the highest probabiliy\n",
      "matches the correct choice. To get into the mechanics a bit more, it's really only the probabilities \n",
      "of the \"choice part\" that are looked at for each of the n \"prompts\". The \"choice part\" is what comes after\n",
      "the query. The query is repeated in each prompt, forming a common prefix. Think about it as which choice\n",
      "has the highest probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Query: Question: Which statement best explains why photosynthesis is the foundation of most food webs?\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: Question: Which statement best explains why photosynthesis is the foundation of most food webs?\n",
      "Answer: Sunlight is the source of energy for nearly all ecosystems.\n",
      "\n",
      "prompt 1: Question: Which statement best explains why photosynthesis is the foundation of most food webs?\n",
      "Answer: Most ecosystems are found on land instead of in water.\n",
      "\n",
      "prompt 2: Question: Which statement best explains why photosynthesis is the foundation of most food webs?\n",
      "Answer: Carbon dioxide is more available than other gases.\n",
      "\n",
      "prompt 3: Question: Which statement best explains why photosynthesis is the foundation of most food webs?\n",
      "Answer: The producers in all ecosystems are plants.\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Query: Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\n",
      "Answer: safety goggles\n",
      "\n",
      "prompt 1: Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\n",
      "Answer: breathing mask\n",
      "\n",
      "prompt 2: Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\n",
      "Answer: rubber gloves\n",
      "\n",
      "prompt 3: Question: Which piece of safety equipment is used to keep mold spores from entering the respiratory system?\n",
      "Answer: lead apron\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Query: Question: Meiosis is a type of cell division in which germ cells divide to produce haploid cells. Where does meiosis occur?\n",
      "Correct prompt: 3\n",
      "\n",
      "prompt 0: Question: Meiosis is a type of cell division in which germ cells divide to produce haploid cells. Where does meiosis occur?\n",
      "Answer: brain cells\n",
      "\n",
      "prompt 1: Question: Meiosis is a type of cell division in which germ cells divide to produce haploid cells. Where does meiosis occur?\n",
      "Answer: bone cells\n",
      "\n",
      "prompt 2: Question: Meiosis is a type of cell division in which germ cells divide to produce haploid cells. Where does meiosis occur?\n",
      "Answer: muscle cells\n",
      "\n",
      "prompt 3: Question: Meiosis is a type of cell division in which germ cells divide to produce haploid cells. Where does meiosis occur?\n",
      "Answer: ovary cells\n",
      "\n",
      "============= arc_challenge =============\n",
      "\n",
      "This is multiple choice so each item will be scored correct if the choice with the highest probabiliy\n",
      "matches the correct choice. To get into the mechanics a bit more, it's really only the probabilities \n",
      "of the \"choice part\" that are looked at for each of the n \"prompts\". The \"choice part\" is what comes after\n",
      "the query. The query is repeated in each prompt, forming a common prefix. Think about it as which choice\n",
      "has the highest probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Query: Question: An astronomer observes that a planet rotates faster after a meteorite impact. Which is the most likely effect of this increase in rotation?\n",
      "Correct prompt: 2\n",
      "\n",
      "prompt 0: Question: An astronomer observes that a planet rotates faster after a meteorite impact. Which is the most likely effect of this increase in rotation?\n",
      "Answer: Planetary density will decrease.\n",
      "\n",
      "prompt 1: Question: An astronomer observes that a planet rotates faster after a meteorite impact. Which is the most likely effect of this increase in rotation?\n",
      "Answer: Planetary years will become longer.\n",
      "\n",
      "prompt 2: Question: An astronomer observes that a planet rotates faster after a meteorite impact. Which is the most likely effect of this increase in rotation?\n",
      "Answer: Planetary days will become shorter.\n",
      "\n",
      "prompt 3: Question: An astronomer observes that a planet rotates faster after a meteorite impact. Which is the most likely effect of this increase in rotation?\n",
      "Answer: Planetary gravity will become stronger.\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Query: Question: A group of engineers wanted to know how different building designs would respond during an earthquake. They made several models of buildings and tested each for its ability to withstand earthquake conditions. Which will most likely result from testing different building designs?\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: Question: A group of engineers wanted to know how different building designs would respond during an earthquake. They made several models of buildings and tested each for its ability to withstand earthquake conditions. Which will most likely result from testing different building designs?\n",
      "Answer: buildings will be built faster\n",
      "\n",
      "prompt 1: Question: A group of engineers wanted to know how different building designs would respond during an earthquake. They made several models of buildings and tested each for its ability to withstand earthquake conditions. Which will most likely result from testing different building designs?\n",
      "Answer: buildings will be made safer\n",
      "\n",
      "prompt 2: Question: A group of engineers wanted to know how different building designs would respond during an earthquake. They made several models of buildings and tested each for its ability to withstand earthquake conditions. Which will most likely result from testing different building designs?\n",
      "Answer: building designs will look nicer\n",
      "\n",
      "prompt 3: Question: A group of engineers wanted to know how different building designs would respond during an earthquake. They made several models of buildings and tested each for its ability to withstand earthquake conditions. Which will most likely result from testing different building designs?\n",
      "Answer: building materials will be cheaper\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Query: Question: The end result in the process of photosynthesis is the production of sugar and oxygen. Which step signals the beginning of photosynthesis?\n",
      "Correct prompt: 2\n",
      "\n",
      "prompt 0: Question: The end result in the process of photosynthesis is the production of sugar and oxygen. Which step signals the beginning of photosynthesis?\n",
      "Answer: Chemical energy is absorbed through the roots.\n",
      "\n",
      "prompt 1: Question: The end result in the process of photosynthesis is the production of sugar and oxygen. Which step signals the beginning of photosynthesis?\n",
      "Answer: Light energy is converted to chemical energy.\n",
      "\n",
      "prompt 2: Question: The end result in the process of photosynthesis is the production of sugar and oxygen. Which step signals the beginning of photosynthesis?\n",
      "Answer: Chlorophyll in the leaf captures light energy.\n",
      "\n",
      "prompt 3: Question: The end result in the process of photosynthesis is the production of sugar and oxygen. Which step signals the beginning of photosynthesis?\n",
      "Answer: Sunlight is converted into chlorophyll.\n",
      "\n",
      "============= copa =============\n",
      "\n",
      "This is multiple choice so each item will be scored correct if the choice with the highest probabiliy\n",
      "matches the correct choice. To get into the mechanics a bit more, it's really only the probabilities \n",
      "of the \"choice part\" that are looked at for each of the n \"prompts\". The \"choice part\" is what comes after\n",
      "the query. The query is repeated in each prompt, forming a common prefix. Think about it as which choice\n",
      "has the highest probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Query: The man turned on the faucet, therefore\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: The man turned on the faucet, therefore the toilet filled with water.\n",
      "\n",
      "prompt 1: The man turned on the faucet, therefore water flowed from the spout.\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Query: The girl found a bug in her cereal, therefore\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: The girl found a bug in her cereal, therefore she poured milk in the bowl.\n",
      "\n",
      "prompt 1: The girl found a bug in her cereal, therefore she lost her appetite.\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Query: The woman retired, therefore\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: The woman retired, therefore she received her pension.\n",
      "\n",
      "prompt 1: The woman retired, therefore she paid off her mortgage.\n",
      "\n",
      "============= commonsense_qa =============\n",
      "\n",
      "This is multiple choice so each item will be scored correct if the choice with the highest probabiliy\n",
      "matches the correct choice. To get into the mechanics a bit more, it's really only the probabilities \n",
      "of the \"choice part\" that are looked at for each of the n \"prompts\". The \"choice part\" is what comes after\n",
      "the query. The query is repeated in each prompt, forming a common prefix. Think about it as which choice\n",
      "has the highest probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Query: Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\n",
      "Choices:\n",
      "A. bank\n",
      "B. department store\n",
      "C. mall\n",
      "D. new york\n",
      "Answer:\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\n",
      "Choices:\n",
      "A. bank\n",
      "B. department store\n",
      "C. mall\n",
      "D. new york\n",
      "Answer: A\n",
      "\n",
      "prompt 1: Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\n",
      "Choices:\n",
      "A. bank\n",
      "B. department store\n",
      "C. mall\n",
      "D. new york\n",
      "Answer: B\n",
      "\n",
      "prompt 2: Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\n",
      "Choices:\n",
      "A. bank\n",
      "B. department store\n",
      "C. mall\n",
      "D. new york\n",
      "Answer: C\n",
      "\n",
      "prompt 3: Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\n",
      "Choices:\n",
      "A. bank\n",
      "B. department store\n",
      "C. mall\n",
      "D. new york\n",
      "Answer: D\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Query: Question: What do people aim to do at work?\n",
      "Choices:\n",
      "A. complete job\n",
      "B. kill animals\n",
      "C. wear hats\n",
      "D. talk to each other\n",
      "Answer:\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: Question: What do people aim to do at work?\n",
      "Choices:\n",
      "A. complete job\n",
      "B. kill animals\n",
      "C. wear hats\n",
      "D. talk to each other\n",
      "Answer: A\n",
      "\n",
      "prompt 1: Question: What do people aim to do at work?\n",
      "Choices:\n",
      "A. complete job\n",
      "B. kill animals\n",
      "C. wear hats\n",
      "D. talk to each other\n",
      "Answer: B\n",
      "\n",
      "prompt 2: Question: What do people aim to do at work?\n",
      "Choices:\n",
      "A. complete job\n",
      "B. kill animals\n",
      "C. wear hats\n",
      "D. talk to each other\n",
      "Answer: C\n",
      "\n",
      "prompt 3: Question: What do people aim to do at work?\n",
      "Choices:\n",
      "A. complete job\n",
      "B. kill animals\n",
      "C. wear hats\n",
      "D. talk to each other\n",
      "Answer: D\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Query: Question: Where would you find magazines along side many other printed works?\n",
      "Choices:\n",
      "A. doctor\n",
      "B. bookstore\n",
      "C. train station\n",
      "D. mortuary\n",
      "Answer:\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: Question: Where would you find magazines along side many other printed works?\n",
      "Choices:\n",
      "A. doctor\n",
      "B. bookstore\n",
      "C. train station\n",
      "D. mortuary\n",
      "Answer: A\n",
      "\n",
      "prompt 1: Question: Where would you find magazines along side many other printed works?\n",
      "Choices:\n",
      "A. doctor\n",
      "B. bookstore\n",
      "C. train station\n",
      "D. mortuary\n",
      "Answer: B\n",
      "\n",
      "prompt 2: Question: Where would you find magazines along side many other printed works?\n",
      "Choices:\n",
      "A. doctor\n",
      "B. bookstore\n",
      "C. train station\n",
      "D. mortuary\n",
      "Answer: C\n",
      "\n",
      "prompt 3: Question: Where would you find magazines along side many other printed works?\n",
      "Choices:\n",
      "A. doctor\n",
      "B. bookstore\n",
      "C. train station\n",
      "D. mortuary\n",
      "Answer: D\n",
      "\n",
      "============= piqa =============\n",
      "\n",
      "This is multiple choice so each item will be scored correct if the choice with the highest probabiliy\n",
      "matches the correct choice. To get into the mechanics a bit more, it's really only the probabilities \n",
      "of the \"choice part\" that are looked at for each of the n \"prompts\". The \"choice part\" is what comes after\n",
      "the query. The query is repeated in each prompt, forming a common prefix. Think about it as which choice\n",
      "has the highest probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Query: Question: How do I ready a guinea pig cage for it's new occupants?\n",
      "\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: Question: How do I ready a guinea pig cage for it's new occupants?\n",
      "\n",
      "Answer: Provide the guinea pig with a cage full of a few inches of bedding made of ripped paper strips, you will also need to supply it with a water bottle and a food dish.\n",
      "\n",
      "prompt 1: Question: How do I ready a guinea pig cage for it's new occupants?\n",
      "\n",
      "Answer: Provide the guinea pig with a cage full of a few inches of bedding made of ripped jeans material, you will also need to supply it with a water bottle and a food dish.\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Query: Question: dresser\n",
      "\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: Question: dresser\n",
      "\n",
      "Answer: replace drawer with bobby pin\n",
      "\n",
      "prompt 1: Question: dresser\n",
      "\n",
      "Answer: finish, woodgrain with  bobby pin\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Query: Question: To fight Ivan Drago in Rocky for sega master system.\n",
      "\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: Question: To fight Ivan Drago in Rocky for sega master system.\n",
      "\n",
      "Answer: Drago isn't in this game because it was released before Rocky IV.\n",
      "\n",
      "prompt 1: Question: To fight Ivan Drago in Rocky for sega master system.\n",
      "\n",
      "Answer: You have to defeat Apollo Creed and Clubber Lang first.\n",
      "\n",
      "============= openbook_qa =============\n",
      "\n",
      "This is multiple choice so each item will be scored correct if the choice with the highest probabiliy\n",
      "matches the correct choice. To get into the mechanics a bit more, it's really only the probabilities \n",
      "of the \"choice part\" that are looked at for each of the n \"prompts\". The \"choice part\" is what comes after\n",
      "the query. The query is repeated in each prompt, forming a common prefix. Think about it as which choice\n",
      "has the highest probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Query: A person wants to start saving money so that they can afford a nice vacation at the end of the year. After looking over their budget and expenses, they decide the best way to save money is to\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: A person wants to start saving money so that they can afford a nice vacation at the end of the year. After looking over their budget and expenses, they decide the best way to save money is to make more phone calls\n",
      "\n",
      "prompt 1: A person wants to start saving money so that they can afford a nice vacation at the end of the year. After looking over their budget and expenses, they decide the best way to save money is to quit eating lunch out\n",
      "\n",
      "prompt 2: A person wants to start saving money so that they can afford a nice vacation at the end of the year. After looking over their budget and expenses, they decide the best way to save money is to buy less with monopoly money\n",
      "\n",
      "prompt 3: A person wants to start saving money so that they can afford a nice vacation at the end of the year. After looking over their budget and expenses, they decide the best way to save money is to have lunch with friends\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Query: There is most likely going to be fog around:\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: There is most likely going to be fog around: a marsh\n",
      "\n",
      "prompt 1: There is most likely going to be fog around: a tundra\n",
      "\n",
      "prompt 2: There is most likely going to be fog around: the plains\n",
      "\n",
      "prompt 3: There is most likely going to be fog around: a desert\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Query: Predators eat\n",
      "Correct prompt: 2\n",
      "\n",
      "prompt 0: Predators eat lions\n",
      "\n",
      "prompt 1: Predators eat humans\n",
      "\n",
      "prompt 2: Predators eat bunnies\n",
      "\n",
      "prompt 3: Predators eat grass\n",
      "\n",
      "============= lambada_openai =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored correct if the model generates\n",
      "the expected continuation.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Expected continuation: signs\n",
      "\n",
      "prompt 0: In my palm is a clear stone, and inside it is a small ivory statuette. A guardian angel.\n",
      "\n",
      "\"Figured if you're going to be out at night getting hit by cars, you might as well have some backup.\"\n",
      "\n",
      "I look at him, feeling stunned. Like this is some sort of sign. But as I stare at Harlin, his mouth curved in a confident grin, I don't care about\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Expected continuation: Shane\n",
      "\n",
      "prompt 0: Give me a minute to change and I'll meet you at the docks.\" She'd forced those words through her teeth.\n",
      "\n",
      "\"No need to change. We won't be that long.\"\n",
      "\n",
      "Shane gripped her arm and started leading her to the dock.\n",
      "\n",
      "\"I can make it there on my own,\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Expected continuation: insurance\n",
      "\n",
      "prompt 0: \"Only one source I know of that would be likely to cough up enough money to finance a phony sleep research facility and pay people big bucks to solve crimes in their dreams,\" Farrell concluded dryly.\n",
      "\n",
      "\"What can I say?\" Ellis unfolded his arms and widened his hands. \"Your tax dollars at work.\"\n",
      "\n",
      "Before Farrell could respond, Leila's voice rose from inside the house.\n",
      "\n",
      "\"No insurance?\" she wailed. \"What do you mean you don't have any\n",
      "\n",
      "============= hellaswag =============\n",
      "\n",
      "This is multiple choice so each item will be scored correct if the choice with the highest probabiliy\n",
      "matches the correct choice. To get into the mechanics a bit more, it's really only the probabilities \n",
      "of the \"choice part\" that are looked at for each of the n \"prompts\". The \"choice part\" is what comes after\n",
      "the query. The query is repeated in each prompt, forming a common prefix. Think about it as which choice\n",
      "has the highest probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Query: Roof shingle removal: A man is sitting on a roof. He\n",
      "Correct prompt: 3\n",
      "\n",
      "prompt 0: Roof shingle removal: A man is sitting on a roof. He is using wrap to wrap a pair of skis.\n",
      "\n",
      "prompt 1: Roof shingle removal: A man is sitting on a roof. He is ripping level tiles off.\n",
      "\n",
      "prompt 2: Roof shingle removal: A man is sitting on a roof. He is holding a rubik's cube.\n",
      "\n",
      "prompt 3: Roof shingle removal: A man is sitting on a roof. He starts pulling up roofing on a roof.\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Query: Clean and jerk: A lady walks to a barbell. She bends down and grabs the pole. The lady\n",
      "Correct prompt: 3\n",
      "\n",
      "prompt 0: Clean and jerk: A lady walks to a barbell. She bends down and grabs the pole. The lady swings and lands in her arms.\n",
      "\n",
      "prompt 1: Clean and jerk: A lady walks to a barbell. She bends down and grabs the pole. The lady pulls the barbell forward.\n",
      "\n",
      "prompt 2: Clean and jerk: A lady walks to a barbell. She bends down and grabs the pole. The lady pulls a rope attached to the barbell.\n",
      "\n",
      "prompt 3: Clean and jerk: A lady walks to a barbell. She bends down and grabs the pole. The lady stands and lifts the weight over her head.\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Query: Canoeing: Two women in a child are shown in a canoe while a man pulls the canoe while standing in the water, with other individuals visible in the background. The child and a different man\n",
      "Correct prompt: 2\n",
      "\n",
      "prompt 0: Canoeing: Two women in a child are shown in a canoe while a man pulls the canoe while standing in the water, with other individuals visible in the background. The child and a different man are then shown paddling down a river in a boat while a woman talks.\n",
      "\n",
      "prompt 1: Canoeing: Two women in a child are shown in a canoe while a man pulls the canoe while standing in the water, with other individuals visible in the background. The child and a different man are driving the canoe, they go down the river flowing side to side.\n",
      "\n",
      "prompt 2: Canoeing: Two women in a child are shown in a canoe while a man pulls the canoe while standing in the water, with other individuals visible in the background. The child and a different man sit in a canoe while the man paddles.\n",
      "\n",
      "prompt 3: Canoeing: Two women in a child are shown in a canoe while a man pulls the canoe while standing in the water, with other individuals visible in the background. The child and a different man walking go down the rapids, while the man in his helicopter almost falls and goes out of canoehood.\n",
      "\n",
      "============= winograd =============\n",
      "\n",
      "This is a schema task type. Each item will be scored correct if the \"continuation part\" with the\n",
      "highest probability is in the correct prompt. This is similar to multiple choice except here we\n",
      "have a common suffix (the continuation) and in multiple choice we have a common prefix (the query).\n",
      "The continuations are the same in each prompt so in isolation they would have the same probability.\n",
      "The key is they are judged in the context of the full prompt. It's also important that we look at\n",
      "the probabilities only of the continuation parts, because we're interested in which is most probable\n",
      "in the given context, not which prompt overall is more likely.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Continuation part: feared violence.\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: The city councilmen refused the demonstrators a permit because the city councilmen feared violence.\n",
      "\n",
      "prompt 1: The city councilmen refused the demonstrators a permit because the demonstrators feared violence.\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Continuation part: advocated violence.\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: The city councilmen refused the demonstrators a permit because the city councilmen advocated violence.\n",
      "\n",
      "prompt 1: The city councilmen refused the demonstrators a permit because the demonstrators advocated violence.\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Continuation part: is too large.\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: The trophy doesn't fit into the brown suitcase because the trophy is too large.\n",
      "\n",
      "prompt 1: The trophy doesn't fit into the brown suitcase because the suitcase is too large.\n",
      "\n",
      "============= winogrande =============\n",
      "\n",
      "This is a schema task type. Each item will be scored correct if the \"continuation part\" with the\n",
      "highest probability is in the correct prompt. This is similar to multiple choice except here we\n",
      "have a common suffix (the continuation) and in multiple choice we have a common prefix (the query).\n",
      "The continuations are the same in each prompt so in isolation they would have the same probability.\n",
      "The key is they are judged in the context of the full prompt. It's also important that we look at\n",
      "the probabilities only of the continuation parts, because we're interested in which is most probable\n",
      "in the given context, not which prompt overall is more likely.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Continuation part: always got the easier cases.\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: Sarah was a much better surgeon than Maria so Sarah always got the easier cases.\n",
      "\n",
      "prompt 1: Sarah was a much better surgeon than Maria so Maria always got the easier cases.\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Continuation part: always got the harder cases.\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: Sarah was a much better surgeon than Maria so Sarah always got the harder cases.\n",
      "\n",
      "prompt 1: Sarah was a much better surgeon than Maria so Maria always got the harder cases.\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Continuation part: was't ruined.\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: They were worried the wine would ruin the bed and the blanket, but the blanket was't ruined.\n",
      "\n",
      "prompt 1: They were worried the wine would ruin the bed and the blanket, but the bed was't ruined.\n",
      "\n",
      "============= bigbench_dyck_languages =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored correct if the model generates\n",
      "the expected continuation.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Expected continuation: } ]\n",
      "\n",
      "prompt 0: Complete the rest of the sequence, making sure that the parentheses are closed properly. \n",
      "\n",
      "Input: [ < < { } > [ { [ ] ( ( ( ( < ( ( ) ) > ) ) ) [ ] ) } ] { } < [ { ( { < ( ) > } ) } ( ) ] > > {\n",
      "Output:\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Expected continuation: >\n",
      "\n",
      "prompt 0: Complete the rest of the sequence, making sure that the parentheses are closed properly. \n",
      "\n",
      "Input: < < [ ( ) ] >\n",
      "Output:\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Expected continuation: )\n",
      "\n",
      "prompt 0: Complete the rest of the sequence, making sure that the parentheses are closed properly. \n",
      "\n",
      "Input: ( { < > }\n",
      "Output:\n",
      "\n",
      "============= agi_eval_lsat_ar =============\n",
      "\n",
      "This is multiple choice so each item will be scored correct if the choice with the highest probabiliy\n",
      "matches the correct choice. To get into the mechanics a bit more, it's really only the probabilities \n",
      "of the \"choice part\" that are looked at for each of the n \"prompts\". The \"choice part\" is what comes after\n",
      "the query. The query is repeated in each prompt, forming a common prefix. Think about it as which choice\n",
      "has the highest probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Query: Passage: Of the eight students—George, Helen, Irving, Kyle, Lenore, Nina, Olivia, and Robert—in a seminar, exactly six will give individual oral reports during three consecutive days—Monday, Tuesday, and Wednesday. Exactly two reports will be given each day—one in the morning and one in the afternoon—according to the following conditions: Tuesday is the only day on which George can give a report. Neither Olivia nor Robert can give an afternoon report. If Nina gives a report, then on the next day Helen and Irving must both give reports, unless Nina's report is given on Wednesday.\n",
      "Q: Which one of the following could be the schedule of the students' reports?\n",
      "Choices:\n",
      "A.) Mon. morning: Irving; Mon. afternoon: Olivia Tues. morning: Helen; Tues. afternoon: Kyle Wed. morning: Nina; Wed. afternoon: Lenore\n",
      "B.) Mon. morning: Lenore; Mon. afternoon: Helen Tues. morning: George; Tues. afternoon: Kyle Wed. morning: Robert; Wed. afternoon: Irving\n",
      "C.) Mon. morning: Helen; Mon. afternoon: Robert Tues. morning: Olivia; Tues. afternoon: Irving Wed. morning: Lenore; Wed. afternoon: Kyle\n",
      "D.) Mon. morning: Olivia; Mon. afternoon: Nina Tues. morning: Irving; Tues. afternoon: Helen Wed. morning: Kyle; Wed. afternoon: George\n",
      "Answer:\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: Passage: Of the eight students—George, Helen, Irving, Kyle, Lenore, Nina, Olivia, and Robert—in a seminar, exactly six will give individual oral reports during three consecutive days—Monday, Tuesday, and Wednesday. Exactly two reports will be given each day—one in the morning and one in the afternoon—according to the following conditions: Tuesday is the only day on which George can give a report. Neither Olivia nor Robert can give an afternoon report. If Nina gives a report, then on the next day Helen and Irving must both give reports, unless Nina's report is given on Wednesday.\n",
      "Q: Which one of the following could be the schedule of the students' reports?\n",
      "Choices:\n",
      "A.) Mon. morning: Irving; Mon. afternoon: Olivia Tues. morning: Helen; Tues. afternoon: Kyle Wed. morning: Nina; Wed. afternoon: Lenore\n",
      "B.) Mon. morning: Lenore; Mon. afternoon: Helen Tues. morning: George; Tues. afternoon: Kyle Wed. morning: Robert; Wed. afternoon: Irving\n",
      "C.) Mon. morning: Helen; Mon. afternoon: Robert Tues. morning: Olivia; Tues. afternoon: Irving Wed. morning: Lenore; Wed. afternoon: Kyle\n",
      "D.) Mon. morning: Olivia; Mon. afternoon: Nina Tues. morning: Irving; Tues. afternoon: Helen Wed. morning: Kyle; Wed. afternoon: George\n",
      "Answer: A\n",
      "\n",
      "prompt 1: Passage: Of the eight students—George, Helen, Irving, Kyle, Lenore, Nina, Olivia, and Robert—in a seminar, exactly six will give individual oral reports during three consecutive days—Monday, Tuesday, and Wednesday. Exactly two reports will be given each day—one in the morning and one in the afternoon—according to the following conditions: Tuesday is the only day on which George can give a report. Neither Olivia nor Robert can give an afternoon report. If Nina gives a report, then on the next day Helen and Irving must both give reports, unless Nina's report is given on Wednesday.\n",
      "Q: Which one of the following could be the schedule of the students' reports?\n",
      "Choices:\n",
      "A.) Mon. morning: Irving; Mon. afternoon: Olivia Tues. morning: Helen; Tues. afternoon: Kyle Wed. morning: Nina; Wed. afternoon: Lenore\n",
      "B.) Mon. morning: Lenore; Mon. afternoon: Helen Tues. morning: George; Tues. afternoon: Kyle Wed. morning: Robert; Wed. afternoon: Irving\n",
      "C.) Mon. morning: Helen; Mon. afternoon: Robert Tues. morning: Olivia; Tues. afternoon: Irving Wed. morning: Lenore; Wed. afternoon: Kyle\n",
      "D.) Mon. morning: Olivia; Mon. afternoon: Nina Tues. morning: Irving; Tues. afternoon: Helen Wed. morning: Kyle; Wed. afternoon: George\n",
      "Answer: B\n",
      "\n",
      "prompt 2: Passage: Of the eight students—George, Helen, Irving, Kyle, Lenore, Nina, Olivia, and Robert—in a seminar, exactly six will give individual oral reports during three consecutive days—Monday, Tuesday, and Wednesday. Exactly two reports will be given each day—one in the morning and one in the afternoon—according to the following conditions: Tuesday is the only day on which George can give a report. Neither Olivia nor Robert can give an afternoon report. If Nina gives a report, then on the next day Helen and Irving must both give reports, unless Nina's report is given on Wednesday.\n",
      "Q: Which one of the following could be the schedule of the students' reports?\n",
      "Choices:\n",
      "A.) Mon. morning: Irving; Mon. afternoon: Olivia Tues. morning: Helen; Tues. afternoon: Kyle Wed. morning: Nina; Wed. afternoon: Lenore\n",
      "B.) Mon. morning: Lenore; Mon. afternoon: Helen Tues. morning: George; Tues. afternoon: Kyle Wed. morning: Robert; Wed. afternoon: Irving\n",
      "C.) Mon. morning: Helen; Mon. afternoon: Robert Tues. morning: Olivia; Tues. afternoon: Irving Wed. morning: Lenore; Wed. afternoon: Kyle\n",
      "D.) Mon. morning: Olivia; Mon. afternoon: Nina Tues. morning: Irving; Tues. afternoon: Helen Wed. morning: Kyle; Wed. afternoon: George\n",
      "Answer: C\n",
      "\n",
      "prompt 3: Passage: Of the eight students—George, Helen, Irving, Kyle, Lenore, Nina, Olivia, and Robert—in a seminar, exactly six will give individual oral reports during three consecutive days—Monday, Tuesday, and Wednesday. Exactly two reports will be given each day—one in the morning and one in the afternoon—according to the following conditions: Tuesday is the only day on which George can give a report. Neither Olivia nor Robert can give an afternoon report. If Nina gives a report, then on the next day Helen and Irving must both give reports, unless Nina's report is given on Wednesday.\n",
      "Q: Which one of the following could be the schedule of the students' reports?\n",
      "Choices:\n",
      "A.) Mon. morning: Irving; Mon. afternoon: Olivia Tues. morning: Helen; Tues. afternoon: Kyle Wed. morning: Nina; Wed. afternoon: Lenore\n",
      "B.) Mon. morning: Lenore; Mon. afternoon: Helen Tues. morning: George; Tues. afternoon: Kyle Wed. morning: Robert; Wed. afternoon: Irving\n",
      "C.) Mon. morning: Helen; Mon. afternoon: Robert Tues. morning: Olivia; Tues. afternoon: Irving Wed. morning: Lenore; Wed. afternoon: Kyle\n",
      "D.) Mon. morning: Olivia; Mon. afternoon: Nina Tues. morning: Irving; Tues. afternoon: Helen Wed. morning: Kyle; Wed. afternoon: George\n",
      "Answer: D\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Query: Passage: Of the eight students—George, Helen, Irving, Kyle, Lenore, Nina, Olivia, and Robert—in a seminar, exactly six will give individual oral reports during three consecutive days—Monday, Tuesday, and Wednesday. Exactly two reports will be given each day—one in the morning and one in the afternoon—according to the following conditions: Tuesday is the only day on which George can give a report. Neither Olivia nor Robert can give an afternoon report. If Nina gives a report, then on the next day Helen and Irving must both give reports, unless Nina's report is given on Wednesday.\n",
      "Q: If Kyle and Lenore do not give reports, then the morning reports on Monday, Tuesday, and Wednesday, respectively, could be given by\n",
      "Choices:\n",
      "A.) Helen, George, and Nina\n",
      "B.) Robert, George, and Helen\n",
      "C.) Irving, Robert, and Helen\n",
      "D.) Olivia, Robert, and Irving\n",
      "Answer:\n",
      "Correct prompt: 3\n",
      "\n",
      "prompt 0: Passage: Of the eight students—George, Helen, Irving, Kyle, Lenore, Nina, Olivia, and Robert—in a seminar, exactly six will give individual oral reports during three consecutive days—Monday, Tuesday, and Wednesday. Exactly two reports will be given each day—one in the morning and one in the afternoon—according to the following conditions: Tuesday is the only day on which George can give a report. Neither Olivia nor Robert can give an afternoon report. If Nina gives a report, then on the next day Helen and Irving must both give reports, unless Nina's report is given on Wednesday.\n",
      "Q: If Kyle and Lenore do not give reports, then the morning reports on Monday, Tuesday, and Wednesday, respectively, could be given by\n",
      "Choices:\n",
      "A.) Helen, George, and Nina\n",
      "B.) Robert, George, and Helen\n",
      "C.) Irving, Robert, and Helen\n",
      "D.) Olivia, Robert, and Irving\n",
      "Answer: A\n",
      "\n",
      "prompt 1: Passage: Of the eight students—George, Helen, Irving, Kyle, Lenore, Nina, Olivia, and Robert—in a seminar, exactly six will give individual oral reports during three consecutive days—Monday, Tuesday, and Wednesday. Exactly two reports will be given each day—one in the morning and one in the afternoon—according to the following conditions: Tuesday is the only day on which George can give a report. Neither Olivia nor Robert can give an afternoon report. If Nina gives a report, then on the next day Helen and Irving must both give reports, unless Nina's report is given on Wednesday.\n",
      "Q: If Kyle and Lenore do not give reports, then the morning reports on Monday, Tuesday, and Wednesday, respectively, could be given by\n",
      "Choices:\n",
      "A.) Helen, George, and Nina\n",
      "B.) Robert, George, and Helen\n",
      "C.) Irving, Robert, and Helen\n",
      "D.) Olivia, Robert, and Irving\n",
      "Answer: B\n",
      "\n",
      "prompt 2: Passage: Of the eight students—George, Helen, Irving, Kyle, Lenore, Nina, Olivia, and Robert—in a seminar, exactly six will give individual oral reports during three consecutive days—Monday, Tuesday, and Wednesday. Exactly two reports will be given each day—one in the morning and one in the afternoon—according to the following conditions: Tuesday is the only day on which George can give a report. Neither Olivia nor Robert can give an afternoon report. If Nina gives a report, then on the next day Helen and Irving must both give reports, unless Nina's report is given on Wednesday.\n",
      "Q: If Kyle and Lenore do not give reports, then the morning reports on Monday, Tuesday, and Wednesday, respectively, could be given by\n",
      "Choices:\n",
      "A.) Helen, George, and Nina\n",
      "B.) Robert, George, and Helen\n",
      "C.) Irving, Robert, and Helen\n",
      "D.) Olivia, Robert, and Irving\n",
      "Answer: C\n",
      "\n",
      "prompt 3: Passage: Of the eight students—George, Helen, Irving, Kyle, Lenore, Nina, Olivia, and Robert—in a seminar, exactly six will give individual oral reports during three consecutive days—Monday, Tuesday, and Wednesday. Exactly two reports will be given each day—one in the morning and one in the afternoon—according to the following conditions: Tuesday is the only day on which George can give a report. Neither Olivia nor Robert can give an afternoon report. If Nina gives a report, then on the next day Helen and Irving must both give reports, unless Nina's report is given on Wednesday.\n",
      "Q: If Kyle and Lenore do not give reports, then the morning reports on Monday, Tuesday, and Wednesday, respectively, could be given by\n",
      "Choices:\n",
      "A.) Helen, George, and Nina\n",
      "B.) Robert, George, and Helen\n",
      "C.) Irving, Robert, and Helen\n",
      "D.) Olivia, Robert, and Irving\n",
      "Answer: D\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Query: Passage: Of the eight students—George, Helen, Irving, Kyle, Lenore, Nina, Olivia, and Robert—in a seminar, exactly six will give individual oral reports during three consecutive days—Monday, Tuesday, and Wednesday. Exactly two reports will be given each day—one in the morning and one in the afternoon—according to the following conditions: Tuesday is the only day on which George can give a report. Neither Olivia nor Robert can give an afternoon report. If Nina gives a report, then on the next day Helen and Irving must both give reports, unless Nina's report is given on Wednesday.\n",
      "Q: Which one of the following is a pair of students who, if they give reports on the same day as each other, must give reports on Wednesday?\n",
      "Choices:\n",
      "A.) George and Lenore\n",
      "B.) Helen and Nina\n",
      "C.) Irving and Robert\n",
      "D.) Olivia and Kyle\n",
      "Answer:\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: Passage: Of the eight students—George, Helen, Irving, Kyle, Lenore, Nina, Olivia, and Robert—in a seminar, exactly six will give individual oral reports during three consecutive days—Monday, Tuesday, and Wednesday. Exactly two reports will be given each day—one in the morning and one in the afternoon—according to the following conditions: Tuesday is the only day on which George can give a report. Neither Olivia nor Robert can give an afternoon report. If Nina gives a report, then on the next day Helen and Irving must both give reports, unless Nina's report is given on Wednesday.\n",
      "Q: Which one of the following is a pair of students who, if they give reports on the same day as each other, must give reports on Wednesday?\n",
      "Choices:\n",
      "A.) George and Lenore\n",
      "B.) Helen and Nina\n",
      "C.) Irving and Robert\n",
      "D.) Olivia and Kyle\n",
      "Answer: A\n",
      "\n",
      "prompt 1: Passage: Of the eight students—George, Helen, Irving, Kyle, Lenore, Nina, Olivia, and Robert—in a seminar, exactly six will give individual oral reports during three consecutive days—Monday, Tuesday, and Wednesday. Exactly two reports will be given each day—one in the morning and one in the afternoon—according to the following conditions: Tuesday is the only day on which George can give a report. Neither Olivia nor Robert can give an afternoon report. If Nina gives a report, then on the next day Helen and Irving must both give reports, unless Nina's report is given on Wednesday.\n",
      "Q: Which one of the following is a pair of students who, if they give reports on the same day as each other, must give reports on Wednesday?\n",
      "Choices:\n",
      "A.) George and Lenore\n",
      "B.) Helen and Nina\n",
      "C.) Irving and Robert\n",
      "D.) Olivia and Kyle\n",
      "Answer: B\n",
      "\n",
      "prompt 2: Passage: Of the eight students—George, Helen, Irving, Kyle, Lenore, Nina, Olivia, and Robert—in a seminar, exactly six will give individual oral reports during three consecutive days—Monday, Tuesday, and Wednesday. Exactly two reports will be given each day—one in the morning and one in the afternoon—according to the following conditions: Tuesday is the only day on which George can give a report. Neither Olivia nor Robert can give an afternoon report. If Nina gives a report, then on the next day Helen and Irving must both give reports, unless Nina's report is given on Wednesday.\n",
      "Q: Which one of the following is a pair of students who, if they give reports on the same day as each other, must give reports on Wednesday?\n",
      "Choices:\n",
      "A.) George and Lenore\n",
      "B.) Helen and Nina\n",
      "C.) Irving and Robert\n",
      "D.) Olivia and Kyle\n",
      "Answer: C\n",
      "\n",
      "prompt 3: Passage: Of the eight students—George, Helen, Irving, Kyle, Lenore, Nina, Olivia, and Robert—in a seminar, exactly six will give individual oral reports during three consecutive days—Monday, Tuesday, and Wednesday. Exactly two reports will be given each day—one in the morning and one in the afternoon—according to the following conditions: Tuesday is the only day on which George can give a report. Neither Olivia nor Robert can give an afternoon report. If Nina gives a report, then on the next day Helen and Irving must both give reports, unless Nina's report is given on Wednesday.\n",
      "Q: Which one of the following is a pair of students who, if they give reports on the same day as each other, must give reports on Wednesday?\n",
      "Choices:\n",
      "A.) George and Lenore\n",
      "B.) Helen and Nina\n",
      "C.) Irving and Robert\n",
      "D.) Olivia and Kyle\n",
      "Answer: D\n",
      "\n",
      "============= bigbench_cs_algorithms =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored correct if the model generates\n",
      "the expected continuation.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Expected continuation: 8\n",
      "\n",
      "prompt 0: Given two strings, determine the length of the longest common subsequence.\n",
      "\n",
      "Strings: ZUIEJOBQXVLXBMVOOMNJQSHJPY OVCOZNKRCXBYSMWOPRWMXIPCMRKX\n",
      "Length of longest common subsequence:\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Expected continuation: 4\n",
      "\n",
      "prompt 0: Given two strings, determine the length of the longest common subsequence.\n",
      "\n",
      "Strings: GDMKSZERXGPYTKJRQMLSSKLEALAJGS IEENJJUFVSYNX\n",
      "Length of longest common subsequence:\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Expected continuation: 2\n",
      "\n",
      "prompt 0: Given two strings, determine the length of the longest common subsequence.\n",
      "\n",
      "Strings: MKBOAUC SZFLGZIFSVPNIANSPZCXZSDJPLMBYK\n",
      "Length of longest common subsequence:\n",
      "\n",
      "============= bigbench_operators =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored correct if the model generates\n",
      "the expected continuation.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Expected continuation: 17\n",
      "\n",
      "prompt 0: Given the definition of the op operator, compute the result.\n",
      "op i is either the half of i when i is even, or its double when it is not.\n",
      "op (op 17) =\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Expected continuation: 128\n",
      "\n",
      "prompt 0: Given the definition of the op operator, compute the result.\n",
      "op n1 n2 ... nn extracts the last multiple of 8 from the n listed numbers.\n",
      "op 4 32 128 132 =\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Expected continuation: 3\n",
      "\n",
      "prompt 0: Given the definition of the op operator, compute the result.\n",
      "i op1 j is the product of i and j; i op2 j is the integer division of i by j.\n",
      "2 op1 6 op2 4 =\n",
      "\n",
      "============= bigbench_repeat_copy_logic =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored correct if the model generates\n",
      "the expected continuation.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Expected continuation: they're delicious they're delicious they're delicious they're delicious they're delicious they're delicious they're delicious\n",
      "\n",
      "prompt 0: repeat with logic:\n",
      "\n",
      "Q: A watermelon has seven seeds. Repeat they're delicious once for every seed\n",
      "A:\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Expected continuation: python python data python python data python python data\n",
      "\n",
      "prompt 0: repeat with logic:\n",
      "\n",
      "Q: say python twice and data once, and then repeat all of this three times.\n",
      "A:\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Expected continuation: if you teach a person to fish you will if you teach a person to fish you will if you teach a person to fish you will\n",
      "\n",
      "prompt 0: repeat with logic:\n",
      "\n",
      "Q: output if you teach a person to fish you will three times\n",
      "A:\n",
      "\n",
      "============= squad =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored correct if the model generates\n",
      "the expected continuation.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Expected continuation: Denver Broncos\n",
      "\n",
      "prompt 0: Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "Question: Which NFL team represented the AFC at Super Bowl 50?\n",
      "Answer:\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Expected continuation: Carolina Panthers\n",
      "\n",
      "prompt 0: Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "Question: Which NFL team represented the NFC at Super Bowl 50?\n",
      "Answer:\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Expected continuation: Santa Clara, California\n",
      "\n",
      "prompt 0: Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "Question: Where did Super Bowl 50 take place?\n",
      "Answer:\n",
      "\n",
      "============= coqa =============\n",
      "\n",
      "This is a language modeling task type. Each item will be scored correct if the model generates\n",
      "the expected continuation.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Expected continuation: white\n",
      "\n",
      "prompt 0: Below is a story followed by a series of related questions. Please answer the final question by referring to the story and the previous questions.\n",
      "Story: Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
      "\n",
      "\"What are you doing, Cotton?!\" \n",
      "\n",
      "\"I only wanted to be more like you\". \n",
      "\n",
      "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
      "\n",
      "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
      "\n",
      "Then Cotton thought, \"I change my mind. I like being special\".\n",
      "\n",
      "Final question:\n",
      "Question: What color was Cotton?\n",
      "Answer:\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Expected continuation: in a barn\n",
      "\n",
      "prompt 0: Below is a story followed by a series of related questions. Please answer the final question by referring to the story and the previous questions.\n",
      "Story: Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
      "\n",
      "\"What are you doing, Cotton?!\" \n",
      "\n",
      "\"I only wanted to be more like you\". \n",
      "\n",
      "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
      "\n",
      "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
      "\n",
      "Then Cotton thought, \"I change my mind. I like being special\".\n",
      "Preceding questions:\n",
      "Question: What color was Cotton?\n",
      "Answer: white\n",
      "\n",
      "Final question:\n",
      "Question: Where did she live?\n",
      "Answer:\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Expected continuation: no\n",
      "\n",
      "prompt 0: Below is a story followed by a series of related questions. Please answer the final question by referring to the story and the previous questions.\n",
      "Story: Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
      "\n",
      "\"What are you doing, Cotton?!\" \n",
      "\n",
      "\"I only wanted to be more like you\". \n",
      "\n",
      "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
      "\n",
      "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
      "\n",
      "Then Cotton thought, \"I change my mind. I like being special\".\n",
      "Preceding questions:\n",
      "Question: What color was Cotton?\n",
      "Answer: white\n",
      "Question: Where did she live?\n",
      "Answer: in a barn\n",
      "\n",
      "Final question:\n",
      "Question: Did she live alone?\n",
      "Answer:\n",
      "\n",
      "============= boolq =============\n",
      "\n",
      "This is multiple choice so each item will be scored correct if the choice with the highest probabiliy\n",
      "matches the correct choice. To get into the mechanics a bit more, it's really only the probabilities \n",
      "of the \"choice part\" that are looked at for each of the n \"prompts\". The \"choice part\" is what comes after\n",
      "the query. The query is repeated in each prompt, forming a common prefix. Think about it as which choice\n",
      "has the highest probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Query: Passage: All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline.\n",
      "Question: does ethanol take more energy make that produces?\n",
      "Correct prompt: 0\n",
      "\n",
      "prompt 0: Passage: All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline.\n",
      "Question: does ethanol take more energy make that produces?\n",
      "Answer: no\n",
      "\n",
      "prompt 1: Passage: All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline.\n",
      "Question: does ethanol take more energy make that produces?\n",
      "Answer: yes\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Query: Passage: Property tax or 'house tax' is a local tax on buildings, along with appurtenant land. It is and imposed on the Possessor (not the custodian of property as per 1978, 44th amendment of constitution). It resembles the US-type wealth tax and differs from the excise-type UK rate. The tax power is vested in the states and is delegated to local bodies, specifying the valuation method, rate band, and collection procedures. The tax base is the annual rental value (ARV) or area-based rating. Owner-occupied and other properties not producing rent are assessed on cost and then converted into ARV by applying a percentage of cost, usually four percent. Vacant land is generally exempt. Central government properties are exempt. Instead a 'service charge' is permissible under executive order. Properties of foreign missions also enjoy tax exemption without requiring reciprocity. The tax is usually accompanied by service taxes, e.g., water tax, drainage tax, conservancy (sanitation) tax, lighting tax, all using the same tax base. The rate structure is flat on rural (panchayat) properties, but in the urban (municipal) areas it is mildly progressive with about 80% of assessments falling in the first two brackets.\n",
      "Question: is house tax and property tax are same?\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: Passage: Property tax or 'house tax' is a local tax on buildings, along with appurtenant land. It is and imposed on the Possessor (not the custodian of property as per 1978, 44th amendment of constitution). It resembles the US-type wealth tax and differs from the excise-type UK rate. The tax power is vested in the states and is delegated to local bodies, specifying the valuation method, rate band, and collection procedures. The tax base is the annual rental value (ARV) or area-based rating. Owner-occupied and other properties not producing rent are assessed on cost and then converted into ARV by applying a percentage of cost, usually four percent. Vacant land is generally exempt. Central government properties are exempt. Instead a 'service charge' is permissible under executive order. Properties of foreign missions also enjoy tax exemption without requiring reciprocity. The tax is usually accompanied by service taxes, e.g., water tax, drainage tax, conservancy (sanitation) tax, lighting tax, all using the same tax base. The rate structure is flat on rural (panchayat) properties, but in the urban (municipal) areas it is mildly progressive with about 80% of assessments falling in the first two brackets.\n",
      "Question: is house tax and property tax are same?\n",
      "Answer: no\n",
      "\n",
      "prompt 1: Passage: Property tax or 'house tax' is a local tax on buildings, along with appurtenant land. It is and imposed on the Possessor (not the custodian of property as per 1978, 44th amendment of constitution). It resembles the US-type wealth tax and differs from the excise-type UK rate. The tax power is vested in the states and is delegated to local bodies, specifying the valuation method, rate band, and collection procedures. The tax base is the annual rental value (ARV) or area-based rating. Owner-occupied and other properties not producing rent are assessed on cost and then converted into ARV by applying a percentage of cost, usually four percent. Vacant land is generally exempt. Central government properties are exempt. Instead a 'service charge' is permissible under executive order. Properties of foreign missions also enjoy tax exemption without requiring reciprocity. The tax is usually accompanied by service taxes, e.g., water tax, drainage tax, conservancy (sanitation) tax, lighting tax, all using the same tax base. The rate structure is flat on rural (panchayat) properties, but in the urban (municipal) areas it is mildly progressive with about 80% of assessments falling in the first two brackets.\n",
      "Question: is house tax and property tax are same?\n",
      "Answer: yes\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Query: Passage: Phantom pain sensations are described as perceptions that an individual experiences relating to a limb or an organ that is not physically part of the body. Limb loss is a result of either removal by amputation or congenital limb deficiency. However, phantom limb sensations can also occur following nerve avulsion or spinal cord injury.\n",
      "Question: is pain experienced in a missing body part or paralyzed area?\n",
      "Correct prompt: 1\n",
      "\n",
      "prompt 0: Passage: Phantom pain sensations are described as perceptions that an individual experiences relating to a limb or an organ that is not physically part of the body. Limb loss is a result of either removal by amputation or congenital limb deficiency. However, phantom limb sensations can also occur following nerve avulsion or spinal cord injury.\n",
      "Question: is pain experienced in a missing body part or paralyzed area?\n",
      "Answer: no\n",
      "\n",
      "prompt 1: Passage: Phantom pain sensations are described as perceptions that an individual experiences relating to a limb or an organ that is not physically part of the body. Limb loss is a result of either removal by amputation or congenital limb deficiency. However, phantom limb sensations can also occur following nerve avulsion or spinal cord injury.\n",
      "Question: is pain experienced in a missing body part or paralyzed area?\n",
      "Answer: yes\n",
      "\n",
      "============= bigbench_language_identification =============\n",
      "\n",
      "This is multiple choice so each item will be scored correct if the choice with the highest probabiliy\n",
      "matches the correct choice. To get into the mechanics a bit more, it's really only the probabilities \n",
      "of the \"choice part\" that are looked at for each of the n \"prompts\". The \"choice part\" is what comes after\n",
      "the query. The query is repeated in each prompt, forming a common prefix. Think about it as which choice\n",
      "has the highest probability given the text that comes before it. That's the one the model thinks is right.\n",
      "\n",
      "----------- item: 0 ------------\n",
      "Query: Given a sentence, select the correct language among the choices\n",
      "Sentence: Chaymanta Apolos uk lugar Acaya shutiqman rinatinqa, Jesuspi kriyiqkunaqa, kusata animachir uk kartata rurar kaĉharanllapa Acayapi kriyiqkunaman, Apolos ĉhatinqa shumaqta samachinanllapapaq. Chaymi Apolos ĉharqa, Dyus akrashan kriyiqkunataqa, kusata yanaparan.\n",
      "A. Nepali\n",
      "B. Eastern Maroon Creole\n",
      "C. Hdi\n",
      "D. Lambayeque Quechua\n",
      "Answer: \n",
      "Correct prompt: 3\n",
      "\n",
      "prompt 0: Given a sentence, select the correct language among the choices\n",
      "Sentence: Chaymanta Apolos uk lugar Acaya shutiqman rinatinqa, Jesuspi kriyiqkunaqa, kusata animachir uk kartata rurar kaĉharanllapa Acayapi kriyiqkunaman, Apolos ĉhatinqa shumaqta samachinanllapapaq. Chaymi Apolos ĉharqa, Dyus akrashan kriyiqkunataqa, kusata yanaparan.\n",
      "A. Nepali\n",
      "B. Eastern Maroon Creole\n",
      "C. Hdi\n",
      "D. Lambayeque Quechua\n",
      "Answer:  A\n",
      "\n",
      "prompt 1: Given a sentence, select the correct language among the choices\n",
      "Sentence: Chaymanta Apolos uk lugar Acaya shutiqman rinatinqa, Jesuspi kriyiqkunaqa, kusata animachir uk kartata rurar kaĉharanllapa Acayapi kriyiqkunaman, Apolos ĉhatinqa shumaqta samachinanllapapaq. Chaymi Apolos ĉharqa, Dyus akrashan kriyiqkunataqa, kusata yanaparan.\n",
      "A. Nepali\n",
      "B. Eastern Maroon Creole\n",
      "C. Hdi\n",
      "D. Lambayeque Quechua\n",
      "Answer:  B\n",
      "\n",
      "prompt 2: Given a sentence, select the correct language among the choices\n",
      "Sentence: Chaymanta Apolos uk lugar Acaya shutiqman rinatinqa, Jesuspi kriyiqkunaqa, kusata animachir uk kartata rurar kaĉharanllapa Acayapi kriyiqkunaman, Apolos ĉhatinqa shumaqta samachinanllapapaq. Chaymi Apolos ĉharqa, Dyus akrashan kriyiqkunataqa, kusata yanaparan.\n",
      "A. Nepali\n",
      "B. Eastern Maroon Creole\n",
      "C. Hdi\n",
      "D. Lambayeque Quechua\n",
      "Answer:  C\n",
      "\n",
      "prompt 3: Given a sentence, select the correct language among the choices\n",
      "Sentence: Chaymanta Apolos uk lugar Acaya shutiqman rinatinqa, Jesuspi kriyiqkunaqa, kusata animachir uk kartata rurar kaĉharanllapa Acayapi kriyiqkunaman, Apolos ĉhatinqa shumaqta samachinanllapapaq. Chaymi Apolos ĉharqa, Dyus akrashan kriyiqkunataqa, kusata yanaparan.\n",
      "A. Nepali\n",
      "B. Eastern Maroon Creole\n",
      "C. Hdi\n",
      "D. Lambayeque Quechua\n",
      "Answer:  D\n",
      "\n",
      "----------- item: 1 ------------\n",
      "Query: Given a sentence, select the correct language among the choices\n",
      "Sentence: Ñõn ro otemjej i Rom, jitenburu iben Anij, emwij kir ir bwe ren dri kwojarjar: Joij ñõn kom im ainemõn jen Anij Jemer im Iroij Jisõs Kraist.\n",
      "A. Rote Lole\n",
      "B. Swedish\n",
      "C. Marshallese\n",
      "D. Boharic Coptic\n",
      "Answer: \n",
      "Correct prompt: 2\n",
      "\n",
      "prompt 0: Given a sentence, select the correct language among the choices\n",
      "Sentence: Ñõn ro otemjej i Rom, jitenburu iben Anij, emwij kir ir bwe ren dri kwojarjar: Joij ñõn kom im ainemõn jen Anij Jemer im Iroij Jisõs Kraist.\n",
      "A. Rote Lole\n",
      "B. Swedish\n",
      "C. Marshallese\n",
      "D. Boharic Coptic\n",
      "Answer:  A\n",
      "\n",
      "prompt 1: Given a sentence, select the correct language among the choices\n",
      "Sentence: Ñõn ro otemjej i Rom, jitenburu iben Anij, emwij kir ir bwe ren dri kwojarjar: Joij ñõn kom im ainemõn jen Anij Jemer im Iroij Jisõs Kraist.\n",
      "A. Rote Lole\n",
      "B. Swedish\n",
      "C. Marshallese\n",
      "D. Boharic Coptic\n",
      "Answer:  B\n",
      "\n",
      "prompt 2: Given a sentence, select the correct language among the choices\n",
      "Sentence: Ñõn ro otemjej i Rom, jitenburu iben Anij, emwij kir ir bwe ren dri kwojarjar: Joij ñõn kom im ainemõn jen Anij Jemer im Iroij Jisõs Kraist.\n",
      "A. Rote Lole\n",
      "B. Swedish\n",
      "C. Marshallese\n",
      "D. Boharic Coptic\n",
      "Answer:  C\n",
      "\n",
      "prompt 3: Given a sentence, select the correct language among the choices\n",
      "Sentence: Ñõn ro otemjej i Rom, jitenburu iben Anij, emwij kir ir bwe ren dri kwojarjar: Joij ñõn kom im ainemõn jen Anij Jemer im Iroij Jisõs Kraist.\n",
      "A. Rote Lole\n",
      "B. Swedish\n",
      "C. Marshallese\n",
      "D. Boharic Coptic\n",
      "Answer:  D\n",
      "\n",
      "----------- item: 2 ------------\n",
      "Query: Given a sentence, select the correct language among the choices\n",
      "Sentence: Kantyané mau terus niba njaluk kawelasan ngomong: ‘Mbok sabar ta! Utangku bakal tak bayar kabèh!’\n",
      "A. Eastern Tzutujil\n",
      "B. Zazaki\n",
      "C. Bena-bena\n",
      "D. Caribbean Javanese\n",
      "Answer: \n",
      "Correct prompt: 3\n",
      "\n",
      "prompt 0: Given a sentence, select the correct language among the choices\n",
      "Sentence: Kantyané mau terus niba njaluk kawelasan ngomong: ‘Mbok sabar ta! Utangku bakal tak bayar kabèh!’\n",
      "A. Eastern Tzutujil\n",
      "B. Zazaki\n",
      "C. Bena-bena\n",
      "D. Caribbean Javanese\n",
      "Answer:  A\n",
      "\n",
      "prompt 1: Given a sentence, select the correct language among the choices\n",
      "Sentence: Kantyané mau terus niba njaluk kawelasan ngomong: ‘Mbok sabar ta! Utangku bakal tak bayar kabèh!’\n",
      "A. Eastern Tzutujil\n",
      "B. Zazaki\n",
      "C. Bena-bena\n",
      "D. Caribbean Javanese\n",
      "Answer:  B\n",
      "\n",
      "prompt 2: Given a sentence, select the correct language among the choices\n",
      "Sentence: Kantyané mau terus niba njaluk kawelasan ngomong: ‘Mbok sabar ta! Utangku bakal tak bayar kabèh!’\n",
      "A. Eastern Tzutujil\n",
      "B. Zazaki\n",
      "C. Bena-bena\n",
      "D. Caribbean Javanese\n",
      "Answer:  C\n",
      "\n",
      "prompt 3: Given a sentence, select the correct language among the choices\n",
      "Sentence: Kantyané mau terus niba njaluk kawelasan ngomong: ‘Mbok sabar ta! Utangku bakal tak bayar kabèh!’\n",
      "A. Eastern Tzutujil\n",
      "B. Zazaki\n",
      "C. Bena-bena\n",
      "D. Caribbean Javanese\n",
      "Answer:  D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "814e4a49-dc57-4fec-a320-b1bfb40d1779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2f9ede80-6eeb-4def-84f3-ad19aa72768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_examples():\n",
    "    \n",
    "    import sys\n",
    "    sys.path.append('../my_nanochat')\n",
    "    import os\n",
    "    import yaml\n",
    "    import json\n",
    "    from my_nanochat.my_common import get_base_dir\n",
    "    from my_nanochat.my_core_eval import render_prompts_mc, render_prompts_lm, render_prompts_schema\n",
    "    \n",
    "    base_dir = get_base_dir()\n",
    "    eval_bundle_dir = os.path.join(base_dir, \"eval_bundle\")\n",
    "    config_path = os.path.join(eval_bundle_dir, \"core.yaml\")\n",
    "    data_base_path = os.path.join(eval_bundle_dir, \"eval_data\")\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    tasks = config['icl_tasks']\n",
    "    for task in tasks:\n",
    "        print(f\"============= {task['label']} =============\")\n",
    "    \n",
    "        task_type = task['icl_task_type']\n",
    "        if task_type == 'multiple_choice':\n",
    "            print(\"\"\"\n",
    "This is multiple choice so each item will be scored correct if the choice with the highest probabiliy\n",
    "matches the correct choice. To get into the mechanics a bit more, it's really only the probabilities \n",
    "of the \"choice part\" that are looked at for each of the n \"prompts\". The \"choice part\" is what comes after\n",
    "the query. The query is repeated in each prompt, forming a common prefix. Think about it as which choice\n",
    "has the highest probability given the text that comes before it. That's the one the model thinks is right.\\n\"\"\")\n",
    "        elif task_type == 'language_modeling':\n",
    "            print(\"\"\"\n",
    "This is a language modeling task type. Each item will be scored correct if the model generates\n",
    "the expected continuation.\\n\"\"\")\n",
    "        elif task_type == 'schema':\n",
    "             print(\"\"\"\n",
    "This is a schema task type. Each item will be scored correct if the \"continuation part\" with the\n",
    "highest probability is in the correct prompt. This is similar to multiple choice except here we\n",
    "have a common suffix (the continuation) and in multiple choice we have a common prefix (the query).\n",
    "The continuations are the same in each prompt so in isolation they would have the same probability.\n",
    "The key is they are judged in the context of the full prompt. It's also important that we look at\n",
    "the probabilities only of the continuation parts, because we're interested in which is most probable\n",
    "in the given context, not which prompt overall is more likely.\\n\"\"\")\n",
    "        else:\n",
    "            assert False\n",
    "        \n",
    "        continuation_delimiter = task.get('continuation_delimiter', ' ')\n",
    "        data_path = os.path.join(data_base_path, task['dataset_uri'])\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            data = [json.loads(line.strip()) for line in f]\n",
    "    \n",
    "        for i in range(3):\n",
    "            print(f\"----------- item: {i} ------------\")\n",
    "            item = data[i]\n",
    "            if task_type == 'multiple_choice':\n",
    "                print(f\"Query: {item['query']}\")\n",
    "                print(f\"Correct prompt: {item['gold']}\\n\") \n",
    "                prompts = render_prompts_mc(item, continuation_delimiter, [])\n",
    "            elif task_type == 'language_modeling':\n",
    "                print(f\"Expected continuation: {item['continuation']}\\n\")\n",
    "                prompts = render_prompts_lm(item, continuation_delimiter, [])\n",
    "                prompts = prompts[:-1] # because in CORE eval we only use the first method of scoring\n",
    "            elif task_type == 'schema':\n",
    "                print(f\"Continuation part: {item['continuation']}\")\n",
    "                print(f\"Correct prompt: {item['gold']}\\n\")            \n",
    "                prompts = render_prompts_schema(item, continuation_delimiter, [])\n",
    "            else:\n",
    "                assert False\n",
    "    \n",
    "            for j, prompt in enumerate(prompts):\n",
    "                print(f\"prompt {j}: {prompt}\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a75e87-d083-4bfa-9b64-bd33730e518a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
