{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64619e17-8458-4506-ae9d-e36a68b26c91",
   "metadata": {},
   "source": [
    "### Try CORE evaluation on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4287e4bf-2cee-4c73-b978-8b1898084afd",
   "metadata": {},
   "source": [
    "This is not the main notebook in this challenge. See `understand-core-metric.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eba420-579b-4656-95fa-9126c0052279",
   "metadata": {},
   "source": [
    "#### Existing model\n",
    "\n",
    "First load our existing model and see if can get the full evaluation (limited to 20 examples per task) to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21606e74-c15f-4709-b6fa-e9bcda0bbbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "import os\n",
    "import torch\n",
    "from my_nanochat.my_common import get_base_dir, autodetect_device_type\n",
    "from my_nanochat.my_checkpoint_manager import build_model\n",
    "from contextlib import nullcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b552669d-0847-497a-ac4c-8dfd5fd484c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n"
     ]
    }
   ],
   "source": [
    "device_type = autodetect_device_type()\n",
    "device = torch.device(device_type)\n",
    "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef6e8c96-122d-4845-b63a-43dc431dd713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with config: {'sequence_len': 1000, 'vocab_size': 65537, 'n_layer': 12, 'n_head': 6, 'n_kv_head': 6, 'n_embd': 768}\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = os.path.join(get_base_dir(), \"base_checkpoints\", \"d12\")\n",
    "model, tokenizer, meta_data = build_model(checkpoint_dir, step=21000, device=device, phase=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6181eb15-825c-42c7-ab8a-b57adc55c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.my_base_eval import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71a28768-fa2e-485b-abff-92201d62c360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.3500 | centered: 0.1333 | time: 1.13s\n",
      "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.72s\n",
      "Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.51s\n",
      "Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.2000 | centered: -0.0667 | time: 2.45s\n",
      "Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.1500 | centered: -0.1333 | time: 3.07s\n",
      "Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.5500 | centered: 0.1000 | time: 0.36s\n",
      "Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.1500 | centered: -0.0625 | time: 3.46s\n",
      "Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.7500 | centered: 0.5000 | time: 1.58s\n",
      "Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.2000 | centered: -0.0667 | time: 0.39s\n",
      "Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.42s\n",
      "Evaluating: hellaswag (10-shot, type: multiple_choice)... accuracy: 0.3000 | centered: 0.0667 | time: 8.48s\n",
      "Evaluating: winograd (0-shot, type: schema)... accuracy: 0.5500 | centered: 0.1000 | time: 0.35s\n",
      "Evaluating: winogrande (0-shot, type: schema)... accuracy: 0.3500 | centered: -0.3000 | time: 0.35s\n",
      "Evaluating: bigbench_dyck_languages (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 1.09s\n",
      "Evaluating: agi_eval_lsat_ar (3-shot, type: multiple_choice)... accuracy: 0.1500 | centered: -0.0625 | time: 6.06s\n",
      "Evaluating: bigbench_cs_algorithms (10-shot, type: language_modeling)... accuracy: 0.3000 | centered: 0.3000 | time: 1.11s\n",
      "Evaluating: bigbench_operators (10-shot, type: language_modeling)... accuracy: 0.0500 | centered: 0.0500 | time: 0.88s\n",
      "Evaluating: bigbench_repeat_copy_logic (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 1.03s\n",
      "Evaluating: squad (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 4.61s\n",
      "Evaluating: coqa (0-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 1.10s\n",
      "Evaluating: boolq (10-shot, type: multiple_choice)... accuracy: 0.4000 | centered: -0.5789 | time: 8.39s\n",
      "Evaluating: bigbench_language_identification (10-shot, type: multiple_choice)... accuracy: 0.2000 | centered: 0.1199 | time: 16.64s\n"
     ]
    }
   ],
   "source": [
    "with autocast_ctx:\n",
    "    results = evaluate_model(model, tokenizer, device, max_per_task=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcc6563-0248-48c3-9c4d-f60b32899900",
   "metadata": {},
   "source": [
    "Well that was much faster and easier than on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28dd2df3-44e2-45f5-8d4a-c7d043eb8d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': {'hellaswag_zeroshot': 0.3499999940395355,\n",
       "  'jeopardy': 0.0,\n",
       "  'bigbench_qa_wikidata': 0.0,\n",
       "  'arc_easy': 0.20000000298023224,\n",
       "  'arc_challenge': 0.15000000596046448,\n",
       "  'copa': 0.550000011920929,\n",
       "  'commonsense_qa': 0.15000000596046448,\n",
       "  'piqa': 0.75,\n",
       "  'openbook_qa': 0.20000000298023224,\n",
       "  'lambada_openai': 0.0,\n",
       "  'hellaswag': 0.30000001192092896,\n",
       "  'winograd': 0.550000011920929,\n",
       "  'winogrande': 0.3499999940395355,\n",
       "  'bigbench_dyck_languages': 0.0,\n",
       "  'agi_eval_lsat_ar': 0.15000000596046448,\n",
       "  'bigbench_cs_algorithms': 0.30000001192092896,\n",
       "  'bigbench_operators': 0.05000000074505806,\n",
       "  'bigbench_repeat_copy_logic': 0.0,\n",
       "  'squad': 0.0,\n",
       "  'coqa': 0.0,\n",
       "  'boolq': 0.4000000059604645,\n",
       "  'bigbench_language_identification': 0.20000000298023224},\n",
       " 'centered_result': {'hellaswag_zeroshot': 0.13333332538604736,\n",
       "  'jeopardy': 0.0,\n",
       "  'bigbench_qa_wikidata': 0.0,\n",
       "  'arc_easy': -0.06666666269302368,\n",
       "  'arc_challenge': -0.13333332538604736,\n",
       "  'copa': 0.10000002384185791,\n",
       "  'commonsense_qa': -0.06249999254941942,\n",
       "  'piqa': 0.5,\n",
       "  'openbook_qa': -0.06666666269302368,\n",
       "  'lambada_openai': 0.0,\n",
       "  'hellaswag': 0.0666666825612386,\n",
       "  'winograd': 0.10000002384185791,\n",
       "  'winogrande': -0.30000001192092896,\n",
       "  'bigbench_dyck_languages': 0.0,\n",
       "  'agi_eval_lsat_ar': -0.06249999254941942,\n",
       "  'bigbench_cs_algorithms': 0.30000001192092896,\n",
       "  'bigbench_operators': 0.05000000074505806,\n",
       "  'bigbench_repeat_copy_logic': 0.0,\n",
       "  'squad': 0.0,\n",
       "  'coqa': 0.0,\n",
       "  'boolq': -0.5789473527356198,\n",
       "  'bigbench_language_identification': 0.11991199447770323},\n",
       " 'core_metric': 0.004513548283964075}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b552711f-fe16-4664-91ba-08ab128e4bfb",
   "metadata": {},
   "source": [
    "#### During training\n",
    "\n",
    "Train a tiny model to make sure doing the CORE eval from the training loop works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19148196-68b3-4971-ace5-af50200da531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"../my_nanochat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25172e60-d741-49d8-84f2-a2a92ad0eb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overriding depth = 4\n",
      "overriding max_seq_len = 1000\n",
      "overriding device_batch_size = 2\n",
      "overriding num_iterations = 100\n",
      "overriding total_batch_size = 2000\n",
      "overriding eval_every = 10\n",
      "overriding eval_tokens = 4000\n",
      "overriding sample_every = 50\n",
      "overriding core_metric_every = 50\n",
      "overriding core_metric_max_per_task = 20\n",
      "user_config: {'device_type': '', 'depth': 4, 'max_seq_len': 1000, 'num_iterations': 100, 'device_batch_size': 2, 'total_batch_size': 2000, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 10, 'eval_tokens': 4000, 'core_metric_every': 50, 'core_metric_max_per_task': 20, 'sample_every': 50, 'model_tag': ''}\n",
      "Autodetected device type: cuda\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Vocab size: 65,537\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 2 x 1000 = 2,000\n",
      "Tokens / micro-batch: 2,000\n",
      "Total batch size 2,000 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65537, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
      ")\n",
      "Number of parameters: 36,700,672\n",
      "Using user-provided number of iterations: 100\n",
      "Total number of training tokens: 200,000\n",
      "tokens : param ratio: 0.01 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "x.shape is [2, 1000], y.shape is [2, 1000] -- should match\n",
      "W1114 14:47:12.450000 6203 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000 | Validation bpb: 3.3274\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "step 00000/00100 (0.00%) | loss: 11.090369 | grad norm: 0.4901 | lrm: 1.00 | dt: 763.14ms | tok/sec: 2,620 | mfu: -1.00 | total time: 0.00m\n",
      "step 00001/00100 (1.00%) | loss: 11.005203 | grad norm: 2.4326 | lrm: 1.00 | dt: 119.52ms | tok/sec: 16,734 | mfu: -1.00 | total time: 0.00m\n",
      "step 00002/00100 (2.00%) | loss: 10.864455 | grad norm: 3.6754 | lrm: 1.00 | dt: 120.40ms | tok/sec: 16,611 | mfu: -1.00 | total time: 0.00m\n",
      "step 00003/00100 (3.00%) | loss: 10.587919 | grad norm: 4.5411 | lrm: 1.00 | dt: 117.77ms | tok/sec: 16,982 | mfu: -1.00 | total time: 0.00m\n",
      "step 00004/00100 (4.00%) | loss: 10.342331 | grad norm: 4.8741 | lrm: 1.00 | dt: 120.32ms | tok/sec: 16,622 | mfu: -1.00 | total time: 0.00m\n",
      "step 00005/00100 (5.00%) | loss: 10.132541 | grad norm: 3.6872 | lrm: 1.00 | dt: 120.87ms | tok/sec: 16,547 | mfu: -1.00 | total time: 0.00m\n",
      "step 00006/00100 (6.00%) | loss: 9.877790 | grad norm: 3.4877 | lrm: 1.00 | dt: 120.69ms | tok/sec: 16,571 | mfu: -1.00 | total time: 0.00m\n",
      "step 00007/00100 (7.00%) | loss: 9.609137 | grad norm: 3.4235 | lrm: 1.00 | dt: 120.68ms | tok/sec: 16,573 | mfu: -1.00 | total time: 0.00m\n",
      "step 00008/00100 (8.00%) | loss: 9.539441 | grad norm: 6.2645 | lrm: 1.00 | dt: 120.84ms | tok/sec: 16,551 | mfu: -1.00 | total time: 0.00m\n",
      "step 00009/00100 (9.00%) | loss: 9.415767 | grad norm: 5.5210 | lrm: 1.00 | dt: 120.74ms | tok/sec: 16,564 | mfu: -1.00 | total time: 0.00m\n",
      "step 00010 | Validation bpb: 2.5269\n",
      "step 00010/00100 (10.00%) | loss: 9.202004 | grad norm: 2.4842 | lrm: 1.00 | dt: 121.91ms | tok/sec: 16,405 | mfu: -1.00 | total time: 0.00m\n",
      "step 00011/00100 (11.00%) | loss: 9.041427 | grad norm: 2.1765 | lrm: 1.00 | dt: 121.16ms | tok/sec: 16,506 | mfu: -1.00 | total time: 0.00m\n",
      "step 00012/00100 (12.00%) | loss: 9.007414 | grad norm: 4.9158 | lrm: 1.00 | dt: 121.30ms | tok/sec: 16,488 | mfu: -1.00 | total time: 0.00m\n",
      "step 00013/00100 (13.00%) | loss: 8.924272 | grad norm: 1.8731 | lrm: 1.00 | dt: 120.77ms | tok/sec: 16,560 | mfu: -1.00 | total time: 0.01m\n",
      "step 00014/00100 (14.00%) | loss: 8.740223 | grad norm: 2.9835 | lrm: 1.00 | dt: 121.11ms | tok/sec: 16,513 | mfu: -1.00 | total time: 0.01m\n",
      "step 00015/00100 (15.00%) | loss: 8.624925 | grad norm: 3.3528 | lrm: 1.00 | dt: 120.90ms | tok/sec: 16,542 | mfu: -1.00 | total time: 0.01m\n",
      "step 00016/00100 (16.00%) | loss: 8.533557 | grad norm: 2.4524 | lrm: 1.00 | dt: 121.32ms | tok/sec: 16,485 | mfu: -1.00 | total time: 0.01m\n",
      "step 00017/00100 (17.00%) | loss: 8.471615 | grad norm: 2.0234 | lrm: 1.00 | dt: 121.45ms | tok/sec: 16,468 | mfu: -1.00 | total time: 0.01m\n",
      "step 00018/00100 (18.00%) | loss: 8.340482 | grad norm: 2.0264 | lrm: 1.00 | dt: 120.77ms | tok/sec: 16,559 | mfu: -1.00 | total time: 0.02m\n",
      "step 00019/00100 (19.00%) | loss: 8.295081 | grad norm: 3.0442 | lrm: 1.00 | dt: 120.89ms | tok/sec: 16,543 | mfu: -1.00 | total time: 0.02m\n",
      "step 00020 | Validation bpb: 2.4161\n",
      "step 00020/00100 (20.00%) | loss: 8.189740 | grad norm: 2.9497 | lrm: 1.00 | dt: 122.04ms | tok/sec: 16,388 | mfu: -1.00 | total time: 0.02m\n",
      "step 00021/00100 (21.00%) | loss: 8.082181 | grad norm: 1.3351 | lrm: 1.00 | dt: 120.83ms | tok/sec: 16,552 | mfu: -1.00 | total time: 0.02m\n",
      "step 00022/00100 (22.00%) | loss: 7.986411 | grad norm: 1.1470 | lrm: 1.00 | dt: 121.02ms | tok/sec: 16,525 | mfu: -1.00 | total time: 0.02m\n",
      "step 00023/00100 (23.00%) | loss: 7.938792 | grad norm: 1.8257 | lrm: 1.00 | dt: 120.90ms | tok/sec: 16,542 | mfu: -1.00 | total time: 0.03m\n",
      "step 00024/00100 (24.00%) | loss: 7.887733 | grad norm: 2.1398 | lrm: 1.00 | dt: 120.98ms | tok/sec: 16,531 | mfu: -1.00 | total time: 0.03m\n",
      "step 00025/00100 (25.00%) | loss: 7.874937 | grad norm: 2.9104 | lrm: 1.00 | dt: 121.44ms | tok/sec: 16,469 | mfu: -1.00 | total time: 0.03m\n",
      "step 00026/00100 (26.00%) | loss: 7.862727 | grad norm: 1.8626 | lrm: 1.00 | dt: 121.07ms | tok/sec: 16,519 | mfu: -1.00 | total time: 0.03m\n",
      "step 00027/00100 (27.00%) | loss: 7.838864 | grad norm: 3.0169 | lrm: 1.00 | dt: 121.44ms | tok/sec: 16,469 | mfu: -1.00 | total time: 0.03m\n",
      "step 00028/00100 (28.00%) | loss: 7.826892 | grad norm: 2.3641 | lrm: 1.00 | dt: 121.05ms | tok/sec: 16,521 | mfu: -1.00 | total time: 0.04m\n",
      "step 00029/00100 (29.00%) | loss: 7.856354 | grad norm: 1.4562 | lrm: 1.00 | dt: 121.20ms | tok/sec: 16,501 | mfu: -1.00 | total time: 0.04m\n",
      "step 00030 | Validation bpb: 2.3441\n",
      "step 00030/00100 (30.00%) | loss: 7.800278 | grad norm: 1.6368 | lrm: 1.00 | dt: 121.97ms | tok/sec: 16,397 | mfu: -1.00 | total time: 0.04m\n",
      "step 00031/00100 (31.00%) | loss: 7.775831 | grad norm: 2.1896 | lrm: 1.00 | dt: 121.33ms | tok/sec: 16,484 | mfu: -1.00 | total time: 0.04m\n",
      "step 00032/00100 (32.00%) | loss: 7.731588 | grad norm: 1.1259 | lrm: 1.00 | dt: 120.96ms | tok/sec: 16,534 | mfu: -1.00 | total time: 0.04m\n",
      "step 00033/00100 (33.00%) | loss: 7.697375 | grad norm: 1.4626 | lrm: 1.00 | dt: 121.25ms | tok/sec: 16,495 | mfu: -1.00 | total time: 0.05m\n",
      "step 00034/00100 (34.00%) | loss: 7.655858 | grad norm: 1.3823 | lrm: 1.00 | dt: 121.41ms | tok/sec: 16,473 | mfu: -1.00 | total time: 0.05m\n",
      "step 00035/00100 (35.00%) | loss: 7.636769 | grad norm: 2.6302 | lrm: 1.00 | dt: 121.68ms | tok/sec: 16,436 | mfu: -1.00 | total time: 0.05m\n",
      "step 00036/00100 (36.00%) | loss: 7.597827 | grad norm: 1.3581 | lrm: 1.00 | dt: 121.26ms | tok/sec: 16,493 | mfu: -1.00 | total time: 0.05m\n",
      "step 00037/00100 (37.00%) | loss: 7.539198 | grad norm: 1.9410 | lrm: 1.00 | dt: 121.36ms | tok/sec: 16,480 | mfu: -1.00 | total time: 0.05m\n",
      "step 00038/00100 (38.00%) | loss: 7.541516 | grad norm: 2.8688 | lrm: 1.00 | dt: 121.73ms | tok/sec: 16,429 | mfu: -1.00 | total time: 0.06m\n",
      "step 00039/00100 (39.00%) | loss: 7.527042 | grad norm: 1.4237 | lrm: 1.00 | dt: 121.20ms | tok/sec: 16,501 | mfu: -1.00 | total time: 0.06m\n",
      "step 00040 | Validation bpb: 2.3559\n",
      "step 00040/00100 (40.00%) | loss: 7.527799 | grad norm: 1.7665 | lrm: 1.00 | dt: 122.55ms | tok/sec: 16,320 | mfu: -1.00 | total time: 0.06m\n",
      "step 00041/00100 (41.00%) | loss: 7.569347 | grad norm: 2.1306 | lrm: 1.00 | dt: 121.28ms | tok/sec: 16,491 | mfu: -1.00 | total time: 0.06m\n",
      "step 00042/00100 (42.00%) | loss: 7.574180 | grad norm: 1.5226 | lrm: 1.00 | dt: 121.24ms | tok/sec: 16,496 | mfu: -1.00 | total time: 0.06m\n",
      "step 00043/00100 (43.00%) | loss: 7.542275 | grad norm: 1.6053 | lrm: 1.00 | dt: 121.18ms | tok/sec: 16,504 | mfu: -1.00 | total time: 0.07m\n",
      "step 00044/00100 (44.00%) | loss: 7.540087 | grad norm: 2.0709 | lrm: 1.00 | dt: 121.85ms | tok/sec: 16,413 | mfu: -1.00 | total time: 0.07m\n",
      "step 00045/00100 (45.00%) | loss: 7.534517 | grad norm: 1.2598 | lrm: 1.00 | dt: 121.82ms | tok/sec: 16,417 | mfu: -1.00 | total time: 0.07m\n",
      "step 00046/00100 (46.00%) | loss: 7.555648 | grad norm: 1.2200 | lrm: 1.00 | dt: 121.70ms | tok/sec: 16,433 | mfu: -1.00 | total time: 0.07m\n",
      "step 00047/00100 (47.00%) | loss: 7.523943 | grad norm: 1.2970 | lrm: 1.00 | dt: 121.49ms | tok/sec: 16,462 | mfu: -1.00 | total time: 0.07m\n",
      "step 00048/00100 (48.00%) | loss: 7.509429 | grad norm: 1.1723 | lrm: 1.00 | dt: 122.10ms | tok/sec: 16,380 | mfu: -1.00 | total time: 0.08m\n",
      "step 00049/00100 (49.00%) | loss: 7.545209 | grad norm: 1.7908 | lrm: 1.00 | dt: 122.70ms | tok/sec: 16,299 | mfu: -1.00 | total time: 0.08m\n",
      "step 00050 | Validation bpb: 2.3312\n",
      "Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.2000 | centered: -0.0667 | time: 0.39s\n",
      "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.31s\n",
      "Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.29s\n",
      "Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.2000 | centered: -0.0667 | time: 0.59s\n",
      "Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.1500 | centered: -0.1333 | time: 0.69s\n",
      "Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.6500 | centered: 0.3000 | time: 0.19s\n",
      "Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.2000 | centered: 0.0000 | time: 0.76s\n",
      "Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.7500 | centered: 0.5000 | time: 0.39s\n",
      "Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.2000 | centered: -0.0667 | time: 0.17s\n",
      "Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.20s\n",
      "Evaluating: hellaswag (10-shot, type: multiple_choice)... accuracy: 0.2000 | centered: -0.0667 | time: 1.71s\n",
      "Evaluating: winograd (0-shot, type: schema)... accuracy: 0.6000 | centered: 0.2000 | time: 0.15s\n",
      "Evaluating: winogrande (0-shot, type: schema)... accuracy: 0.4500 | centered: -0.1000 | time: 0.15s\n",
      "Evaluating: bigbench_dyck_languages (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.31s\n",
      "Evaluating: agi_eval_lsat_ar (3-shot, type: multiple_choice)... accuracy: 0.1000 | centered: -0.1250 | time: 1.22s\n",
      "Evaluating: bigbench_cs_algorithms (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.32s\n",
      "Evaluating: bigbench_operators (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.26s\n",
      "Evaluating: bigbench_repeat_copy_logic (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.29s\n",
      "Evaluating: squad (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.98s\n",
      "Evaluating: coqa (0-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.39s\n",
      "Evaluating: boolq (10-shot, type: multiple_choice)... accuracy: 0.4000 | centered: -0.5789 | time: 1.59s\n",
      "Evaluating: bigbench_language_identification (10-shot, type: multiple_choice)... accuracy: 0.2000 | centered: 0.1199 | time: 3.29s\n",
      "Step 00050: CORE metric: -0.0038\n",
      "<bos>The capital of France is a large on the word “the. The day\n",
      "<bos>The chemical symbol of gold is a large on the word “the. The day\n",
      "<bos>If yesterday was Friday, then tomorrow will be a large on the world. The day. The\n",
      "<bos>The opposite of hot is a large on the word “the. The day\n",
      "<bos>The planets of the solar system are: (the. The day. The day. The\n",
      "<bos>My favorite color is a large on the word “the. The day\n",
      "<bos>If 5*x + 3 = 13, then x is a large on the same time. The day.\n",
      "step 00050/00100 (50.00%) | loss: 7.526902 | grad norm: 2.7385 | lrm: 1.00 | dt: 122.37ms | tok/sec: 16,344 | mfu: -1.00 | total time: 0.08m\n",
      "step 00051/00100 (51.00%) | loss: 7.507307 | grad norm: 1.4860 | lrm: 1.00 | dt: 123.34ms | tok/sec: 16,215 | mfu: -1.00 | total time: 0.08m\n",
      "step 00052/00100 (52.00%) | loss: 7.504683 | grad norm: 1.1633 | lrm: 1.00 | dt: 122.87ms | tok/sec: 16,277 | mfu: -1.00 | total time: 0.09m\n",
      "step 00053/00100 (53.00%) | loss: 7.545003 | grad norm: 1.3956 | lrm: 1.00 | dt: 123.29ms | tok/sec: 16,221 | mfu: -1.00 | total time: 0.09m\n",
      "step 00054/00100 (54.00%) | loss: 7.489925 | grad norm: 1.7335 | lrm: 1.00 | dt: 123.14ms | tok/sec: 16,241 | mfu: -1.00 | total time: 0.09m\n",
      "step 00055/00100 (55.00%) | loss: 7.473414 | grad norm: 2.6223 | lrm: 1.00 | dt: 123.48ms | tok/sec: 16,197 | mfu: -1.00 | total time: 0.09m\n",
      "step 00056/00100 (56.00%) | loss: 7.475302 | grad norm: 1.4254 | lrm: 1.00 | dt: 123.79ms | tok/sec: 16,155 | mfu: -1.00 | total time: 0.09m\n",
      "step 00057/00100 (57.00%) | loss: 7.504075 | grad norm: 1.9262 | lrm: 1.00 | dt: 123.40ms | tok/sec: 16,207 | mfu: -1.00 | total time: 0.10m\n",
      "step 00058/00100 (58.00%) | loss: 7.447597 | grad norm: 1.5166 | lrm: 1.00 | dt: 123.84ms | tok/sec: 16,150 | mfu: -1.00 | total time: 0.10m\n",
      "step 00059/00100 (59.00%) | loss: 7.395489 | grad norm: 1.3432 | lrm: 1.00 | dt: 124.58ms | tok/sec: 16,053 | mfu: -1.00 | total time: 0.10m\n",
      "step 00060 | Validation bpb: 2.3436\n",
      "step 00060/00100 (60.00%) | loss: 7.372085 | grad norm: 1.4012 | lrm: 1.00 | dt: 124.95ms | tok/sec: 16,006 | mfu: -1.00 | total time: 0.10m\n",
      "step 00061/00100 (61.00%) | loss: 7.363614 | grad norm: 1.5805 | lrm: 1.00 | dt: 123.39ms | tok/sec: 16,208 | mfu: -1.00 | total time: 0.10m\n",
      "step 00062/00100 (62.00%) | loss: 7.350656 | grad norm: 1.5130 | lrm: 1.00 | dt: 123.10ms | tok/sec: 16,246 | mfu: -1.00 | total time: 0.11m\n",
      "step 00063/00100 (63.00%) | loss: 7.378762 | grad norm: 2.0140 | lrm: 1.00 | dt: 122.95ms | tok/sec: 16,266 | mfu: -1.00 | total time: 0.11m\n",
      "step 00064/00100 (64.00%) | loss: 7.370634 | grad norm: 1.3482 | lrm: 1.00 | dt: 122.92ms | tok/sec: 16,271 | mfu: -1.00 | total time: 0.11m\n",
      "step 00065/00100 (65.00%) | loss: 7.379942 | grad norm: 1.6338 | lrm: 1.00 | dt: 123.97ms | tok/sec: 16,133 | mfu: -1.00 | total time: 0.11m\n",
      "step 00066/00100 (66.00%) | loss: 7.364788 | grad norm: 1.9395 | lrm: 1.00 | dt: 124.29ms | tok/sec: 16,090 | mfu: -1.00 | total time: 0.11m\n",
      "step 00067/00100 (67.00%) | loss: 7.309784 | grad norm: 1.3425 | lrm: 1.00 | dt: 123.84ms | tok/sec: 16,149 | mfu: -1.00 | total time: 0.12m\n",
      "step 00068/00100 (68.00%) | loss: 7.288547 | grad norm: 1.7306 | lrm: 1.00 | dt: 123.96ms | tok/sec: 16,133 | mfu: -1.00 | total time: 0.12m\n",
      "step 00069/00100 (69.00%) | loss: 7.253218 | grad norm: 1.0398 | lrm: 1.00 | dt: 124.12ms | tok/sec: 16,113 | mfu: -1.00 | total time: 0.12m\n",
      "step 00070 | Validation bpb: 2.3380\n",
      "step 00070/00100 (70.00%) | loss: 7.273345 | grad norm: 1.8261 | lrm: 1.00 | dt: 124.78ms | tok/sec: 16,028 | mfu: -1.00 | total time: 0.12m\n",
      "step 00071/00100 (71.00%) | loss: 7.255781 | grad norm: 1.2201 | lrm: 1.00 | dt: 123.88ms | tok/sec: 16,144 | mfu: -1.00 | total time: 0.12m\n",
      "step 00072/00100 (72.00%) | loss: 7.253540 | grad norm: 1.7429 | lrm: 1.00 | dt: 124.16ms | tok/sec: 16,107 | mfu: -1.00 | total time: 0.13m\n",
      "step 00073/00100 (73.00%) | loss: 7.302127 | grad norm: 1.9725 | lrm: 1.00 | dt: 123.64ms | tok/sec: 16,175 | mfu: -1.00 | total time: 0.13m\n",
      "step 00074/00100 (74.00%) | loss: 7.342652 | grad norm: 2.2218 | lrm: 1.00 | dt: 123.60ms | tok/sec: 16,181 | mfu: -1.00 | total time: 0.13m\n",
      "step 00075/00100 (75.00%) | loss: 7.308905 | grad norm: 1.3962 | lrm: 1.00 | dt: 123.61ms | tok/sec: 16,179 | mfu: -1.00 | total time: 0.13m\n",
      "step 00076/00100 (76.00%) | loss: 7.361388 | grad norm: 3.0650 | lrm: 1.00 | dt: 124.46ms | tok/sec: 16,069 | mfu: -1.00 | total time: 0.13m\n",
      "step 00077/00100 (77.00%) | loss: 7.349094 | grad norm: 3.5853 | lrm: 1.00 | dt: 123.95ms | tok/sec: 16,134 | mfu: -1.00 | total time: 0.14m\n",
      "step 00078/00100 (78.00%) | loss: 7.343082 | grad norm: 3.5756 | lrm: 1.00 | dt: 124.63ms | tok/sec: 16,047 | mfu: -1.00 | total time: 0.14m\n",
      "step 00079/00100 (79.00%) | loss: 7.299697 | grad norm: 1.4992 | lrm: 1.00 | dt: 123.98ms | tok/sec: 16,132 | mfu: -1.00 | total time: 0.14m\n",
      "step 00080 | Validation bpb: 2.3610\n",
      "step 00080/00100 (80.00%) | loss: 7.330133 | grad norm: 1.6843 | lrm: 1.00 | dt: 124.75ms | tok/sec: 16,031 | mfu: -1.00 | total time: 0.14m\n",
      "step 00081/00100 (81.00%) | loss: 7.370850 | grad norm: 1.4982 | lrm: 0.95 | dt: 123.89ms | tok/sec: 16,143 | mfu: -1.00 | total time: 0.14m\n",
      "step 00082/00100 (82.00%) | loss: 7.397053 | grad norm: 1.4251 | lrm: 0.90 | dt: 123.92ms | tok/sec: 16,139 | mfu: -1.00 | total time: 0.15m\n",
      "step 00083/00100 (83.00%) | loss: 7.427249 | grad norm: 1.6995 | lrm: 0.85 | dt: 124.02ms | tok/sec: 16,125 | mfu: -1.00 | total time: 0.15m\n",
      "step 00084/00100 (84.00%) | loss: 7.415541 | grad norm: 1.0870 | lrm: 0.80 | dt: 124.10ms | tok/sec: 16,116 | mfu: -1.00 | total time: 0.15m\n",
      "step 00085/00100 (85.00%) | loss: 7.381865 | grad norm: 1.5640 | lrm: 0.75 | dt: 123.69ms | tok/sec: 16,168 | mfu: -1.00 | total time: 0.15m\n",
      "step 00086/00100 (86.00%) | loss: 7.387500 | grad norm: 1.2153 | lrm: 0.70 | dt: 123.49ms | tok/sec: 16,195 | mfu: -1.00 | total time: 0.16m\n",
      "step 00087/00100 (87.00%) | loss: 7.377464 | grad norm: 1.1023 | lrm: 0.65 | dt: 123.94ms | tok/sec: 16,136 | mfu: -1.00 | total time: 0.16m\n",
      "step 00088/00100 (88.00%) | loss: 7.354213 | grad norm: 0.8871 | lrm: 0.60 | dt: 124.74ms | tok/sec: 16,033 | mfu: -1.00 | total time: 0.16m\n",
      "step 00089/00100 (89.00%) | loss: 7.318971 | grad norm: 0.8642 | lrm: 0.55 | dt: 124.32ms | tok/sec: 16,087 | mfu: -1.00 | total time: 0.16m\n",
      "step 00090 | Validation bpb: 2.2834\n",
      "step 00090/00100 (90.00%) | loss: 7.300013 | grad norm: 0.8818 | lrm: 0.50 | dt: 124.76ms | tok/sec: 16,030 | mfu: -1.00 | total time: 0.16m\n",
      "step 00091/00100 (91.00%) | loss: 7.298484 | grad norm: 1.1653 | lrm: 0.45 | dt: 124.36ms | tok/sec: 16,082 | mfu: -1.00 | total time: 0.17m\n",
      "step 00092/00100 (92.00%) | loss: 7.276680 | grad norm: 0.9520 | lrm: 0.40 | dt: 124.04ms | tok/sec: 16,123 | mfu: -1.00 | total time: 0.17m\n",
      "step 00093/00100 (93.00%) | loss: 7.259159 | grad norm: 0.7814 | lrm: 0.35 | dt: 124.52ms | tok/sec: 16,061 | mfu: -1.00 | total time: 0.17m\n",
      "step 00094/00100 (94.00%) | loss: 7.313885 | grad norm: 1.3467 | lrm: 0.30 | dt: 124.19ms | tok/sec: 16,104 | mfu: -1.00 | total time: 0.17m\n",
      "step 00095/00100 (95.00%) | loss: 7.314012 | grad norm: 1.3676 | lrm: 0.25 | dt: 123.99ms | tok/sec: 16,130 | mfu: -1.00 | total time: 0.17m\n",
      "step 00096/00100 (96.00%) | loss: 7.311188 | grad norm: 1.0695 | lrm: 0.20 | dt: 124.08ms | tok/sec: 16,118 | mfu: -1.00 | total time: 0.18m\n",
      "step 00097/00100 (97.00%) | loss: 7.288311 | grad norm: 1.0851 | lrm: 0.15 | dt: 123.88ms | tok/sec: 16,144 | mfu: -1.00 | total time: 0.18m\n",
      "step 00098/00100 (98.00%) | loss: 7.255169 | grad norm: 1.1383 | lrm: 0.10 | dt: 124.61ms | tok/sec: 16,049 | mfu: -1.00 | total time: 0.18m\n",
      "step 00099/00100 (99.00%) | loss: 7.206910 | grad norm: 0.9241 | lrm: 0.05 | dt: 124.04ms | tok/sec: 16,124 | mfu: -1.00 | total time: 0.18m\n",
      "step 00100 | Validation bpb: 2.2750\n",
      "Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.2500 | centered: 0.0000 | time: 0.35s\n",
      "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.27s\n",
      "Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.26s\n",
      "Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.1500 | centered: -0.1333 | time: 0.57s\n",
      "Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.2000 | centered: -0.0667 | time: 0.69s\n",
      "Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.7500 | centered: 0.5000 | time: 0.16s\n",
      "Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.2000 | centered: 0.0000 | time: 0.77s\n",
      "Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.5500 | centered: 0.1000 | time: 0.39s\n",
      "Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.2500 | centered: 0.0000 | time: 0.17s\n",
      "Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.20s\n",
      "Evaluating: hellaswag (10-shot, type: multiple_choice)... accuracy: 0.2500 | centered: 0.0000 | time: 1.74s\n",
      "Evaluating: winograd (0-shot, type: schema)... accuracy: 0.6000 | centered: 0.2000 | time: 0.16s\n",
      "Evaluating: winogrande (0-shot, type: schema)... accuracy: 0.6500 | centered: 0.3000 | time: 0.16s\n",
      "Evaluating: bigbench_dyck_languages (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.31s\n",
      "Evaluating: agi_eval_lsat_ar (3-shot, type: multiple_choice)... accuracy: 0.1000 | centered: -0.1250 | time: 1.24s\n",
      "Evaluating: bigbench_cs_algorithms (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.31s\n",
      "Evaluating: bigbench_operators (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.27s\n",
      "Evaluating: bigbench_repeat_copy_logic (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.30s\n",
      "Evaluating: squad (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.98s\n",
      "Evaluating: coqa (0-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.38s\n",
      "Evaluating: boolq (10-shot, type: multiple_choice)... accuracy: 0.4000 | centered: -0.5789 | time: 1.61s\n",
      "Evaluating: bigbench_language_identification (10-shot, type: multiple_choice)... accuracy: 0.2000 | centered: 0.1199 | time: 3.03s\n",
      "Step 00100: CORE metric: 0.0144\n",
      "<bos>The capital of France is a few years, and the most of the most\n",
      "<bos>The chemical symbol of gold is a very good, and the most of the most\n",
      "<bos>If yesterday was Friday, then tomorrow will be a few days, and the most important to the\n",
      "<bos>The opposite of hot is a few days, and the most of the most\n",
      "<bos>The planets of the solar system are: The most of the most of the most of the\n",
      "<bos>My favorite color is a very good, and the most of the most\n",
      "<bos>If 5*x + 3 = 13, then x is a result of the most important to the most important\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000100.pt\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/model_000100.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d4/meta_000100.json\n",
      "Peak memory usage: 4583.22MiB\n",
      "Total training time: 0.18m\n",
      "Minimum validation bpb: 2.2750\n",
      "[W1114 14:48:02.059299478 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.my_base_train \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=1000 \\\n",
    "    --device_batch_size=2 \\\n",
    "    --num_iterations=100 \\\n",
    "    --total_batch_size=2000 \\\n",
    "    --eval_every=10 \\\n",
    "    --eval_tokens=4000 \\\n",
    "    --sample_every=50 \\\n",
    "    --core_metric_every=50 \\\n",
    "    --core_metric_max_per_task=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd830e2-dce3-4c23-b626-c64e5e1224b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
