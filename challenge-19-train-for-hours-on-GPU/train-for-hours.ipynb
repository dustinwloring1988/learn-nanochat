{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd5171d-90e2-41e1-8d3a-fd2c0a871260",
   "metadata": {},
   "source": [
    "I want to confirm I can train for hours on the GPU using `base_train.py`. I'll do a few quick 100 step tests to confirm that:\n",
    "\n",
    "- The new code added in the last few challenges is working correctly in this environment and on the GPU\n",
    "- I know how to use tmux and can end my ssh session without killing the process\n",
    "- I'm correctly redirecting stdout and stderr to a file\n",
    "- I can load a checkpoint\n",
    "- I've chosen config params that won't OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fefabf72-b884-4c36-9d74-5497d14b4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "import os\n",
    "import torch\n",
    "from my_nanochat.my_common import get_base_dir, autodetect_device_type\n",
    "from my_nanochat.my_checkpoint_manager import build_model\n",
    "from contextlib import nullcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc79d1be-b3c3-46c5-981c-4ac63699b5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n"
     ]
    }
   ],
   "source": [
    "device_type = autodetect_device_type()\n",
    "device = torch.device(device_type)\n",
    "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a1d400-3725-4946-a896-d399f0aa9e9d",
   "metadata": {},
   "source": [
    "In tmux shell:\n",
    "\n",
    "```\n",
    "source .venv/bin/activate\n",
    "\n",
    "cd challenge-19-train-for-hours-on-GPU\n",
    "\n",
    "export PYTHONPATH=../my_nanochat/\n",
    "\n",
    "python -m scripts.my_base_train --depth=10 --max_seq_len=400 --device_batch_size=2 --num_iterations=100 --total_batch_size=800 --eval_every=10 --eval_tokens=8000 > base_train_output_001.txt 2>&1\n",
    "```\n",
    "\n",
    "Instructions for later: If playing with torch / model in notebook, interrupt notebook kernel before running training from the shell so the notebook isn't holding onto GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e95ba314-f6f9-4bb7-94d7-dd4e83530947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 00095/00100 (95.00%) | loss: 7.347408 | grad norm: 1.4555 | lrm: 0.25 | dt: 409.17ms | tok/sec: 1,955 | mfu: -1.00 | total time: 0.57m\n",
      "step 00096/00100 (96.00%) | loss: 7.397736 | grad norm: 3.1600 | lrm: 0.20 | dt: 409.63ms | tok/sec: 1,953 | mfu: -1.00 | total time: 0.58m\n",
      "step 00097/00100 (97.00%) | loss: 7.439689 | grad norm: 3.1425 | lrm: 0.15 | dt: 409.14ms | tok/sec: 1,955 | mfu: -1.00 | total time: 0.58m\n",
      "step 00098/00100 (98.00%) | loss: 7.463372 | grad norm: 2.2365 | lrm: 0.10 | dt: 408.96ms | tok/sec: 1,956 | mfu: -1.00 | total time: 0.59m\n",
      "step 00099/00100 (99.00%) | loss: 7.462241 | grad norm: 1.7811 | lrm: 0.05 | dt: 410.58ms | tok/sec: 1,948 | mfu: -1.00 | total time: 0.60m\n",
      "step 00100 | Validation bpb: 2.7558\n",
      "TODO evaluate CORE metric\n",
      "TODO sample\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d10/model_000100.pt\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d10/model_000100.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d10/meta_000100.json\n",
      "Peak memory usage: 2334.81MiB\n",
      "Total training time: 0.60m\n",
      "Minimum validation bpb: 2.7558\n",
      "[W1112 01:58:51.519875793 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!tail -15 base_train_output_001.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e52db3-1ce0-4236-874d-cc713dc69317",
   "metadata": {},
   "source": [
    "Looks it ran fine. Try to load the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70be4084-7faa-4011-a4dc-a3d5eafbb827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with config: {'sequence_len': 400, 'vocab_size': 65537, 'n_layer': 10, 'n_head': 5, 'n_kv_head': 5, 'n_embd': 640}\n",
      "The person,| is| of\n",
      "He went to the| be| \n",
      "1 + 2 = 20|2|3\n",
      "first of the| water| a\n",
      "3 cats and 2.|,| and\n",
      "mom and smart| resiliency| water\n",
      "the red,| and|.\n",
      "She,|.| of\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = os.path.join(get_base_dir(), \"base_checkpoints\", \"d10\")\n",
    "model, tokenizer, meta_data = build_model(checkpoint_dir, step=100, device=device, phase=\"eval\")\n",
    "bos_token_id = tokenizer.get_bos_token_id()\n",
    "with torch.no_grad():\n",
    "    for prompt in ['The person', 'He went to', '1 + 2 = ', 'first of', '3 cats and 2', 'mom and', 'the red', 'She']:\n",
    "        with autocast_ctx:\n",
    "            logits = model(torch.tensor([tokenizer.encode(prompt, prepend=bos_token_id)], device=device)).detach()\n",
    "            top_3_next_tokens = torch.topk(logits[0,-1,:], k=3).indices\n",
    "            print(f\"{prompt}{'|'.join([tokenizer.decode([token]) for token in top_3_next_tokens])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc5ff38-95c1-4fb6-a246-9e6aede4d2a9",
   "metadata": {},
   "source": [
    "Try a few more configurations still with just 100 steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6946281-ea1b-44f5-a309-d7fbef79f62a",
   "metadata": {},
   "source": [
    "```\n",
    "python -m scripts.my_base_train --depth=12 --max_seq_len=600 --device_batch_size=2 --num_iterations=100 --total_batch_size=1200 --eval_every=10 --eval_tokens=8000 > base_train_output_002.txt 2>&1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f125b046-497f-4b98-8449-20ee6d499dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 00095/00100 (95.00%) | loss: 7.628609 | grad norm: 2.4277 | lrm: 0.25 | dt: 832.14ms | tok/sec: 1,442 | mfu: -1.00 | total time: 1.16m\n",
      "step 00096/00100 (96.00%) | loss: 7.576799 | grad norm: 1.8703 | lrm: 0.20 | dt: 831.21ms | tok/sec: 1,443 | mfu: -1.00 | total time: 1.17m\n",
      "step 00097/00100 (97.00%) | loss: 7.524256 | grad norm: 1.9663 | lrm: 0.15 | dt: 833.41ms | tok/sec: 1,439 | mfu: -1.00 | total time: 1.19m\n",
      "step 00098/00100 (98.00%) | loss: 7.467676 | grad norm: 1.6936 | lrm: 0.10 | dt: 832.56ms | tok/sec: 1,441 | mfu: -1.00 | total time: 1.20m\n",
      "step 00099/00100 (99.00%) | loss: 7.425730 | grad norm: 1.4603 | lrm: 0.05 | dt: 833.03ms | tok/sec: 1,440 | mfu: -1.00 | total time: 1.21m\n",
      "step 00100 | Validation bpb: 2.6234\n",
      "TODO evaluate CORE metric\n",
      "TODO sample\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d12/model_000100.pt\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d12/model_000100.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d12/meta_000100.json\n",
      "Peak memory usage: 3655.33MiB\n",
      "Total training time: 1.21m\n",
      "Minimum validation bpb: 2.6234\n",
      "[W1112 02:21:22.939348657 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!tail -15 base_train_output_002.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ef217-812f-4209-bf2e-138d51a42694",
   "metadata": {},
   "source": [
    "```\n",
    "python -m scripts.my_base_train --depth=12 --max_seq_len=800 --device_batch_size=2 --num_iterations=100 --total_batch_size=1600 --eval_every=10 --eval_tokens=8000 > base_train_output_003.txt 2>&1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659d7569-98ad-4c03-9f3f-18ba64745ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 00095/00100 (95.00%) | loss: 7.574343 | grad norm: 3.1946 | lrm: 0.25 | dt: 949.87ms | tok/sec: 1,684 | mfu: -1.00 | total time: 1.33m\n",
      "step 00096/00100 (96.00%) | loss: 7.604295 | grad norm: 2.6211 | lrm: 0.20 | dt: 950.84ms | tok/sec: 1,682 | mfu: -1.00 | total time: 1.34m\n",
      "step 00097/00100 (97.00%) | loss: 7.578888 | grad norm: 4.6293 | lrm: 0.15 | dt: 950.04ms | tok/sec: 1,684 | mfu: -1.00 | total time: 1.36m\n",
      "step 00098/00100 (98.00%) | loss: 7.519093 | grad norm: 2.2574 | lrm: 0.10 | dt: 951.97ms | tok/sec: 1,680 | mfu: -1.00 | total time: 1.37m\n",
      "step 00099/00100 (99.00%) | loss: 7.469648 | grad norm: 1.7135 | lrm: 0.05 | dt: 951.25ms | tok/sec: 1,682 | mfu: -1.00 | total time: 1.39m\n",
      "step 00100 | Validation bpb: 2.6396\n",
      "TODO evaluate CORE metric\n",
      "TODO sample\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d12/model_000100.pt\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d12/model_000100.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d12/meta_000100.json\n",
      "Peak memory usage: 4366.14MiB\n",
      "Total training time: 1.39m\n",
      "Minimum validation bpb: 2.6396\n",
      "[W1112 02:26:04.796365526 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!tail -15 base_train_output_003.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8f9e0dd-9c32-4639-a799-fe801defc4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with config: {'sequence_len': 800, 'vocab_size': 65537, 'n_layer': 12, 'n_head': 6, 'n_kv_head': 6, 'n_embd': 768}\n",
      "The person was| were|,\n",
      "He went to the| take| \n",
      "1 + 2 = 20|17|10\n",
      "first of the| | a\n",
      "3 cats and 2/|,|–\n",
      "mom and | the| a\n",
      "the red in| from|,\n",
      "Sheia|,| and\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = os.path.join(get_base_dir(), \"base_checkpoints\", \"d12\")\n",
    "model, tokenizer, meta_data = build_model(checkpoint_dir, step=100, device=device, phase=\"eval\")\n",
    "bos_token_id = tokenizer.get_bos_token_id()\n",
    "with torch.no_grad():\n",
    "    for prompt in ['The person', 'He went to', '1 + 2 = ', 'first of', '3 cats and 2', 'mom and', 'the red', 'She']:\n",
    "        with autocast_ctx:\n",
    "            logits = model(torch.tensor([tokenizer.encode(prompt, prepend=bos_token_id)], device=device)).detach()\n",
    "            top_3_next_tokens = torch.topk(logits[0,-1,:], k=3).indices\n",
    "            print(f\"{prompt}{'|'.join([tokenizer.decode([token]) for token in top_3_next_tokens])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c408921-6837-4cbc-be5b-0f8bfba5f68e",
   "metadata": {},
   "source": [
    "```\n",
    "python -m scripts.my_base_train --depth=12 --max_seq_len=1000 --device_batch_size=2 --num_iterations=100 --total_batch_size=2000 --eval_every=10 --eval_tokens=8000 > base_train_output_004.txt 2>&1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e17074e-5648-4634-98af-96cb4019d4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 00095/00100 (95.00%) | loss: 7.455878 | grad norm: 1.5860 | lrm: 0.25 | dt: 1071.99ms | tok/sec: 1,865 | mfu: -1.00 | total time: 1.49m\n",
      "step 00096/00100 (96.00%) | loss: 7.446316 | grad norm: 1.4359 | lrm: 0.20 | dt: 1069.92ms | tok/sec: 1,869 | mfu: -1.00 | total time: 1.51m\n",
      "step 00097/00100 (97.00%) | loss: 7.415680 | grad norm: 1.1721 | lrm: 0.15 | dt: 1070.60ms | tok/sec: 1,868 | mfu: -1.00 | total time: 1.53m\n",
      "step 00098/00100 (98.00%) | loss: 7.382005 | grad norm: 1.2093 | lrm: 0.10 | dt: 1071.05ms | tok/sec: 1,867 | mfu: -1.00 | total time: 1.55m\n",
      "step 00099/00100 (99.00%) | loss: 7.330871 | grad norm: 1.1584 | lrm: 0.05 | dt: 1069.64ms | tok/sec: 1,869 | mfu: -1.00 | total time: 1.56m\n",
      "step 00100 | Validation bpb: 2.5969\n",
      "TODO evaluate CORE metric\n",
      "TODO sample\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d12/model_000100.pt\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d12/model_000100.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d12/meta_000100.json\n",
      "Peak memory usage: 5120.90MiB\n",
      "Total training time: 1.56m\n",
      "Minimum validation bpb: 2.5969\n",
      "[W1112 02:32:03.729165022 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!tail -15 base_train_output_004.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe288ab-ab00-47d1-b5d4-868a881d0acd",
   "metadata": {},
   "source": [
    "### hours long run\n",
    "\n",
    "Here's the config for the long run. First do 100 steps to figure out timing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e3a5ac-36df-436b-9da2-4be8a3e996ce",
   "metadata": {},
   "source": [
    "```\n",
    "python -m scripts.my_base_train --depth=12 --max_seq_len=1000 --device_batch_size=3 --num_iterations=100 --total_batch_size=3000 --eval_every=50 --eval_tokens=8000 > base_train_output_005.txt 2>&1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76f2cb25-867e-49fe-8e75-d383e3ec4dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 00095/00100 (95.00%) | loss: 7.256703 | grad norm: 1.0395 | lrm: 0.25 | dt: 1341.12ms | tok/sec: 2,236 | mfu: -1.00 | total time: 1.88m\n",
      "step 00096/00100 (96.00%) | loss: 7.224554 | grad norm: 0.9416 | lrm: 0.20 | dt: 1343.25ms | tok/sec: 2,233 | mfu: -1.00 | total time: 1.90m\n",
      "step 00097/00100 (97.00%) | loss: 7.211367 | grad norm: 1.3675 | lrm: 0.15 | dt: 1340.66ms | tok/sec: 2,237 | mfu: -1.00 | total time: 1.93m\n",
      "step 00098/00100 (98.00%) | loss: 7.195680 | grad norm: 1.0433 | lrm: 0.10 | dt: 1341.77ms | tok/sec: 2,235 | mfu: -1.00 | total time: 1.95m\n",
      "step 00099/00100 (99.00%) | loss: 7.287336 | grad norm: 2.1298 | lrm: 0.05 | dt: 1340.90ms | tok/sec: 2,237 | mfu: -1.00 | total time: 1.97m\n",
      "step 00100 | Validation bpb: 2.3093\n",
      "TODO evaluate CORE metric\n",
      "TODO sample\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d12/model_000100.pt\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d12/model_000100.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d12/meta_000100.json\n",
      "Peak memory usage: 6785.41MiB\n",
      "Total training time: 1.97m\n",
      "Minimum validation bpb: 2.3093\n",
      "[W1112 02:36:27.444664766 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!tail -15 base_train_output_005.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfa7e76-a7da-47fe-9f2f-873d151cc1e8",
   "metadata": {},
   "source": [
    "So let's say we do those settings and train for 8 hours. We could complete this many iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74686a82-b4f6-4f65-9361-b78e1b52a7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21333.333333333332"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8 * 60 * 60 * 1000 / 1350"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00173ece-922f-40fa-9ef2-c9c71a48e2f2",
   "metadata": {},
   "source": [
    "#### start run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0253eaba-bafb-46c0-bd03-1377372d5353",
   "metadata": {},
   "source": [
    "```\n",
    "python -m scripts.my_base_train --depth=12 --max_seq_len=1000 --device_batch_size=3 --num_iterations=21000 --total_batch_size=3000 --eval_every=100 --eval_tokens=9000 > base_train_output_006.txt 2>&1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a22f48f-a62c-4f10-a93c-53184a589eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20995/21000 (99.98%) | loss: 4.733284 | grad norm: 0.5301 | lrm: 0.00 | dt: 1340.62ms | tok/sec: 2,237 | mfu: -1.00 | total time: 468.66m\n",
      "step 20996/21000 (99.98%) | loss: 4.764493 | grad norm: 0.4904 | lrm: 0.00 | dt: 1337.63ms | tok/sec: 2,242 | mfu: -1.00 | total time: 468.69m\n",
      "step 20997/21000 (99.99%) | loss: 4.822114 | grad norm: 0.5699 | lrm: 0.00 | dt: 1340.07ms | tok/sec: 2,238 | mfu: -1.00 | total time: 468.71m\n",
      "step 20998/21000 (99.99%) | loss: 4.887818 | grad norm: 0.7300 | lrm: 0.00 | dt: 1340.34ms | tok/sec: 2,238 | mfu: -1.00 | total time: 468.73m\n",
      "step 20999/21000 (100.00%) | loss: 4.906813 | grad norm: 0.5375 | lrm: 0.00 | dt: 1340.44ms | tok/sec: 2,238 | mfu: -1.00 | total time: 468.75m\n",
      "step 21000 | Validation bpb: 1.6605\n",
      "TODO evaluate CORE metric\n",
      "TODO sample\n",
      "saved model to /home/paperspace/.cache/my_nanochat/base_checkpoints/d12/model_021000.pt\n",
      "saved optimizer to /home/paperspace/.cache/my_nanochat/base_checkpoints/d12/model_021000.pt\n",
      "saved metadata to /home/paperspace/.cache/my_nanochat/base_checkpoints/d12/meta_021000.json\n",
      "Peak memory usage: 6785.41MiB\n",
      "Total training time: 468.75m\n",
      "Minimum validation bpb: 1.6605\n",
      "[W1112 10:36:41.410016193 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!tail -15 base_train_output_006.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0751f1b7-0f14-4de8-85c6-d50af20a2600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.816666666666666"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "469 / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d728bb05-dd97-4269-89b5-d825eb5d4f1c",
   "metadata": {},
   "source": [
    "^ It completed in the expected time. Time per step is so consistent. Maybe this is another nice thing about using a GPU where nothing else is sharing, interrupting, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fad710a-a85c-46c7-9a91-43bc5395acac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 paperspace paperspace  775 Nov 12 10:36 meta_021000.json\n",
      "-rw-rw-r-- 1 paperspace paperspace 613M Nov 12 10:36 model_021000.pt\n",
      "-rw-rw-r-- 1 paperspace paperspace 901M Nov 12 10:36 optim_021000.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /home/paperspace/.cache/my_nanochat/base_checkpoints/d12 | grep 21000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "160de619-7298-4dec-956d-304463ea1340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 00000 | Validation bpb: 3.7352\n",
      "step 00100 | Validation bpb: 2.7826\n",
      "step 00200 | Validation bpb: 2.6076\n",
      "step 00300 | Validation bpb: 2.5682\n",
      "step 00400 | Validation bpb: 2.5566\n",
      "grep: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!grep Validation base_train_output_006.txt | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26e169bc-779d-4238-9c92-fa377f8fb5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20200 | Validation bpb: 1.6699\n",
      "step 20300 | Validation bpb: 1.6677\n",
      "step 20400 | Validation bpb: 1.6664\n",
      "step 20500 | Validation bpb: 1.6643\n",
      "step 20600 | Validation bpb: 1.6627\n",
      "step 20700 | Validation bpb: 1.6609\n",
      "step 20800 | Validation bpb: 1.6610\n",
      "step 20900 | Validation bpb: 1.6612\n",
      "step 21000 | Validation bpb: 1.6605\n",
      "Minimum validation bpb: 1.6605\n"
     ]
    }
   ],
   "source": [
    "!grep -i Validation base_train_output_006.txt | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b7a7a-55ce-423c-b106-2232c335cc9b",
   "metadata": {},
   "source": [
    "^ From validation bpb it looks like training behaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ff7ec8f-6c1e-4df8-9775-36900899f546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 00900 | Validation bpb: 2.3005\n",
      "step 01900 | Validation bpb: 2.3223\n",
      "step 02900 | Validation bpb: 2.1992\n",
      "step 03900 | Validation bpb: 2.1091\n",
      "step 04900 | Validation bpb: 2.0760\n",
      "step 05900 | Validation bpb: 2.0606\n",
      "step 06900 | Validation bpb: 1.9630\n",
      "step 07900 | Validation bpb: 1.8506\n",
      "step 08900 | Validation bpb: 1.8735\n",
      "step 09900 | Validation bpb: 1.8573\n",
      "step 10900 | Validation bpb: 1.8685\n",
      "step 11900 | Validation bpb: 1.8520\n",
      "step 12900 | Validation bpb: 1.8543\n",
      "step 13900 | Validation bpb: 1.8200\n",
      "step 14900 | Validation bpb: 1.8304\n",
      "step 15900 | Validation bpb: 1.8280\n",
      "step 16900 | Validation bpb: 1.7717\n",
      "step 17900 | Validation bpb: 1.7440\n",
      "step 18900 | Validation bpb: 1.6767\n",
      "step 19900 | Validation bpb: 1.6658\n",
      "step 20900 | Validation bpb: 1.6612\n"
     ]
    }
   ],
   "source": [
    "!grep -i Validation base_train_output_006.txt | awk 'NR % 10 == 0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "daa9941c-bdb9-4dbc-9b52-b20a7882ddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 00999/21000 (4.76%) | loss: 6.194658\n",
      "step 01999/21000 (9.52%) | loss: 6.036501\n",
      "step 02999/21000 (14.28%) | loss: 5.881425\n",
      "step 03999/21000 (19.04%) | loss: 5.771294\n",
      "step 04999/21000 (23.80%) | loss: 5.630048\n",
      "step 05999/21000 (28.57%) | loss: 5.656877\n",
      "step 06999/21000 (33.33%) | loss: 5.477202\n",
      "step 07999/21000 (38.09%) | loss: 5.520780\n",
      "step 08999/21000 (42.85%) | loss: 5.365104\n",
      "step 09999/21000 (47.61%) | loss: 5.355028\n",
      "step 10999/21000 (52.38%) | loss: 5.312786\n",
      "step 11999/21000 (57.14%) | loss: 5.364322\n",
      "step 12999/21000 (61.90%) | loss: 5.363304\n",
      "step 13999/21000 (66.66%) | loss: 5.347305\n",
      "step 14999/21000 (71.42%) | loss: 5.287267\n",
      "step 15999/21000 (76.19%) | loss: 5.282485\n",
      "step 16999/21000 (80.95%) | loss: 5.140184\n",
      "step 17999/21000 (85.71%) | loss: 5.024324\n",
      "step 18999/21000 (90.47%) | loss: 4.944702\n",
      "step 19999/21000 (95.23%) | loss: 4.858329\n",
      "step 20999/21000 (100.00%) | loss: 4.906813\n"
     ]
    }
   ],
   "source": [
    "!grep -o 'step [0-9/()%.]*[^|]*| loss: [0-9.]*' base_train_output_006.txt | awk 'NR % 1000 == 0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abd6323-23d4-4356-851a-98805c4e1b74",
   "metadata": {},
   "source": [
    "^ And from training loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302cad86-cf8d-44e4-ae1c-8532dd174f33",
   "metadata": {},
   "source": [
    "Now the fun part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b2d7202-c4c2-4ec2-ad83-4130707f38ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with config: {'sequence_len': 1000, 'vocab_size': 65537, 'n_layer': 12, 'n_head': 6, 'n_kv_head': 6, 'n_embd': 768}\n",
      "The person who|’s| of\n",
      "He went to the| a| his\n",
      "1 + 2 = 1|2|3\n",
      "first of the| all| \n",
      "3 cats and 2 dogs| cats|\n",
      "\n",
      "mom and the| its| a\n",
      "the red line| light| blood\n",
      "She is| was| has\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = os.path.join(get_base_dir(), \"base_checkpoints\", \"d12\")\n",
    "model, tokenizer, meta_data = build_model(checkpoint_dir, step=21000, device=device, phase=\"eval\")\n",
    "bos_token_id = tokenizer.get_bos_token_id()\n",
    "with torch.no_grad():\n",
    "    for prompt in ['The person', 'He went to', '1 + 2 = ', 'first of', '3 cats and 2', 'mom and', 'the red', 'She']:\n",
    "        with autocast_ctx:\n",
    "            logits = model(torch.tensor([tokenizer.encode(prompt, prepend=bos_token_id)], device=device)).detach()\n",
    "            top_3_next_tokens = torch.topk(logits[0,-1,:], k=3).indices\n",
    "            print(f\"{prompt}{'|'.join([tokenizer.decode([token]) for token in top_3_next_tokens])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d066fe0-76df-4ed3-b767-a70131c84369",
   "metadata": {},
   "source": [
    "^ Finally, \"3 cats and 2 dogs\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9178e6f5-8791-47a8-b593-f92bfdad5e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt):\n",
    "    tokens = tokenizer.encode(prompt, prepend=bos_token_id)\n",
    "    for _ in range(20):\n",
    "        with autocast_ctx:\n",
    "            logits = model(torch.tensor([tokens], device=device))\n",
    "            tokens.append(logits[0,-1,:].argmax().item())\n",
    "    return tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1c855d8-a008-44cd-ae12-03b4265fd082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>The person who is a member of the family of the family of the family of the family of the family of'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"The person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "823c7bdd-9dc9-4ab7-b8ff-9af6dc6ba307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>3 cats and 2 dogs\\nThe first human being was born in the 19th century. The first human being was'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"3 cats and 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c918dae-09ba-4ef7-a752-3069cc68e7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>First take a right on main street, and then the next stop is to the left of the street. The next stop is to the'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"First take a right on main street\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eac5aaa-e0aa-459e-bd97-5f2e3354ad4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>The first president of the United States was the 19th president of the United States. The first president of the United States was the '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"The first president of the United States was\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdf071a7-f7e5-439d-ab21-06ebd66df06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>2 + 3 = 1 + 2 + 2 + 2 + 2 + 2 + 2'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"2 + 3 =\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c67cb819-5796-4d50-828b-cdf7b80686c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>A common breakfast food is a food that is not served as a food. It is a food that is not served as a'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"A common breakfast food is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "275a6498-daa5-485e-ac1f-b5ee630c9ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<bos>For breakfast, I'll have to go to the store, and I'll be able to get a little more of the energy you\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"For breakfast, I'll have\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b639d85-8252-4d88-878d-f6e0d220a38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>At breakfast I eat a lot of fruits and vegetables, and I am a little more than a little bit more than a'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"At breakfast I eat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cab11844-9a58-4c6d-9e48-91420feaf932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<bos>For breakfast I'll have two more minutes of lunch each day. I'll have a few more minutes of lunch each day. I\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"For breakfast I'll have two\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3346e5ac-f62a-4483-8e0c-07d8fc05a999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>My favorite breakfast foods are: A new study from the University of California, San Diego, and the University of California, San Diego'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"My favorite breakfast foods are:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93ace01d-7e79-4ae8-8cb7-11f8bfb14acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>My favorite breakfast foods are:\\n- 1.5% of the daily recommended daily intake of fruits and vegetables. This is'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"My favorite breakfast foods are\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c788b5d-970c-4c9f-bf7f-429752fd84d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>I like green eggs and my own friends. I like green eggs and my own friends. I like green eggs and my own'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"I like green eggs and\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5ad4caa-4990-4011-98cb-89b85c63050d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>if x > 1\\nThe answer is: 1.5.1.1.1.1.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"if x >\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5633c613-2533-496b-bac1-0ab66527a371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>print(1) 1: 1: 1: 1: 1: 1:'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"print(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50baddcb-498d-4f53-8d2f-26869e0a1eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>for i in the first place\\nThe first thing to do is to get a better understanding of the world around us'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"for i in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84f086e0-f6e5-41bb-967b-f865f3fa0413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>for i in range of 1,000\\nThe first thing to do is to get a 1,00'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"for i in range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ed615-b913-4843-a07f-47daaa3e6c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
