{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "592f5725-9dea-4d74-8428-662550d58d1d",
   "metadata": {},
   "source": [
    "This is not the main notebook in this challenge. See `midtrain-d20.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa551f2-156f-4059-99e4-1d1fa0eea253",
   "metadata": {},
   "source": [
    "## Investigate questions\n",
    "\n",
    "Investigate the 3 questions raised in `midtrain-d20.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102315a1-c0e8-4650-a43c-d2ad68a1d381",
   "metadata": {},
   "source": [
    "### 1) diff in bpb loss eval\n",
    "\n",
    "Why was final val bpb in base training (done in challenge 25) 0.8135 but the starting one in this mid training run was 0.6856?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addee07a-cbf4-48d1-b86d-f71fe0020421",
   "metadata": {},
   "source": [
    "In `my_base_train.py` bpb loss eval is done like this:\n",
    "\n",
    "```\n",
    "        val_loader = build_val_loader()\n",
    "        eval_steps = eval_tokens // (device_batch_size * max_seq_len * ddp_world_size)\n",
    "        with autocast_ctx:\n",
    "            val_bpb = evaluate_bpb(model, val_loader, eval_steps, token_bytes)\n",
    "```\n",
    "\n",
    "In `my_mid_train.py` bpb loss eval is done like this:\n",
    "\n",
    "```\n",
    "        val_loader = build_val_loader()\n",
    "        eval_steps = eval_tokens // (device_batch_size * max_seq_len * ddp_world_size)\n",
    "        with autocast_ctx:\n",
    "            val_bpb = evaluate_bpb(model, val_loader, eval_steps, token_bytes)\n",
    "```\n",
    "\n",
    "Ah, they shouldn't match. One is using base train validation data (tons and tons of text from the internet) and the other is using mid train validation data (the user / assistant conversations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10c24c4-cc5a-460e-b91f-535a4941621d",
   "metadata": {},
   "source": [
    "### 2) diff in CORE metric\n",
    "\n",
    "Why was the final CORE metric in base training 0.2084 but the one I measured here on the same model was 0.2012?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae2bbdf-e47c-4e6c-99bf-7f31d74f38c0",
   "metadata": {},
   "source": [
    "In `my_base_train.py` CORE metric is evaluated like this:\n",
    "\n",
    "```\n",
    "model.eval()\n",
    "        with autocast_ctx:\n",
    "            results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)\n",
    "        print0(f\"Step {step:05d}: CORE metric: {results['core_metric']:.4f}\")\n",
    "```\n",
    "\n",
    "And in `my_base_eval.py`:\n",
    "\n",
    "```\n",
    "    with autocast_ctx:\n",
    "        results = evaluate_model(model, tokenizer, device, max_per_task=args.max_per_task, tasks_to_run=tasks_to_run)\n",
    "\n",
    "    print0(f\"CORE metric: {results['core_metric']:.4f}\")\n",
    "```\n",
    "\n",
    "When I did the base train I didn't specify core_metric_max_per_task so it defaulted to 500, which I can nicely confirm in wandb (run=challenge-25-4). When I called my_base_eval I didn't specify max-per-task so it would have run all. I'm pretty sure that explains it. Before I was wondering why he ran base_eval after the training in speedrun.sh. This explains it: because the CORE metric being evaluated after the last step in training is only on a subset of tasks. I suppose if I really want to verify I can get back on the GPU machine and run my_base_eval with --max-per-task=500."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b7bc93-f285-4c2d-beb9-d8e3ca74e681",
   "metadata": {},
   "source": [
    "### SpellingBee high accuracy\n",
    "\n",
    "Can SpellingBee accuracy really be 96.88%?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d93f37-5df0-4b93-a972-09febe903bff",
   "metadata": {},
   "source": [
    "A few initial thoughts...\n",
    "\n",
    "- Make sure the train / test stuff is working and the model didn't memorize the 1000 examples\n",
    "- Would be nice to have an easy way to view conversations where it gets it wrong (and right). Perhaps add.\n",
    "- Try a few examples right now on my mac.\n",
    "- There is no python use yet. So for the model to do this it has to learn which tokens together form the word, how to spell each of them, how to count specific letters within that, and to output the answer at the end after \"###\"...could it really be doing it so well already?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e857b6a-d251-4c1a-b2d4-7f7bcf5c0a07",
   "metadata": {},
   "source": [
    "#### Check train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708bbc3f-a1b6-4053-84a5-e5edcf8e4b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_tokenizer import get_tokenizer\n",
    "from my_tasks.my_spellingbee import MySpellingBee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68fd633a-e909-489e-bb2b-f516058346ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task = MySpellingBee(split=\"train\")\n",
    "val_task = MySpellingBee(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d051f223-c75f-4212-bf4e-06a48e931f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count the number of 's' in \"nonclassifiable\"?\n",
      "\"cottonization\"にnは何個ありますか\n",
      "Find the count of 'a' in \"beaune\"?\n",
      "In 'giftedness', count the \"f\"\n",
      "Count the a in gentman?\n",
      "'touchy'に'u'が何回出てくる\n",
      "spookにoは何個ありますか\n",
      "lebistes라는 단어에 'm'가 몇 개?\n",
      "tell me the number of r in fretters?\n",
      "tell me the number of 'o' in pollocks?\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(train_task[i]['messages'][0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "320a3de8-39da-4239-b0b5-04906e7bf660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"cottonization\"にnは何個ありますか\n",
      "Find the count of 'a' in \"beaune\"?\n",
      "In 'giftedness', count the \"f\"\n",
      "Count the a in gentman?\n",
      "'touchy'に'u'が何回出てくる\n",
      "spookにoは何個ありますか\n",
      "lebistes라는 단어에 'm'가 몇 개?\n",
      "tell me the number of r in fretters?\n",
      "tell me the number of 'o' in pollocks?\n",
      "数一下\"sphygmuses\"中的\"s\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(val_task[i]['messages'][0]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beafc23f-3b1f-455a-b3cd-e2325fd13734",
   "metadata": {},
   "source": [
    "It's not right!\n",
    "\n",
    "Did I copy something wrong? My code:\n",
    "\n",
    "```\n",
    "        seed = index if self.split == 'train' else -(index + 1)\n",
    "        rng = random.Random(seed)\n",
    "```\n",
    "\n",
    "Code from [spellingbee.cpy](https://github.com/karpathy/nanochat/blob/master/tasks/spellingbee.py):\n",
    "\n",
    "```\n",
    "        seed = index if self.split == 'train' else -(index + 1)\n",
    "        rng = random.Random(seed)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ed9dc0d-be10-4f08-b94a-e0e103e53f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af63f0ef-3d3b-42ca-aed3-5402aba52223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, -6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 5\n",
    "train_seed = index\n",
    "test_seed = -(index + 1)\n",
    "train_seed, test_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6192e1bc-5935-4a0c-b3a0-f534eb075116",
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = list(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2443107-862b-40e8-98fd-cf36a423901e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "637"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = random.Random(5)\n",
    "rng.choice(choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e40a2b5-e359-48a5-8dfd-7d9d2d1dc27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "637"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = random.Random(-5)\n",
    "rng.choice(choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "701553d6-2c32-4173-ba97-730b77e6aed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = random.Random(6)\n",
    "rng.choice(choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2354aa01-621a-41f7-9345-9e67627490f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = random.Random(-6)\n",
    "rng.choice(choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3ee1bea-e7d5-4372-9d68-55aa313782f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6229016948897019"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.Random(5).random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7cdf740-f5e8-49f5-80f4-5fe3736ed515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6229016948897019"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.Random(-5).random()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68776613-37e6-4cfd-afec-35d6ce8ba556",
   "metadata": {},
   "source": [
    "So at least in this version of python, seeding with x and -x are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216f9bb2-471d-450c-b351-7cea19932df4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
