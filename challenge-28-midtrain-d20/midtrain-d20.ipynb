{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661e825c-20ff-4b37-ae93-3fcbfea939c1",
   "metadata": {},
   "source": [
    "I'll use lambda cloud. Assuming I can create a machine in us-south-2, my storage is already there so I won't need to download data files, scp the tokenizer and model, etc. Here are the instructions from `challenge-25-pretrain-d20/trying-lambda-cloud.ipynb` without the stuff I won't need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997bcd79-9da4-4e13-a33a-0698b4a70c61",
   "metadata": {},
   "source": [
    "```\n",
    "ssh ssh ubuntu@[ip]\n",
    "\n",
    "# ssh key for git\n",
    "ssh-keygen -t ed25519 -C \"lambda-cloud\"\n",
    "cat ~/.ssh/id_ed25519.pub\n",
    "copy into github UI (https://github.com/settings/keys)\n",
    "\n",
    "git config --global user.email \"ericsilberstein@gmail.com\"\n",
    "git config --global user.name \"Eric Silberstein\"\n",
    "\n",
    "# clone this repo\n",
    "git clone git@github.com:ericsilberstein1/nanogpt-learning.git\n",
    "\n",
    "# UV\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# rust\n",
    "curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n",
    "echo '. \"$HOME/.cargo/env\"' >> .bashrc\n",
    "\n",
    "echo 'export NANOCHAT_BASE_DIR=\"/home/ubuntu/mynanochat\"' >> .bashrc\n",
    "\n",
    "# in .bashrc add\n",
    "# export WANDB_API_KEY=\"XXX\"\n",
    "\n",
    "source .bashrc\n",
    "\n",
    "cd nanogpt-learning\n",
    "\n",
    "uv sync\n",
    "source .venv/bin/activate\n",
    "\n",
    "# for now until organize this better\n",
    "uv tool install maturin\n",
    "cd challenge-07-rust-and-python-simplified-tokenizer/rust_tokenizer\n",
    "maturin develop\n",
    "cd -\n",
    "\n",
    "# looks like lambda automatically runs jupyter but for now at least let me run it\n",
    "# in the way I understand\n",
    "uv run jupyter lab --port=7001\n",
    "jupyter server list\n",
    "\n",
    "# ON MY LAPTOP make a tunnel to jupyter\n",
    "ssh -N -L 7001:localhost:7001 ubuntu@[ip]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519ce58-3f42-42c4-a9d0-0c666380abd7",
   "metadata": {},
   "source": [
    "If my calculation in challenge 26 is right, training will be around 10 minutes, so I'll just run everything from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f89b98-57a3-4598-a459-6c338d6a8975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"../my_nanochat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f417d92a-f58d-44fd-bc9f-3728707219f6",
   "metadata": {},
   "source": [
    "First do a CORE evaluation. This is a sanity check. It should match the final eval from the training in challenge 24.\n",
    "\n",
    "```\n",
    "Step 21400: CORE metric: 0.2084\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca40da2-791a-4935-b059-6778e6e98e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_base_eval -- --source=base --model-tag=d20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01be01f3-4a53-415d-b23d-4ede3aae8476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53782fff-1b58-4c46-b38b-9473890eda9b",
   "metadata": {},
   "source": [
    "Do the midtraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff5d9fb-db15-4459-99d9-80b38495e99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_mid_train -- --model_tag=d20 --run=challenge-28-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec48fd01-9466-4970-a9fb-fbaf6b12b21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11a8a1ff-a287-49f9-a6dc-68b82be03bc9",
   "metadata": {},
   "source": [
    "Chat eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42db5a9e-9c6d-4fab-a872-6f0689a48345",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- --source=mid --model-tag=d20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e76c3-76a5-4954-96f6-98defc1c59d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9a4a760-2805-48a9-a5dd-3bb4dd95d7a0",
   "metadata": {},
   "source": [
    "Also run limited chat evals on base and mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e42e91-a08d-4302-972f-c0c50c9b67e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- --source=base --model-tag=d20 --max-problems=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d8825f-50f9-4be2-a844-fe4de903da5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f541b64d-b44e-432e-83e8-55da2230e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- --source=mid --model-tag=d20 --max-problems=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4876f88c-107f-46de-800a-e957ec3c1955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c95a0-d1b9-4f6d-adac-eeaeda4e954e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "514f14d6-c6a1-4482-8e81-004da4461cb8",
   "metadata": {},
   "source": [
    "Backup, in case decide to run in tmux shell:\n",
    "\n",
    "```\n",
    "source .venv/bin/activate\n",
    "\n",
    "cd challenge-28-midtrain-d20\n",
    "\n",
    "export PYTHONPATH=../my_nanochat/\n",
    "\n",
    "# sanity check, should match what we saw in training output of challenge 24\n",
    "# Step 21400: CORE metric: 0.2084\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.my_base_eval -- --source=base --model-tag=d20 > output_001.txt 2>&1\n",
    "\n",
    "# just to see\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- --source=base --model-tag=d20 --max-problems=100 > output_002.txt 2>&1\n",
    "\n",
    "# train\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.my_mid_train -- --model_tag=d20 --run=challenge-28-1 > output_003.txt 2>&1\n",
    "\n",
    "# repeat of 100 problem chat eval from above\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- --source=mid --model-tag=d20 --max-problems=100 > output_004.txt 2>&1\n",
    "\n",
    "# full chat eval\n",
    "torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- --source=mid --model-tag=d20 > coutput_005.txt 2>&1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062f5b1a-759c-403a-a235-a776c5757ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
