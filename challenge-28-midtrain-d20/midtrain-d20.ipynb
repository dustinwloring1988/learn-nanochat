{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661e825c-20ff-4b37-ae93-3fcbfea939c1",
   "metadata": {},
   "source": [
    "I'll use lambda cloud. Assuming I can create a machine in us-south-2, my storage is already there so I won't need to download data files, scp the tokenizer and model, etc. Here are the instructions from `challenge-25-pretrain-d20/trying-lambda-cloud.ipynb` without the stuff I won't need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997bcd79-9da4-4e13-a33a-0698b4a70c61",
   "metadata": {},
   "source": [
    "```\n",
    "ssh ssh ubuntu@[ip]\n",
    "\n",
    "# ssh key for git\n",
    "ssh-keygen -t ed25519 -C \"lambda-cloud\"\n",
    "cat ~/.ssh/id_ed25519.pub\n",
    "copy into github UI (https://github.com/settings/keys)\n",
    "\n",
    "git config --global user.email \"ericsilberstein@gmail.com\"\n",
    "git config --global user.name \"Eric Silberstein\"\n",
    "\n",
    "# clone this repo\n",
    "git clone git@github.com:ericsilberstein1/nanogpt-learning.git\n",
    "\n",
    "# UV\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# rust\n",
    "curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n",
    "echo '. \"$HOME/.cargo/env\"' >> .bashrc\n",
    "\n",
    "echo 'export NANOCHAT_BASE_DIR=\"/home/ubuntu/mynanochat\"' >> .bashrc\n",
    "\n",
    "# in .bashrc add\n",
    "# export WANDB_API_KEY=\"XXX\"\n",
    "\n",
    "source .bashrc\n",
    "\n",
    "cd nanogpt-learning\n",
    "\n",
    "uv sync\n",
    "source .venv/bin/activate\n",
    "\n",
    "# for now until organize this better\n",
    "uv tool install maturin\n",
    "cd challenge-07-rust-and-python-simplified-tokenizer/rust_tokenizer\n",
    "maturin develop\n",
    "cd -\n",
    "\n",
    "# looks like lambda automatically runs jupyter but for now at least let me run it\n",
    "# in the way I understand\n",
    "uv run jupyter lab --port=7001\n",
    "jupyter server list\n",
    "\n",
    "# ON MY LAPTOP make a tunnel to jupyter\n",
    "ssh -N -L 7001:localhost:7001 ubuntu@[ip]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8858295c-ba9d-402b-8e8c-27e225e97094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 19 12:55:14 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |\n",
      "| N/A   24C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |\n",
      "| N/A   26C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |\n",
      "| N/A   27C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |\n",
      "| N/A   24C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |\n",
      "| N/A   24C    P0             72W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |\n",
      "| N/A   27C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |\n",
      "| N/A   26C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |\n",
      "| N/A   25C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519ce58-3f42-42c4-a9d0-0c666380abd7",
   "metadata": {},
   "source": [
    "If my calculation in challenge 26 is right, training will be around 10 minutes, so I'll just run everything from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f89b98-57a3-4598-a459-6c338d6a8975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"../my_nanochat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f417d92a-f58d-44fd-bc9f-3728707219f6",
   "metadata": {},
   "source": [
    "First do a CORE evaluation. This is a sanity check. It should match the final eval from the training in challenge 24.\n",
    "\n",
    "```\n",
    "Step 21400: CORE metric: 0.2084\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca40da2-791a-4935-b059-6778e6e98e32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1119 12:55:24.046000 15698 torch/distributed/run.py:803] \n",
      "W1119 12:55:24.046000 15698 torch/distributed/run.py:803] *****************************************\n",
      "W1119 12:55:24.046000 15698 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1119 12:55:24.046000 15698 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/base_checkpoints/d20 with step 21400\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.4425 | centered: 0.2567 | time: 13.38s\n",
      "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.1186 | centered: 0.1186 | time: 3.10s\n",
      "Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.5366 | centered: 0.5366 | time: 27.08s\n",
      "Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.6469 | centered: 0.5292 | time: 4.13s\n",
      "Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.3515 | centered: 0.1354 | time: 1.97s\n",
      "Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.6900 | centered: 0.3800 | time: 0.19s\n",
      "Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.2301 | centered: 0.0377 | time: 2.13s\n",
      "Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.6795 | centered: 0.3591 | time: 3.18s\n",
      "Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.3600 | centered: 0.1467 | time: 0.74s\n",
      "Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.3745 | centered: 0.3745 | time: 6.83s\n",
      "Evaluating: hellaswag (10-shot, type: multiple_choice)... accuracy: 0.4471 | centered: 0.2628 | time: 25.97s\n",
      "Evaluating: winograd (0-shot, type: schema)... accuracy: 0.6410 | centered: 0.2821 | time: 0.38s\n",
      "Evaluating: winogrande (0-shot, type: schema)... accuracy: 0.5280 | centered: 0.0560 | time: 1.60s\n",
      "Evaluating: bigbench_dyck_languages (10-shot, type: language_modeling)... accuracy: 0.1020 | centered: 0.1020 | time: 1.74s\n",
      "Evaluating: agi_eval_lsat_ar (3-shot, type: multiple_choice)... accuracy: 0.2217 | centered: 0.0272 | time: 0.59s\n",
      "Evaluating: bigbench_cs_algorithms (10-shot, type: language_modeling)... accuracy: 0.3583 | centered: 0.3583 | time: 2.10s\n",
      "Evaluating: bigbench_operators (10-shot, type: language_modeling)... accuracy: 0.1667 | centered: 0.1667 | time: 0.36s\n",
      "Evaluating: bigbench_repeat_copy_logic (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.05s\n",
      "Evaluating: squad (10-shot, type: language_modeling)... accuracy: 0.2345 | centered: 0.2345 | time: 21.72s\n",
      "Evaluating: coqa (0-shot, type: language_modeling)... accuracy: 0.1813 | centered: 0.1813 | time: 11.87s\n",
      "Evaluating: boolq (10-shot, type: multiple_choice)... accuracy: 0.5070 | centered: -0.2973 | time: 8.13s\n",
      "Evaluating: bigbench_language_identification (10-shot, type: multiple_choice)... accuracy: 0.2530 | centered: 0.1782 | time: 41.23s\n",
      "CORE metric: 0.2012\n",
      "centered results:\n",
      "{\n",
      "    \"hellaswag_zeroshot\": 0.25672173500061035,\n",
      "    \"jeopardy\": 0.11856400221586227,\n",
      "    \"bigbench_qa_wikidata\": 0.5365877747535706,\n",
      "    \"arc_easy\": 0.5291806856791178,\n",
      "    \"arc_challenge\": 0.13538110256195068,\n",
      "    \"copa\": 0.3799999952316284,\n",
      "    \"commonsense_qa\": 0.03767403215169905,\n",
      "    \"piqa\": 0.35908591747283936,\n",
      "    \"openbook_qa\": 0.14666668574015299,\n",
      "    \"lambada_openai\": 0.3745391070842743,\n",
      "    \"hellaswag\": 0.26282942295074463,\n",
      "    \"winograd\": 0.28205132484436035,\n",
      "    \"winogrande\": 0.05603790283203125,\n",
      "    \"bigbench_dyck_languages\": 0.10200000554323196,\n",
      "    \"agi_eval_lsat_ar\": 0.027173891663551317,\n",
      "    \"bigbench_cs_algorithms\": 0.3583333194255829,\n",
      "    \"bigbench_operators\": 0.1666666716337204,\n",
      "    \"bigbench_repeat_copy_logic\": 0.0,\n",
      "    \"squad\": 0.2345316857099533,\n",
      "    \"coqa\": 0.18126018345355988,\n",
      "    \"boolq\": -0.2972798786665264,\n",
      "    \"bigbench_language_identification\": 0.1782178120775716\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_base_eval -- --source=base --model-tag=d20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dd7c26-88e5-4fed-9522-632bdfdf9803",
   "metadata": {},
   "source": [
    "^ Does not match. That's not good. Will move on and then go back and investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c391e2b2-8afb-4c84-9383-c4eed97f29a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01be01f3-4a53-415d-b23d-4ede3aae8476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53782fff-1b58-4c46-b38b-9473890eda9b",
   "metadata": {},
   "source": [
    "Do the midtraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff5d9fb-db15-4459-99d9-80b38495e99f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1119 13:02:34.300000 240329 torch/distributed/run.py:803] \n",
      "W1119 13:02:34.300000 240329 torch/distributed/run.py:803] *****************************************\n",
      "W1119 13:02:34.300000 240329 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1119 13:02:34.300000 240329 torch/distributed/run.py:803] *****************************************\n",
      "overriding model_tag = d20\n",
      "overriding run = challenge-28-1\n",
      "user_config: {'run': 'challenge-28-1', 'device_type': '', 'dtype': 'bfloat16', 'num_iterations': -1, 'max_seq_len': 2048, 'device_batch_size': 32, 'unembedding_lr': 0.004, 'embedding_lr': 0.2, 'matrix_lr': 0.02, 'init_lr_frac': 1.0, 'weight_decay': 0.0, 'eval_every': 150, 'eval_tokens': 10485760, 'total_batch_size': 524288, 'dry_run': 0}\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mericsilberstein\u001b[0m (\u001b[33mericsilberstein-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m setting up run 6srdpeb7 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/nanogpt-learning/challenge-28-midtrain-d20/wandb/run-20251119_130245-6srdpeb7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mchallenge-28-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat/runs/6srdpeb7\u001b[0m\n",
      "loading the model from /home/ubuntu/mynanochat/base_checkpoints/d20 with step 21400\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "Tokens / micro-batch / rank: 32 x 2048 = 65,536\n",
      "Tokens / micro-batch: 524,288\n",
      "Total batch size 524,288 => gradient accumulation steps: 1\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1280/768) = 0.7745966692414834\n",
      "Muon: Grouping 80 params of shape torch.Size([1280, 1280]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([1280, 5120]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([5120, 1280]), device cuda:0, dtype torch.float32\n",
      "README.md: 2.24kB [00:00, 8.45MB/s]\n",
      "data/train-00000-of-00004.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230M/230M [00:01<00:00, 117MB/s]\n",
      "data/train-00001-of-00004.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230M/230M [00:01<00:00, 142MB/s]\n",
      "data/train-00002-of-00004.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231M/231M [00:01<00:00, 183MB/s]\n",
      "data/train-00003-of-00004.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232M/232M [00:01<00:00, 230MB/s]\n",
      "data/test-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 48.2M/48.2M [00:00<00:00, 77.6MB/s]\n",
      "Generating train split: 100%|â–ˆ| 460341/460341 [00:03<00:00, 146948.21 examples/s\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆ| 24229/24229 [00:00<00:00, 153715.52 examples/s]\n",
      "README.md: 53.2kB [00:00, 129MB/s]\n",
      "dataset_infos.json: 138kB [00:00, 268MB/s]\n",
      "auxiliary_train/train-00000-of-00001.par(â€¦): 100%|â–ˆ| 47.5M/47.5M [00:00<00:00, 7\n",
      "Generating train split: 100%|â–ˆâ–ˆ| 99842/99842 [00:00<00:00, 448670.71 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:04<00:00, 21134.38 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:04<00:00, 20971.48 examples/s]\n",
      "README.md: 7.94kB [00:00, 28.9MB/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:04<00:00, 20933.58 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:04<00:00, 20277.28 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:05<00:00, 19832.36 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:05<00:00, 19622.80 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:05<00:00, 19552.85 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99842/99842 [00:05<00:00, 19050.59 examples/s]\n",
      "main/train-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆ| 2.31M/2.31M [00:00<00:00, 8.14MB/s]\n",
      "main/test-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419k/419k [00:00<00:00, 3.07MB/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 7473/7473 [00:00<00:00, 660235.79 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:00<00:00, 402260.38 examples/s]\n",
      "[rank5]: Traceback (most recent call last):\n",
      "[rank5]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank5]:     return _run_code(code, main_globals, None,\n",
      "[rank5]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank5]:     exec(code, run_globals)\n",
      "[rank5]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/scripts/my_mid_train.py\", line 98, in <module>\n",
      "[rank5]:     MyCustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations\n",
      "[rank5]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/my_tasks/my_customjson.py\", line 12, in __init__\n",
      "[rank5]:     with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
      "[rank5]: FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/mynanochat/identity_conversations.jsonl'\n",
      "[rank3]: Traceback (most recent call last):\n",
      "[rank3]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank3]:     return _run_code(code, main_globals, None,\n",
      "[rank3]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank3]:     exec(code, run_globals)\n",
      "[rank3]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/scripts/my_mid_train.py\", line 98, in <module>\n",
      "[rank3]:     MyCustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations\n",
      "[rank3]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/my_tasks/my_customjson.py\", line 12, in __init__\n",
      "[rank3]:     with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
      "[rank3]: FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/mynanochat/identity_conversations.jsonl'\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank1]:     return _run_code(code, main_globals, None,\n",
      "[rank1]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank1]:     exec(code, run_globals)\n",
      "[rank1]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/scripts/my_mid_train.py\", line 98, in <module>\n",
      "[rank1]:     MyCustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations\n",
      "[rank1]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/my_tasks/my_customjson.py\", line 12, in __init__\n",
      "[rank1]:     with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
      "[rank1]: FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/mynanochat/identity_conversations.jsonl'\n",
      "[rank7]: Traceback (most recent call last):\n",
      "[rank7]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank7]:     return _run_code(code, main_globals, None,\n",
      "[rank7]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank7]:     exec(code, run_globals)\n",
      "[rank7]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/scripts/my_mid_train.py\", line 98, in <module>\n",
      "[rank7]:     MyCustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations\n",
      "[rank7]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/my_tasks/my_customjson.py\", line 12, in __init__\n",
      "[rank7]:     with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
      "[rank7]: FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/mynanochat/identity_conversations.jsonl'\n",
      "[rank6]: Traceback (most recent call last):\n",
      "[rank6]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank6]:     return _run_code(code, main_globals, None,\n",
      "[rank6]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank6]:     exec(code, run_globals)\n",
      "[rank6]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/scripts/my_mid_train.py\", line 98, in <module>\n",
      "[rank6]:     MyCustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations\n",
      "[rank6]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/my_tasks/my_customjson.py\", line 12, in __init__\n",
      "[rank6]:     with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
      "[rank6]: FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/mynanochat/identity_conversations.jsonl'\n",
      "[rank2]: Traceback (most recent call last):\n",
      "[rank2]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank2]:     return _run_code(code, main_globals, None,\n",
      "[rank2]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank2]:     exec(code, run_globals)\n",
      "[rank2]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/scripts/my_mid_train.py\", line 98, in <module>\n",
      "[rank2]:     MyCustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations\n",
      "[rank2]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/my_tasks/my_customjson.py\", line 12, in __init__\n",
      "[rank2]:     with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
      "[rank2]: FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/mynanochat/identity_conversations.jsonl'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/nanogpt-learning/my_nanochat/scripts/my_mid_train.py\", line 98, in <module>\n",
      "    MyCustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations\n",
      "  File \"/home/ubuntu/nanogpt-learning/my_nanochat/my_tasks/my_customjson.py\", line 12, in __init__\n",
      "    with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/mynanochat/identity_conversations.jsonl'\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank0]:     return _run_code(code, main_globals, None,\n",
      "[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank0]:     exec(code, run_globals)\n",
      "[rank0]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/scripts/my_mid_train.py\", line 98, in <module>\n",
      "[rank0]:     MyCustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations\n",
      "[rank0]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/my_tasks/my_customjson.py\", line 12, in __init__\n",
      "[rank0]:     with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
      "[rank0]: FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/mynanochat/identity_conversations.jsonl'\n",
      "[rank4]: Traceback (most recent call last):\n",
      "[rank4]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "[rank4]:     return _run_code(code, main_globals, None,\n",
      "[rank4]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "[rank4]:     exec(code, run_globals)\n",
      "[rank4]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/scripts/my_mid_train.py\", line 98, in <module>\n",
      "[rank4]:     MyCustomJSON(filepath=identity_conversations_filepath), # 1000 rows of synthetic identity conversations\n",
      "[rank4]:   File \"/home/ubuntu/nanogpt-learning/my_nanochat/my_tasks/my_customjson.py\", line 12, in __init__\n",
      "[rank4]:     with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
      "[rank4]: FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/mynanochat/identity_conversations.jsonl'\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: ðŸš€ View run \u001b[33mchallenge-28-1\u001b[0m at: \u001b[34m\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251119_130245-6srdpeb7/logs\u001b[0m\n",
      "[rank3]:[W1119 13:03:08.967626658 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank0]:[W1119 13:03:08.104334164 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "[rank5]:[W1119 13:03:09.522907408 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank1]:[W1119 13:03:09.554252948 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank7]:[W1119 13:03:09.554535389 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank2]:[W1119 13:03:09.614750866 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank4]:[W1119 13:03:09.622136607 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank6]:[W1119 13:03:09.635130744 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[rank0]:[W1119 13:03:09.341390890 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "W1119 13:03:10.388000 240329 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 240428 closing signal SIGTERM\n",
      "W1119 13:03:10.389000 240329 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 240429 closing signal SIGTERM\n",
      "W1119 13:03:10.389000 240329 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 240430 closing signal SIGTERM\n",
      "W1119 13:03:10.389000 240329 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 240432 closing signal SIGTERM\n",
      "W1119 13:03:10.390000 240329 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 240433 closing signal SIGTERM\n",
      "W1119 13:03:10.390000 240329 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 240434 closing signal SIGTERM\n",
      "W1119 13:03:10.390000 240329 torch/distributed/elastic/multiprocessing/api.py:908] Sending process 240435 closing signal SIGTERM\n",
      "E1119 13:03:11.306000 240329 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: 1) local_rank: 3 (pid: 240431) of binary: /home/ubuntu/nanogpt-learning/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/nanogpt-learning/.venv/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 936, in main\n",
      "    run(args)\n",
      "  File \"/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/run.py\", line 927, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 156, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 293, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "scripts.my_mid_train FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-11-19_13:03:10\n",
      "  host      : 192-222-53-251\n",
      "  rank      : 3 (local_rank: 3)\n",
      "  exitcode  : 1 (pid: 240431)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_mid_train -- --model_tag=d20 --run=challenge-28-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bff3e2f-c79e-4e16-9908-3a052f8d1d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2235k  100 2235k    0     0  3982k      0 --:--:-- --:--:-- --:--:-- 3977k\n"
     ]
    }
   ],
   "source": [
    "!curl -L -o $NANOCHAT_BASE_DIR/identity_conversations.jsonl https://karpathy-public.s3.us-west-2.amazonaws.com/identity_conversations.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "866b1c4d-c1f6-480e-9587-abeac67bc6ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1119 13:06:40.457000 241353 torch/distributed/run.py:803] \n",
      "W1119 13:06:40.457000 241353 torch/distributed/run.py:803] *****************************************\n",
      "W1119 13:06:40.457000 241353 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1119 13:06:40.457000 241353 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "overriding model_tag = d20\n",
      "overriding run = challenge-28-2\n",
      "user_config: {'run': 'challenge-28-2', 'device_type': '', 'dtype': 'bfloat16', 'num_iterations': -1, 'max_seq_len': 2048, 'device_batch_size': 32, 'unembedding_lr': 0.004, 'embedding_lr': 0.2, 'matrix_lr': 0.02, 'init_lr_frac': 1.0, 'weight_decay': 0.0, 'eval_every': 150, 'eval_tokens': 10485760, 'total_batch_size': 524288, 'dry_run': 0}\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mericsilberstein\u001b[0m (\u001b[33mericsilberstein-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/nanogpt-learning/challenge-28-midtrain-d20/wandb/run-20251119_130651-hiwg9hf8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mchallenge-28-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat/runs/hiwg9hf8\u001b[0m\n",
      "loading the model from /home/ubuntu/mynanochat/base_checkpoints/d20 with step 21400\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "Tokens / micro-batch / rank: 32 x 2048 = 65,536\n",
      "Tokens / micro-batch: 524,288\n",
      "Total batch size 524,288 => gradient accumulation steps: 1\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(1280/768) = 0.7745966692414834\n",
      "Muon: Grouping 80 params of shape torch.Size([1280, 1280]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([1280, 5120]), device cuda:0, dtype torch.float32\n",
      "Muon: Grouping 20 params of shape torch.Size([5120, 1280]), device cuda:0, dtype torch.float32\n",
      "downloading https://raw.githubusercontent.com/dwyl/english-words/refs/heads/master/words_alpha.txt...\n",
      "downloaded to /home/ubuntu/mynanochat/words_alpha.txt\n",
      "all/test-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.50M/3.50M [00:00<00:00, 8.02MB/s]\n",
      "all/validation-00000-of-00001.parquet: 100%|â–ˆ| 408k/408k [00:00<00:00, 2.48MB/s]\n",
      "all/dev-00000-of-00001.parquet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76.5k/76.5k [00:00<00:00, 528kB/s]\n",
      "all/auxiliary_train-00000-of-00001.parqu(â€¦): 100%|â–ˆ| 47.5M/47.5M [00:00<00:00, 6\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆ| 14042/14042 [00:00<00:00, 656169.00 examples/s]\n",
      "Generating validation split: 100%|â–ˆ| 1531/1531 [00:00<00:00, 416573.43 examples/\n",
      "Generating dev split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [00:00<00:00, 163705.37 examples/s]\n",
      "Generating auxiliary_train split: 100%|â–ˆ| 99842/99842 [00:00<00:00, 473652.68 ex\n",
      "step 00000 | Validation bpb: 0.6856\n",
      "step 00001 (0.23%) | loss: 1.400719 | lrm: 1.00 | dt: 51538.59ms | tok/sec: 10,172 | mfu: -1.00 | total time: 0.00m\n",
      "step 00002 (0.37%) | loss: 1.747523 | lrm: 1.00 | dt: 647.26ms | tok/sec: 810,012 | mfu: -1.00 | total time: 0.00m\n",
      "step 00003 (0.51%) | loss: 1.902251 | lrm: 1.00 | dt: 461.36ms | tok/sec: 1,136,399 | mfu: -1.00 | total time: 0.00m\n",
      "step 00004 (0.63%) | loss: 1.963819 | lrm: 1.00 | dt: 528.19ms | tok/sec: 992,610 | mfu: -1.00 | total time: 0.00m\n",
      "step 00005 (0.75%) | loss: 1.975641 | lrm: 1.00 | dt: 462.28ms | tok/sec: 1,134,126 | mfu: -1.00 | total time: 0.00m\n",
      "step 00006 (0.86%) | loss: 1.975919 | lrm: 1.00 | dt: 468.65ms | tok/sec: 1,118,722 | mfu: -1.00 | total time: 0.00m\n",
      "step 00007 (0.99%) | loss: 1.967225 | lrm: 1.00 | dt: 532.39ms | tok/sec: 984,785 | mfu: -1.00 | total time: 0.00m\n",
      "step 00008 (1.12%) | loss: 1.970013 | lrm: 1.00 | dt: 525.83ms | tok/sec: 997,069 | mfu: -1.00 | total time: 0.00m\n",
      "step 00009 (1.24%) | loss: 1.955470 | lrm: 1.00 | dt: 463.13ms | tok/sec: 1,132,050 | mfu: -1.00 | total time: 0.00m\n",
      "step 00010 (1.36%) | loss: 1.926862 | lrm: 1.00 | dt: 470.36ms | tok/sec: 1,114,650 | mfu: -1.00 | total time: 0.00m\n",
      "step 00011 (1.48%) | loss: 1.921186 | lrm: 1.00 | dt: 521.07ms | tok/sec: 1,006,172 | mfu: -1.00 | total time: 0.01m\n",
      "step 00012 (1.60%) | loss: 1.905569 | lrm: 1.00 | dt: 464.75ms | tok/sec: 1,128,096 | mfu: -1.00 | total time: 0.02m\n",
      "step 00013 (1.72%) | loss: 1.892437 | lrm: 1.00 | dt: 525.61ms | tok/sec: 997,483 | mfu: -1.00 | total time: 0.03m\n",
      "step 00014 (1.85%) | loss: 1.875717 | lrm: 1.00 | dt: 467.57ms | tok/sec: 1,121,300 | mfu: -1.00 | total time: 0.03m\n",
      "step 00015 (1.99%) | loss: 1.863180 | lrm: 1.00 | dt: 471.09ms | tok/sec: 1,112,926 | mfu: -1.00 | total time: 0.04m\n",
      "step 00016 (2.12%) | loss: 1.848839 | lrm: 1.00 | dt: 539.35ms | tok/sec: 972,078 | mfu: -1.00 | total time: 0.05m\n",
      "step 00017 (2.25%) | loss: 1.820703 | lrm: 1.00 | dt: 528.39ms | tok/sec: 992,242 | mfu: -1.00 | total time: 0.06m\n",
      "step 00018 (2.37%) | loss: 1.803278 | lrm: 1.00 | dt: 465.03ms | tok/sec: 1,127,426 | mfu: -1.00 | total time: 0.07m\n",
      "step 00019 (2.50%) | loss: 1.787878 | lrm: 1.00 | dt: 531.31ms | tok/sec: 986,782 | mfu: -1.00 | total time: 0.08m\n",
      "step 00020 (2.62%) | loss: 1.774139 | lrm: 1.00 | dt: 463.80ms | tok/sec: 1,130,409 | mfu: -1.00 | total time: 0.08m\n",
      "step 00021 (2.75%) | loss: 1.758026 | lrm: 1.00 | dt: 585.14ms | tok/sec: 896,011 | mfu: -1.00 | total time: 0.09m\n",
      "step 00022 (2.88%) | loss: 1.735044 | lrm: 1.00 | dt: 527.05ms | tok/sec: 994,759 | mfu: -1.00 | total time: 0.10m\n",
      "step 00023 (2.98%) | loss: 1.713869 | lrm: 1.00 | dt: 464.04ms | tok/sec: 1,129,835 | mfu: -1.00 | total time: 0.11m\n",
      "step 00024 (3.09%) | loss: 1.705340 | lrm: 1.00 | dt: 471.60ms | tok/sec: 1,111,714 | mfu: -1.00 | total time: 0.12m\n",
      "step 00025 (3.21%) | loss: 1.720073 | lrm: 1.00 | dt: 537.52ms | tok/sec: 975,375 | mfu: -1.00 | total time: 0.13m\n",
      "step 00026 (3.34%) | loss: 1.720567 | lrm: 1.00 | dt: 465.41ms | tok/sec: 1,126,497 | mfu: -1.00 | total time: 0.13m\n",
      "step 00027 (3.47%) | loss: 1.718257 | lrm: 1.00 | dt: 472.27ms | tok/sec: 1,110,146 | mfu: -1.00 | total time: 0.14m\n",
      "step 00028 (3.58%) | loss: 1.706081 | lrm: 1.00 | dt: 529.63ms | tok/sec: 989,919 | mfu: -1.00 | total time: 0.15m\n",
      "step 00029 (3.69%) | loss: 1.698468 | lrm: 1.00 | dt: 465.79ms | tok/sec: 1,125,582 | mfu: -1.00 | total time: 0.16m\n",
      "step 00030 (3.83%) | loss: 1.693783 | lrm: 1.00 | dt: 526.15ms | tok/sec: 996,466 | mfu: -1.00 | total time: 0.17m\n",
      "step 00031 (3.95%) | loss: 1.690893 | lrm: 1.00 | dt: 464.81ms | tok/sec: 1,127,973 | mfu: -1.00 | total time: 0.17m\n",
      "step 00032 (4.08%) | loss: 1.677007 | lrm: 1.00 | dt: 475.18ms | tok/sec: 1,103,339 | mfu: -1.00 | total time: 0.18m\n",
      "step 00033 (4.20%) | loss: 1.689859 | lrm: 1.00 | dt: 475.51ms | tok/sec: 1,102,590 | mfu: -1.00 | total time: 0.19m\n",
      "step 00034 (4.33%) | loss: 1.681781 | lrm: 1.00 | dt: 472.77ms | tok/sec: 1,108,970 | mfu: -1.00 | total time: 0.20m\n",
      "step 00035 (4.43%) | loss: 1.673353 | lrm: 1.00 | dt: 473.36ms | tok/sec: 1,107,577 | mfu: -1.00 | total time: 0.21m\n",
      "step 00036 (4.56%) | loss: 1.666692 | lrm: 1.00 | dt: 471.92ms | tok/sec: 1,110,958 | mfu: -1.00 | total time: 0.21m\n",
      "step 00037 (4.70%) | loss: 1.664981 | lrm: 1.00 | dt: 475.16ms | tok/sec: 1,103,393 | mfu: -1.00 | total time: 0.22m\n",
      "step 00038 (4.82%) | loss: 1.658996 | lrm: 1.00 | dt: 615.74ms | tok/sec: 851,472 | mfu: -1.00 | total time: 0.23m\n",
      "step 00039 (4.94%) | loss: 1.655778 | lrm: 1.00 | dt: 523.17ms | tok/sec: 1,002,136 | mfu: -1.00 | total time: 0.24m\n",
      "step 00040 (5.07%) | loss: 1.653775 | lrm: 1.00 | dt: 465.44ms | tok/sec: 1,126,442 | mfu: -1.00 | total time: 0.25m\n",
      "step 00041 (5.16%) | loss: 1.657090 | lrm: 1.00 | dt: 473.71ms | tok/sec: 1,106,772 | mfu: -1.00 | total time: 0.26m\n",
      "step 00042 (5.31%) | loss: 1.640446 | lrm: 1.00 | dt: 474.59ms | tok/sec: 1,104,718 | mfu: -1.00 | total time: 0.26m\n",
      "step 00043 (5.43%) | loss: 1.636663 | lrm: 1.00 | dt: 469.55ms | tok/sec: 1,116,575 | mfu: -1.00 | total time: 0.27m\n",
      "step 00044 (5.59%) | loss: 1.636674 | lrm: 1.00 | dt: 475.86ms | tok/sec: 1,101,770 | mfu: -1.00 | total time: 0.28m\n",
      "step 00045 (5.70%) | loss: 1.626831 | lrm: 1.00 | dt: 473.61ms | tok/sec: 1,107,003 | mfu: -1.00 | total time: 0.29m\n",
      "step 00046 (5.83%) | loss: 1.621240 | lrm: 1.00 | dt: 534.64ms | tok/sec: 980,641 | mfu: -1.00 | total time: 0.30m\n",
      "step 00047 (5.95%) | loss: 1.620626 | lrm: 1.00 | dt: 528.21ms | tok/sec: 992,579 | mfu: -1.00 | total time: 0.31m\n",
      "step 00048 (6.07%) | loss: 1.615212 | lrm: 1.00 | dt: 466.15ms | tok/sec: 1,124,727 | mfu: -1.00 | total time: 0.31m\n",
      "step 00049 (6.19%) | loss: 1.610728 | lrm: 1.00 | dt: 536.32ms | tok/sec: 977,560 | mfu: -1.00 | total time: 0.32m\n",
      "step 00050 (6.32%) | loss: 1.603117 | lrm: 1.00 | dt: 527.73ms | tok/sec: 993,486 | mfu: -1.00 | total time: 0.33m\n",
      "step 00051 (6.44%) | loss: 1.601942 | lrm: 1.00 | dt: 526.95ms | tok/sec: 994,944 | mfu: -1.00 | total time: 0.34m\n",
      "step 00052 (6.59%) | loss: 1.602345 | lrm: 1.00 | dt: 587.84ms | tok/sec: 891,895 | mfu: -1.00 | total time: 0.35m\n",
      "step 00053 (6.71%) | loss: 1.593852 | lrm: 1.00 | dt: 465.42ms | tok/sec: 1,126,491 | mfu: -1.00 | total time: 0.36m\n",
      "step 00054 (6.85%) | loss: 1.587035 | lrm: 1.00 | dt: 475.10ms | tok/sec: 1,103,527 | mfu: -1.00 | total time: 0.37m\n",
      "step 00055 (6.98%) | loss: 1.577263 | lrm: 1.00 | dt: 474.47ms | tok/sec: 1,105,008 | mfu: -1.00 | total time: 0.37m\n",
      "step 00056 (7.09%) | loss: 1.573517 | lrm: 1.00 | dt: 472.76ms | tok/sec: 1,109,001 | mfu: -1.00 | total time: 0.38m\n",
      "step 00057 (7.23%) | loss: 1.559000 | lrm: 1.00 | dt: 474.20ms | tok/sec: 1,105,626 | mfu: -1.00 | total time: 0.39m\n",
      "step 00058 (7.34%) | loss: 1.561112 | lrm: 1.00 | dt: 471.89ms | tok/sec: 1,111,027 | mfu: -1.00 | total time: 0.40m\n",
      "step 00059 (7.45%) | loss: 1.568527 | lrm: 1.00 | dt: 528.80ms | tok/sec: 991,458 | mfu: -1.00 | total time: 0.41m\n",
      "step 00060 (7.57%) | loss: 1.574243 | lrm: 1.00 | dt: 468.06ms | tok/sec: 1,120,125 | mfu: -1.00 | total time: 0.41m\n",
      "step 00061 (7.67%) | loss: 1.586518 | lrm: 1.00 | dt: 541.06ms | tok/sec: 968,992 | mfu: -1.00 | total time: 0.42m\n",
      "step 00062 (7.81%) | loss: 1.582291 | lrm: 1.00 | dt: 466.88ms | tok/sec: 1,122,965 | mfu: -1.00 | total time: 0.43m\n",
      "step 00063 (7.93%) | loss: 1.579162 | lrm: 1.00 | dt: 474.89ms | tok/sec: 1,104,021 | mfu: -1.00 | total time: 0.44m\n",
      "step 00064 (8.05%) | loss: 1.562610 | lrm: 1.00 | dt: 481.62ms | tok/sec: 1,088,596 | mfu: -1.00 | total time: 0.45m\n",
      "step 00065 (8.18%) | loss: 1.562059 | lrm: 1.00 | dt: 469.76ms | tok/sec: 1,116,069 | mfu: -1.00 | total time: 0.45m\n",
      "step 00066 (8.30%) | loss: 1.569454 | lrm: 1.00 | dt: 478.50ms | tok/sec: 1,095,682 | mfu: -1.00 | total time: 0.46m\n",
      "step 00067 (8.42%) | loss: 1.578914 | lrm: 1.00 | dt: 470.85ms | tok/sec: 1,113,493 | mfu: -1.00 | total time: 0.47m\n",
      "step 00068 (8.55%) | loss: 1.569824 | lrm: 1.00 | dt: 535.87ms | tok/sec: 978,379 | mfu: -1.00 | total time: 0.48m\n",
      "step 00069 (8.67%) | loss: 1.565320 | lrm: 1.00 | dt: 527.89ms | tok/sec: 993,178 | mfu: -1.00 | total time: 0.49m\n",
      "step 00070 (8.81%) | loss: 1.564662 | lrm: 1.00 | dt: 466.58ms | tok/sec: 1,123,672 | mfu: -1.00 | total time: 0.50m\n",
      "step 00071 (8.93%) | loss: 1.564871 | lrm: 1.00 | dt: 473.76ms | tok/sec: 1,106,648 | mfu: -1.00 | total time: 0.50m\n",
      "step 00072 (9.04%) | loss: 1.567935 | lrm: 1.00 | dt: 477.22ms | tok/sec: 1,098,632 | mfu: -1.00 | total time: 0.51m\n",
      "step 00073 (9.19%) | loss: 1.557514 | lrm: 1.00 | dt: 594.12ms | tok/sec: 882,468 | mfu: -1.00 | total time: 0.52m\n",
      "step 00074 (9.32%) | loss: 1.554494 | lrm: 1.00 | dt: 518.33ms | tok/sec: 1,011,503 | mfu: -1.00 | total time: 0.53m\n",
      "step 00075 (9.44%) | loss: 1.551050 | lrm: 1.00 | dt: 541.10ms | tok/sec: 968,924 | mfu: -1.00 | total time: 0.54m\n",
      "step 00076 (9.56%) | loss: 1.543270 | lrm: 1.00 | dt: 582.41ms | tok/sec: 900,201 | mfu: -1.00 | total time: 0.55m\n",
      "step 00077 (9.68%) | loss: 1.527433 | lrm: 1.00 | dt: 467.28ms | tok/sec: 1,121,993 | mfu: -1.00 | total time: 0.56m\n",
      "step 00078 (9.82%) | loss: 1.529635 | lrm: 1.00 | dt: 537.10ms | tok/sec: 976,144 | mfu: -1.00 | total time: 0.57m\n",
      "step 00079 (9.93%) | loss: 1.531968 | lrm: 1.00 | dt: 525.66ms | tok/sec: 997,393 | mfu: -1.00 | total time: 0.57m\n",
      "step 00080 (10.06%) | loss: 1.529968 | lrm: 1.00 | dt: 521.31ms | tok/sec: 1,005,719 | mfu: -1.00 | total time: 0.58m\n",
      "step 00081 (10.19%) | loss: 1.523328 | lrm: 1.00 | dt: 464.85ms | tok/sec: 1,127,865 | mfu: -1.00 | total time: 0.59m\n",
      "step 00082 (10.31%) | loss: 1.525329 | lrm: 1.00 | dt: 473.17ms | tok/sec: 1,108,028 | mfu: -1.00 | total time: 0.60m\n",
      "step 00083 (10.46%) | loss: 1.513258 | lrm: 1.00 | dt: 474.78ms | tok/sec: 1,104,275 | mfu: -1.00 | total time: 0.61m\n",
      "step 00084 (10.58%) | loss: 1.518929 | lrm: 1.00 | dt: 470.30ms | tok/sec: 1,114,784 | mfu: -1.00 | total time: 0.61m\n",
      "step 00085 (10.71%) | loss: 1.508393 | lrm: 1.00 | dt: 477.50ms | tok/sec: 1,097,985 | mfu: -1.00 | total time: 0.62m\n",
      "step 00086 (10.82%) | loss: 1.492033 | lrm: 1.00 | dt: 472.28ms | tok/sec: 1,110,118 | mfu: -1.00 | total time: 0.63m\n",
      "step 00087 (10.95%) | loss: 1.496561 | lrm: 1.00 | dt: 472.07ms | tok/sec: 1,110,625 | mfu: -1.00 | total time: 0.64m\n",
      "step 00088 (11.06%) | loss: 1.492397 | lrm: 1.00 | dt: 475.40ms | tok/sec: 1,102,833 | mfu: -1.00 | total time: 0.65m\n",
      "step 00089 (11.18%) | loss: 1.492160 | lrm: 1.00 | dt: 471.77ms | tok/sec: 1,111,319 | mfu: -1.00 | total time: 0.65m\n",
      "step 00090 (11.29%) | loss: 1.477997 | lrm: 1.00 | dt: 596.73ms | tok/sec: 878,601 | mfu: -1.00 | total time: 0.66m\n",
      "step 00091 (11.41%) | loss: 1.482413 | lrm: 1.00 | dt: 468.47ms | tok/sec: 1,119,145 | mfu: -1.00 | total time: 0.67m\n",
      "step 00092 (11.53%) | loss: 1.475598 | lrm: 1.00 | dt: 593.37ms | tok/sec: 883,570 | mfu: -1.00 | total time: 0.68m\n",
      "step 00093 (11.65%) | loss: 1.472640 | lrm: 1.00 | dt: 578.10ms | tok/sec: 906,911 | mfu: -1.00 | total time: 0.69m\n",
      "step 00094 (11.78%) | loss: 1.474808 | lrm: 1.00 | dt: 523.88ms | tok/sec: 1,000,775 | mfu: -1.00 | total time: 0.70m\n",
      "step 00095 (11.91%) | loss: 1.479519 | lrm: 1.00 | dt: 615.94ms | tok/sec: 851,204 | mfu: -1.00 | total time: 0.71m\n",
      "step 00096 (12.03%) | loss: 1.474898 | lrm: 1.00 | dt: 525.38ms | tok/sec: 997,919 | mfu: -1.00 | total time: 0.72m\n",
      "step 00097 (12.14%) | loss: 1.484096 | lrm: 1.00 | dt: 579.94ms | tok/sec: 904,045 | mfu: -1.00 | total time: 0.73m\n",
      "step 00098 (12.26%) | loss: 1.486963 | lrm: 1.00 | dt: 525.67ms | tok/sec: 997,378 | mfu: -1.00 | total time: 0.74m\n",
      "step 00099 (12.38%) | loss: 1.483838 | lrm: 1.00 | dt: 465.17ms | tok/sec: 1,127,095 | mfu: -1.00 | total time: 0.75m\n",
      "step 00100 (12.49%) | loss: 1.482958 | lrm: 1.00 | dt: 473.50ms | tok/sec: 1,107,255 | mfu: -1.00 | total time: 0.75m\n",
      "step 00101 (12.62%) | loss: 1.482401 | lrm: 1.00 | dt: 482.99ms | tok/sec: 1,085,504 | mfu: -1.00 | total time: 0.76m\n",
      "step 00102 (12.75%) | loss: 1.478752 | lrm: 1.00 | dt: 472.42ms | tok/sec: 1,109,780 | mfu: -1.00 | total time: 0.77m\n",
      "step 00103 (12.87%) | loss: 1.481096 | lrm: 1.00 | dt: 536.30ms | tok/sec: 977,593 | mfu: -1.00 | total time: 0.78m\n",
      "step 00104 (13.00%) | loss: 1.475181 | lrm: 1.00 | dt: 526.41ms | tok/sec: 995,964 | mfu: -1.00 | total time: 0.79m\n",
      "step 00105 (13.10%) | loss: 1.470203 | lrm: 1.00 | dt: 613.67ms | tok/sec: 854,347 | mfu: -1.00 | total time: 0.80m\n",
      "step 00106 (13.21%) | loss: 1.467948 | lrm: 1.00 | dt: 467.60ms | tok/sec: 1,121,242 | mfu: -1.00 | total time: 0.80m\n",
      "step 00107 (13.34%) | loss: 1.468807 | lrm: 1.00 | dt: 477.68ms | tok/sec: 1,097,566 | mfu: -1.00 | total time: 0.81m\n",
      "step 00108 (13.45%) | loss: 1.468944 | lrm: 1.00 | dt: 700.47ms | tok/sec: 748,484 | mfu: -1.00 | total time: 0.82m\n",
      "step 00109 (13.57%) | loss: 1.464914 | lrm: 1.00 | dt: 581.26ms | tok/sec: 901,983 | mfu: -1.00 | total time: 0.83m\n",
      "step 00110 (13.70%) | loss: 1.463361 | lrm: 1.00 | dt: 467.42ms | tok/sec: 1,121,662 | mfu: -1.00 | total time: 0.84m\n",
      "step 00111 (13.81%) | loss: 1.456152 | lrm: 1.00 | dt: 523.38ms | tok/sec: 1,001,739 | mfu: -1.00 | total time: 0.85m\n",
      "step 00112 (13.93%) | loss: 1.451192 | lrm: 1.00 | dt: 520.97ms | tok/sec: 1,006,372 | mfu: -1.00 | total time: 0.86m\n",
      "step 00113 (14.07%) | loss: 1.456900 | lrm: 1.00 | dt: 465.27ms | tok/sec: 1,126,846 | mfu: -1.00 | total time: 0.87m\n",
      "step 00114 (14.19%) | loss: 1.472748 | lrm: 1.00 | dt: 519.20ms | tok/sec: 1,009,792 | mfu: -1.00 | total time: 0.88m\n",
      "step 00115 (14.32%) | loss: 1.464826 | lrm: 1.00 | dt: 524.62ms | tok/sec: 999,359 | mfu: -1.00 | total time: 0.88m\n",
      "step 00116 (14.47%) | loss: 1.469439 | lrm: 1.00 | dt: 465.42ms | tok/sec: 1,126,489 | mfu: -1.00 | total time: 0.89m\n",
      "step 00117 (14.59%) | loss: 1.471613 | lrm: 1.00 | dt: 472.58ms | tok/sec: 1,109,409 | mfu: -1.00 | total time: 0.90m\n",
      "step 00118 (14.71%) | loss: 1.464132 | lrm: 1.00 | dt: 532.73ms | tok/sec: 984,151 | mfu: -1.00 | total time: 0.91m\n",
      "step 00119 (14.83%) | loss: 1.462122 | lrm: 1.00 | dt: 608.92ms | tok/sec: 861,013 | mfu: -1.00 | total time: 0.92m\n",
      "step 00120 (14.96%) | loss: 1.459069 | lrm: 1.00 | dt: 684.09ms | tok/sec: 766,396 | mfu: -1.00 | total time: 0.93m\n",
      "step 00121 (15.07%) | loss: 1.456990 | lrm: 1.00 | dt: 602.64ms | tok/sec: 869,988 | mfu: -1.00 | total time: 0.94m\n",
      "step 00122 (15.21%) | loss: 1.451830 | lrm: 1.00 | dt: 528.62ms | tok/sec: 991,801 | mfu: -1.00 | total time: 0.95m\n",
      "step 00123 (15.35%) | loss: 1.450719 | lrm: 1.00 | dt: 466.08ms | tok/sec: 1,124,896 | mfu: -1.00 | total time: 0.96m\n",
      "step 00124 (15.47%) | loss: 1.454913 | lrm: 1.00 | dt: 538.05ms | tok/sec: 974,417 | mfu: -1.00 | total time: 0.97m\n",
      "step 00125 (15.58%) | loss: 1.445208 | lrm: 1.00 | dt: 528.15ms | tok/sec: 992,680 | mfu: -1.00 | total time: 0.97m\n",
      "step 00126 (15.69%) | loss: 1.444891 | lrm: 1.00 | dt: 573.44ms | tok/sec: 914,279 | mfu: -1.00 | total time: 0.98m\n",
      "step 00127 (15.82%) | loss: 1.440246 | lrm: 1.00 | dt: 521.83ms | tok/sec: 1,004,701 | mfu: -1.00 | total time: 0.99m\n",
      "step 00128 (15.96%) | loss: 1.435527 | lrm: 1.00 | dt: 464.45ms | tok/sec: 1,128,835 | mfu: -1.00 | total time: 1.00m\n",
      "step 00129 (16.09%) | loss: 1.440259 | lrm: 1.00 | dt: 596.48ms | tok/sec: 878,968 | mfu: -1.00 | total time: 1.01m\n",
      "step 00130 (16.22%) | loss: 1.439946 | lrm: 1.00 | dt: 673.21ms | tok/sec: 778,792 | mfu: -1.00 | total time: 1.02m\n",
      "step 00131 (16.34%) | loss: 1.453542 | lrm: 1.00 | dt: 519.75ms | tok/sec: 1,008,724 | mfu: -1.00 | total time: 1.03m\n",
      "step 00132 (16.46%) | loss: 1.447675 | lrm: 1.00 | dt: 531.56ms | tok/sec: 986,325 | mfu: -1.00 | total time: 1.04m\n",
      "step 00133 (16.58%) | loss: 1.445934 | lrm: 1.00 | dt: 532.61ms | tok/sec: 984,369 | mfu: -1.00 | total time: 1.05m\n",
      "step 00134 (16.69%) | loss: 1.451379 | lrm: 1.00 | dt: 466.39ms | tok/sec: 1,124,139 | mfu: -1.00 | total time: 1.06m\n",
      "step 00135 (16.81%) | loss: 1.437797 | lrm: 1.00 | dt: 476.17ms | tok/sec: 1,101,049 | mfu: -1.00 | total time: 1.06m\n",
      "step 00136 (16.94%) | loss: 1.445775 | lrm: 1.00 | dt: 618.81ms | tok/sec: 847,245 | mfu: -1.00 | total time: 1.07m\n",
      "step 00137 (17.09%) | loss: 1.443333 | lrm: 1.00 | dt: 528.64ms | tok/sec: 991,764 | mfu: -1.00 | total time: 1.08m\n",
      "step 00138 (17.20%) | loss: 1.439898 | lrm: 1.00 | dt: 528.34ms | tok/sec: 992,334 | mfu: -1.00 | total time: 1.09m\n",
      "step 00139 (17.33%) | loss: 1.448909 | lrm: 1.00 | dt: 678.72ms | tok/sec: 772,466 | mfu: -1.00 | total time: 1.10m\n",
      "step 00140 (17.45%) | loss: 1.441460 | lrm: 1.00 | dt: 597.31ms | tok/sec: 877,748 | mfu: -1.00 | total time: 1.11m\n",
      "step 00141 (17.56%) | loss: 1.436493 | lrm: 1.00 | dt: 657.03ms | tok/sec: 797,969 | mfu: -1.00 | total time: 1.12m\n",
      "step 00142 (17.69%) | loss: 1.429735 | lrm: 1.00 | dt: 684.51ms | tok/sec: 765,937 | mfu: -1.00 | total time: 1.14m\n",
      "step 00143 (17.79%) | loss: 1.426973 | lrm: 1.00 | dt: 526.08ms | tok/sec: 996,592 | mfu: -1.00 | total time: 1.14m\n",
      "step 00144 (17.92%) | loss: 1.431134 | lrm: 1.00 | dt: 464.02ms | tok/sec: 1,129,888 | mfu: -1.00 | total time: 1.15m\n",
      "step 00145 (18.04%) | loss: 1.431802 | lrm: 1.00 | dt: 540.15ms | tok/sec: 970,626 | mfu: -1.00 | total time: 1.16m\n",
      "step 00146 (18.17%) | loss: 1.434683 | lrm: 1.00 | dt: 523.95ms | tok/sec: 1,000,638 | mfu: -1.00 | total time: 1.17m\n",
      "step 00147 (18.30%) | loss: 1.442457 | lrm: 1.00 | dt: 597.18ms | tok/sec: 877,946 | mfu: -1.00 | total time: 1.18m\n",
      "step 00148 (18.40%) | loss: 1.444358 | lrm: 1.00 | dt: 466.89ms | tok/sec: 1,122,939 | mfu: -1.00 | total time: 1.19m\n",
      "step 00149 (18.52%) | loss: 1.443610 | lrm: 1.00 | dt: 535.25ms | tok/sec: 979,518 | mfu: -1.00 | total time: 1.20m\n",
      "step 00150 (18.64%) | loss: 1.441184 | lrm: 1.00 | dt: 581.48ms | tok/sec: 901,649 | mfu: -1.00 | total time: 1.21m\n",
      "step 00150 | Validation bpb: 0.4493\n",
      "step 00151 (18.76%) | loss: 1.433998 | lrm: 1.00 | dt: 550.58ms | tok/sec: 952,244 | mfu: -1.00 | total time: 1.22m\n",
      "step 00152 (18.89%) | loss: 1.432243 | lrm: 1.00 | dt: 467.52ms | tok/sec: 1,121,422 | mfu: -1.00 | total time: 1.22m\n",
      "step 00153 (19.01%) | loss: 1.439116 | lrm: 1.00 | dt: 533.71ms | tok/sec: 982,347 | mfu: -1.00 | total time: 1.23m\n",
      "step 00154 (19.13%) | loss: 1.434504 | lrm: 1.00 | dt: 585.77ms | tok/sec: 895,047 | mfu: -1.00 | total time: 1.24m\n",
      "step 00155 (19.25%) | loss: 1.429834 | lrm: 1.00 | dt: 541.76ms | tok/sec: 967,757 | mfu: -1.00 | total time: 1.25m\n",
      "step 00156 (19.40%) | loss: 1.428446 | lrm: 1.00 | dt: 465.01ms | tok/sec: 1,127,466 | mfu: -1.00 | total time: 1.26m\n",
      "step 00157 (19.52%) | loss: 1.419102 | lrm: 1.00 | dt: 607.20ms | tok/sec: 863,455 | mfu: -1.00 | total time: 1.27m\n",
      "step 00158 (19.66%) | loss: 1.412782 | lrm: 1.00 | dt: 588.04ms | tok/sec: 891,584 | mfu: -1.00 | total time: 1.28m\n",
      "step 00159 (19.79%) | loss: 1.410320 | lrm: 1.00 | dt: 533.28ms | tok/sec: 983,142 | mfu: -1.00 | total time: 1.29m\n",
      "step 00160 (19.92%) | loss: 1.417150 | lrm: 1.00 | dt: 581.74ms | tok/sec: 901,243 | mfu: -1.00 | total time: 1.30m\n",
      "step 00161 (20.06%) | loss: 1.411545 | lrm: 1.00 | dt: 589.52ms | tok/sec: 889,348 | mfu: -1.00 | total time: 1.31m\n",
      "step 00162 (20.19%) | loss: 1.400109 | lrm: 1.00 | dt: 466.19ms | tok/sec: 1,124,618 | mfu: -1.00 | total time: 1.31m\n",
      "step 00163 (20.31%) | loss: 1.397685 | lrm: 1.00 | dt: 529.35ms | tok/sec: 990,432 | mfu: -1.00 | total time: 1.32m\n",
      "step 00164 (20.43%) | loss: 1.394038 | lrm: 1.00 | dt: 527.52ms | tok/sec: 993,877 | mfu: -1.00 | total time: 1.33m\n",
      "step 00165 (20.58%) | loss: 1.387320 | lrm: 1.00 | dt: 522.30ms | tok/sec: 1,003,808 | mfu: -1.00 | total time: 1.34m\n",
      "step 00166 (20.69%) | loss: 1.390896 | lrm: 1.00 | dt: 466.37ms | tok/sec: 1,124,200 | mfu: -1.00 | total time: 1.35m\n",
      "step 00167 (20.80%) | loss: 1.388544 | lrm: 1.00 | dt: 533.22ms | tok/sec: 983,246 | mfu: -1.00 | total time: 1.36m\n",
      "step 00168 (20.94%) | loss: 1.399336 | lrm: 1.00 | dt: 527.70ms | tok/sec: 993,541 | mfu: -1.00 | total time: 1.37m\n",
      "step 00169 (21.07%) | loss: 1.395493 | lrm: 1.00 | dt: 612.86ms | tok/sec: 855,471 | mfu: -1.00 | total time: 1.38m\n",
      "step 00170 (21.18%) | loss: 1.402471 | lrm: 1.00 | dt: 466.50ms | tok/sec: 1,123,870 | mfu: -1.00 | total time: 1.38m\n",
      "step 00171 (21.30%) | loss: 1.398608 | lrm: 1.00 | dt: 533.44ms | tok/sec: 982,844 | mfu: -1.00 | total time: 1.39m\n",
      "step 00172 (21.42%) | loss: 1.398628 | lrm: 1.00 | dt: 531.02ms | tok/sec: 987,319 | mfu: -1.00 | total time: 1.40m\n",
      "step 00173 (21.54%) | loss: 1.396857 | lrm: 1.00 | dt: 525.58ms | tok/sec: 997,539 | mfu: -1.00 | total time: 1.41m\n",
      "step 00174 (21.67%) | loss: 1.389847 | lrm: 1.00 | dt: 604.18ms | tok/sec: 867,761 | mfu: -1.00 | total time: 1.42m\n",
      "step 00175 (21.79%) | loss: 1.393507 | lrm: 1.00 | dt: 526.03ms | tok/sec: 996,697 | mfu: -1.00 | total time: 1.43m\n",
      "step 00176 (21.91%) | loss: 1.386854 | lrm: 1.00 | dt: 465.79ms | tok/sec: 1,125,577 | mfu: -1.00 | total time: 1.44m\n",
      "step 00177 (22.05%) | loss: 1.393102 | lrm: 1.00 | dt: 583.88ms | tok/sec: 897,934 | mfu: -1.00 | total time: 1.45m\n",
      "step 00178 (22.17%) | loss: 1.392526 | lrm: 1.00 | dt: 529.22ms | tok/sec: 990,673 | mfu: -1.00 | total time: 1.46m\n",
      "step 00179 (22.28%) | loss: 1.388190 | lrm: 1.00 | dt: 537.37ms | tok/sec: 975,653 | mfu: -1.00 | total time: 1.47m\n",
      "step 00180 (22.42%) | loss: 1.386485 | lrm: 1.00 | dt: 526.19ms | tok/sec: 996,388 | mfu: -1.00 | total time: 1.47m\n",
      "step 00181 (22.55%) | loss: 1.387389 | lrm: 1.00 | dt: 584.40ms | tok/sec: 897,133 | mfu: -1.00 | total time: 1.48m\n",
      "step 00182 (22.68%) | loss: 1.385751 | lrm: 1.00 | dt: 684.59ms | tok/sec: 765,839 | mfu: -1.00 | total time: 1.50m\n",
      "step 00183 (22.81%) | loss: 1.388674 | lrm: 1.00 | dt: 598.45ms | tok/sec: 876,071 | mfu: -1.00 | total time: 1.51m\n",
      "step 00184 (22.93%) | loss: 1.390654 | lrm: 1.00 | dt: 529.45ms | tok/sec: 990,245 | mfu: -1.00 | total time: 1.51m\n",
      "step 00185 (23.07%) | loss: 1.384720 | lrm: 1.00 | dt: 596.74ms | tok/sec: 878,586 | mfu: -1.00 | total time: 1.52m\n",
      "step 00186 (23.20%) | loss: 1.373532 | lrm: 1.00 | dt: 469.09ms | tok/sec: 1,117,673 | mfu: -1.00 | total time: 1.53m\n",
      "step 00187 (23.31%) | loss: 1.365575 | lrm: 1.00 | dt: 621.75ms | tok/sec: 843,240 | mfu: -1.00 | total time: 1.54m\n",
      "step 00188 (23.46%) | loss: 1.372157 | lrm: 1.00 | dt: 609.49ms | tok/sec: 860,204 | mfu: -1.00 | total time: 1.55m\n",
      "step 00189 (23.57%) | loss: 1.379779 | lrm: 1.00 | dt: 529.59ms | tok/sec: 989,982 | mfu: -1.00 | total time: 1.56m\n",
      "step 00190 (23.71%) | loss: 1.380857 | lrm: 1.00 | dt: 536.32ms | tok/sec: 977,570 | mfu: -1.00 | total time: 1.57m\n",
      "step 00191 (23.84%) | loss: 1.383707 | lrm: 1.00 | dt: 624.86ms | tok/sec: 839,044 | mfu: -1.00 | total time: 1.58m\n",
      "step 00192 (23.96%) | loss: 1.384188 | lrm: 1.00 | dt: 526.41ms | tok/sec: 995,966 | mfu: -1.00 | total time: 1.59m\n",
      "step 00193 (24.07%) | loss: 1.393610 | lrm: 1.00 | dt: 467.31ms | tok/sec: 1,121,923 | mfu: -1.00 | total time: 1.60m\n",
      "step 00194 (24.20%) | loss: 1.396424 | lrm: 1.00 | dt: 526.86ms | tok/sec: 995,121 | mfu: -1.00 | total time: 1.61m\n",
      "step 00195 (24.34%) | loss: 1.397247 | lrm: 1.00 | dt: 701.19ms | tok/sec: 747,707 | mfu: -1.00 | total time: 1.62m\n",
      "step 00196 (24.45%) | loss: 1.402262 | lrm: 1.00 | dt: 587.04ms | tok/sec: 893,111 | mfu: -1.00 | total time: 1.63m\n",
      "step 00197 (24.59%) | loss: 1.401998 | lrm: 1.00 | dt: 643.16ms | tok/sec: 815,176 | mfu: -1.00 | total time: 1.64m\n",
      "step 00198 (24.70%) | loss: 1.399762 | lrm: 1.00 | dt: 534.06ms | tok/sec: 981,702 | mfu: -1.00 | total time: 1.65m\n",
      "step 00199 (24.82%) | loss: 1.406143 | lrm: 1.00 | dt: 527.99ms | tok/sec: 992,983 | mfu: -1.00 | total time: 1.66m\n",
      "step 00200 (24.96%) | loss: 1.400293 | lrm: 1.00 | dt: 468.45ms | tok/sec: 1,119,192 | mfu: -1.00 | total time: 1.66m\n",
      "step 00201 (25.10%) | loss: 1.410797 | lrm: 1.00 | dt: 527.54ms | tok/sec: 993,842 | mfu: -1.00 | total time: 1.67m\n",
      "step 00202 (25.23%) | loss: 1.398757 | lrm: 1.00 | dt: 600.26ms | tok/sec: 873,434 | mfu: -1.00 | total time: 1.68m\n",
      "step 00203 (25.35%) | loss: 1.392462 | lrm: 1.00 | dt: 467.08ms | tok/sec: 1,122,471 | mfu: -1.00 | total time: 1.69m\n",
      "step 00204 (25.47%) | loss: 1.385098 | lrm: 1.00 | dt: 700.29ms | tok/sec: 748,677 | mfu: -1.00 | total time: 1.70m\n",
      "step 00205 (25.60%) | loss: 1.383242 | lrm: 1.00 | dt: 528.28ms | tok/sec: 992,443 | mfu: -1.00 | total time: 1.71m\n",
      "step 00206 (25.72%) | loss: 1.373990 | lrm: 1.00 | dt: 468.04ms | tok/sec: 1,120,185 | mfu: -1.00 | total time: 1.72m\n",
      "step 00207 (25.83%) | loss: 1.381428 | lrm: 1.00 | dt: 626.11ms | tok/sec: 837,379 | mfu: -1.00 | total time: 1.73m\n",
      "step 00208 (25.94%) | loss: 1.382342 | lrm: 1.00 | dt: 467.08ms | tok/sec: 1,122,480 | mfu: -1.00 | total time: 1.74m\n",
      "step 00209 (26.06%) | loss: 1.385231 | lrm: 1.00 | dt: 588.40ms | tok/sec: 891,040 | mfu: -1.00 | total time: 1.75m\n",
      "step 00210 (26.18%) | loss: 1.388406 | lrm: 1.00 | dt: 587.15ms | tok/sec: 892,941 | mfu: -1.00 | total time: 1.76m\n",
      "step 00211 (26.30%) | loss: 1.389073 | lrm: 1.00 | dt: 471.66ms | tok/sec: 1,111,590 | mfu: -1.00 | total time: 1.76m\n",
      "step 00212 (26.45%) | loss: 1.389556 | lrm: 1.00 | dt: 533.93ms | tok/sec: 981,939 | mfu: -1.00 | total time: 1.77m\n",
      "step 00213 (26.57%) | loss: 1.385741 | lrm: 1.00 | dt: 539.88ms | tok/sec: 971,125 | mfu: -1.00 | total time: 1.78m\n",
      "step 00214 (26.69%) | loss: 1.381480 | lrm: 1.00 | dt: 464.42ms | tok/sec: 1,128,911 | mfu: -1.00 | total time: 1.79m\n",
      "step 00215 (26.82%) | loss: 1.377537 | lrm: 1.00 | dt: 534.39ms | tok/sec: 981,094 | mfu: -1.00 | total time: 1.80m\n",
      "step 00216 (26.93%) | loss: 1.374844 | lrm: 1.00 | dt: 534.10ms | tok/sec: 981,634 | mfu: -1.00 | total time: 1.81m\n",
      "step 00217 (27.07%) | loss: 1.371703 | lrm: 1.00 | dt: 464.75ms | tok/sec: 1,128,099 | mfu: -1.00 | total time: 1.82m\n",
      "step 00218 (27.21%) | loss: 1.374722 | lrm: 1.00 | dt: 477.51ms | tok/sec: 1,097,964 | mfu: -1.00 | total time: 1.82m\n",
      "step 00219 (27.33%) | loss: 1.372468 | lrm: 1.00 | dt: 547.04ms | tok/sec: 958,413 | mfu: -1.00 | total time: 1.83m\n",
      "step 00220 (27.45%) | loss: 1.390257 | lrm: 1.00 | dt: 466.70ms | tok/sec: 1,123,394 | mfu: -1.00 | total time: 1.84m\n",
      "step 00221 (27.57%) | loss: 1.390712 | lrm: 1.00 | dt: 474.37ms | tok/sec: 1,105,232 | mfu: -1.00 | total time: 1.85m\n",
      "step 00222 (27.70%) | loss: 1.386724 | lrm: 1.00 | dt: 530.42ms | tok/sec: 988,441 | mfu: -1.00 | total time: 1.86m\n",
      "step 00223 (27.82%) | loss: 1.383378 | lrm: 1.00 | dt: 527.42ms | tok/sec: 994,071 | mfu: -1.00 | total time: 1.87m\n",
      "step 00224 (27.94%) | loss: 1.390184 | lrm: 1.00 | dt: 523.67ms | tok/sec: 1,001,180 | mfu: -1.00 | total time: 1.87m\n",
      "step 00225 (28.07%) | loss: 1.390018 | lrm: 1.00 | dt: 465.79ms | tok/sec: 1,125,577 | mfu: -1.00 | total time: 1.88m\n",
      "step 00226 (28.20%) | loss: 1.395824 | lrm: 1.00 | dt: 476.69ms | tok/sec: 1,099,851 | mfu: -1.00 | total time: 1.89m\n",
      "step 00227 (28.33%) | loss: 1.388964 | lrm: 1.00 | dt: 474.77ms | tok/sec: 1,104,293 | mfu: -1.00 | total time: 1.90m\n",
      "step 00228 (28.47%) | loss: 1.390400 | lrm: 1.00 | dt: 534.05ms | tok/sec: 981,724 | mfu: -1.00 | total time: 1.91m\n",
      "step 00229 (28.60%) | loss: 1.394151 | lrm: 1.00 | dt: 521.38ms | tok/sec: 1,005,578 | mfu: -1.00 | total time: 1.92m\n",
      "step 00230 (28.72%) | loss: 1.391165 | lrm: 1.00 | dt: 526.09ms | tok/sec: 996,568 | mfu: -1.00 | total time: 1.92m\n",
      "step 00231 (28.84%) | loss: 1.384216 | lrm: 1.00 | dt: 464.72ms | tok/sec: 1,128,185 | mfu: -1.00 | total time: 1.93m\n",
      "step 00232 (28.97%) | loss: 1.397894 | lrm: 1.00 | dt: 480.39ms | tok/sec: 1,091,375 | mfu: -1.00 | total time: 1.94m\n",
      "step 00233 (29.08%) | loss: 1.391716 | lrm: 1.00 | dt: 472.89ms | tok/sec: 1,108,679 | mfu: -1.00 | total time: 1.95m\n",
      "step 00234 (29.21%) | loss: 1.388776 | lrm: 1.00 | dt: 533.33ms | tok/sec: 983,043 | mfu: -1.00 | total time: 1.96m\n",
      "step 00235 (29.34%) | loss: 1.396827 | lrm: 1.00 | dt: 523.55ms | tok/sec: 1,001,403 | mfu: -1.00 | total time: 1.97m\n",
      "step 00236 (29.48%) | loss: 1.390509 | lrm: 1.00 | dt: 465.45ms | tok/sec: 1,126,418 | mfu: -1.00 | total time: 1.97m\n",
      "step 00237 (29.58%) | loss: 1.393083 | lrm: 1.00 | dt: 478.28ms | tok/sec: 1,096,200 | mfu: -1.00 | total time: 1.98m\n",
      "step 00238 (29.69%) | loss: 1.401981 | lrm: 1.00 | dt: 471.80ms | tok/sec: 1,111,253 | mfu: -1.00 | total time: 1.99m\n",
      "step 00239 (29.81%) | loss: 1.397152 | lrm: 1.00 | dt: 473.30ms | tok/sec: 1,107,732 | mfu: -1.00 | total time: 2.00m\n",
      "step 00240 (29.93%) | loss: 1.396746 | lrm: 1.00 | dt: 474.07ms | tok/sec: 1,105,918 | mfu: -1.00 | total time: 2.00m\n",
      "step 00241 (30.05%) | loss: 1.402181 | lrm: 1.00 | dt: 534.98ms | tok/sec: 980,020 | mfu: -1.00 | total time: 2.01m\n",
      "step 00242 (30.19%) | loss: 1.409441 | lrm: 1.00 | dt: 525.64ms | tok/sec: 997,422 | mfu: -1.00 | total time: 2.02m\n",
      "step 00243 (30.29%) | loss: 1.415891 | lrm: 1.00 | dt: 526.32ms | tok/sec: 996,132 | mfu: -1.00 | total time: 2.03m\n",
      "step 00244 (30.42%) | loss: 1.410788 | lrm: 1.00 | dt: 518.69ms | tok/sec: 1,010,797 | mfu: -1.00 | total time: 2.04m\n",
      "step 00245 (30.53%) | loss: 1.415683 | lrm: 1.00 | dt: 526.12ms | tok/sec: 996,527 | mfu: -1.00 | total time: 2.05m\n",
      "step 00246 (30.66%) | loss: 1.421179 | lrm: 1.00 | dt: 464.34ms | tok/sec: 1,129,103 | mfu: -1.00 | total time: 2.06m\n",
      "step 00247 (30.76%) | loss: 1.418796 | lrm: 1.00 | dt: 535.77ms | tok/sec: 978,576 | mfu: -1.00 | total time: 2.07m\n",
      "step 00248 (30.90%) | loss: 1.414320 | lrm: 1.00 | dt: 527.71ms | tok/sec: 993,512 | mfu: -1.00 | total time: 2.07m\n",
      "step 00249 (31.02%) | loss: 1.405868 | lrm: 1.00 | dt: 465.05ms | tok/sec: 1,127,390 | mfu: -1.00 | total time: 2.08m\n",
      "step 00250 (31.15%) | loss: 1.398676 | lrm: 1.00 | dt: 474.85ms | tok/sec: 1,104,111 | mfu: -1.00 | total time: 2.09m\n",
      "step 00251 (31.27%) | loss: 1.390683 | lrm: 1.00 | dt: 473.13ms | tok/sec: 1,108,135 | mfu: -1.00 | total time: 2.10m\n",
      "step 00252 (31.39%) | loss: 1.385379 | lrm: 1.00 | dt: 476.02ms | tok/sec: 1,101,406 | mfu: -1.00 | total time: 2.11m\n",
      "step 00253 (31.51%) | loss: 1.392520 | lrm: 1.00 | dt: 474.01ms | tok/sec: 1,106,062 | mfu: -1.00 | total time: 2.11m\n",
      "step 00254 (31.63%) | loss: 1.384228 | lrm: 1.00 | dt: 469.77ms | tok/sec: 1,116,053 | mfu: -1.00 | total time: 2.12m\n",
      "step 00255 (31.75%) | loss: 1.382081 | lrm: 1.00 | dt: 533.11ms | tok/sec: 983,452 | mfu: -1.00 | total time: 2.13m\n",
      "step 00256 (31.88%) | loss: 1.381671 | lrm: 1.00 | dt: 466.49ms | tok/sec: 1,123,906 | mfu: -1.00 | total time: 2.14m\n",
      "step 00257 (32.00%) | loss: 1.371145 | lrm: 1.00 | dt: 477.86ms | tok/sec: 1,097,154 | mfu: -1.00 | total time: 2.15m\n",
      "step 00258 (32.12%) | loss: 1.376814 | lrm: 1.00 | dt: 473.90ms | tok/sec: 1,106,321 | mfu: -1.00 | total time: 2.15m\n",
      "step 00259 (32.26%) | loss: 1.372808 | lrm: 1.00 | dt: 474.12ms | tok/sec: 1,105,813 | mfu: -1.00 | total time: 2.16m\n",
      "step 00260 (32.38%) | loss: 1.367628 | lrm: 1.00 | dt: 470.78ms | tok/sec: 1,113,664 | mfu: -1.00 | total time: 2.17m\n",
      "step 00261 (32.50%) | loss: 1.376178 | lrm: 1.00 | dt: 473.15ms | tok/sec: 1,108,089 | mfu: -1.00 | total time: 2.18m\n",
      "step 00262 (32.62%) | loss: 1.377464 | lrm: 1.00 | dt: 474.65ms | tok/sec: 1,104,583 | mfu: -1.00 | total time: 2.19m\n",
      "step 00263 (32.75%) | loss: 1.373821 | lrm: 1.00 | dt: 469.94ms | tok/sec: 1,115,649 | mfu: -1.00 | total time: 2.19m\n",
      "step 00264 (32.87%) | loss: 1.384896 | lrm: 1.00 | dt: 475.15ms | tok/sec: 1,103,404 | mfu: -1.00 | total time: 2.20m\n",
      "step 00265 (32.97%) | loss: 1.387125 | lrm: 1.00 | dt: 539.99ms | tok/sec: 970,929 | mfu: -1.00 | total time: 2.21m\n",
      "step 00266 (33.10%) | loss: 1.382559 | lrm: 1.00 | dt: 587.03ms | tok/sec: 893,125 | mfu: -1.00 | total time: 2.22m\n",
      "step 00267 (33.22%) | loss: 1.391694 | lrm: 1.00 | dt: 577.70ms | tok/sec: 907,543 | mfu: -1.00 | total time: 2.23m\n",
      "step 00268 (33.33%) | loss: 1.397969 | lrm: 1.00 | dt: 531.12ms | tok/sec: 987,128 | mfu: -1.00 | total time: 2.24m\n",
      "step 00269 (33.44%) | loss: 1.406253 | lrm: 1.00 | dt: 523.29ms | tok/sec: 1,001,905 | mfu: -1.00 | total time: 2.25m\n",
      "step 00270 (33.57%) | loss: 1.400023 | lrm: 1.00 | dt: 533.28ms | tok/sec: 983,139 | mfu: -1.00 | total time: 2.26m\n",
      "step 00271 (33.69%) | loss: 1.403173 | lrm: 1.00 | dt: 467.24ms | tok/sec: 1,122,098 | mfu: -1.00 | total time: 2.26m\n",
      "step 00272 (33.80%) | loss: 1.398426 | lrm: 1.00 | dt: 542.18ms | tok/sec: 967,007 | mfu: -1.00 | total time: 2.27m\n",
      "step 00273 (33.95%) | loss: 1.393507 | lrm: 1.00 | dt: 533.48ms | tok/sec: 982,766 | mfu: -1.00 | total time: 2.28m\n",
      "step 00274 (34.07%) | loss: 1.380992 | lrm: 1.00 | dt: 467.73ms | tok/sec: 1,120,913 | mfu: -1.00 | total time: 2.29m\n",
      "step 00275 (34.19%) | loss: 1.374875 | lrm: 1.00 | dt: 544.62ms | tok/sec: 962,659 | mfu: -1.00 | total time: 2.30m\n",
      "step 00276 (34.31%) | loss: 1.373719 | lrm: 1.00 | dt: 528.57ms | tok/sec: 991,901 | mfu: -1.00 | total time: 2.31m\n",
      "step 00277 (34.43%) | loss: 1.386493 | lrm: 1.00 | dt: 466.13ms | tok/sec: 1,124,768 | mfu: -1.00 | total time: 2.32m\n",
      "step 00278 (34.53%) | loss: 1.383550 | lrm: 1.00 | dt: 477.92ms | tok/sec: 1,097,010 | mfu: -1.00 | total time: 2.32m\n",
      "step 00279 (34.65%) | loss: 1.378935 | lrm: 1.00 | dt: 472.30ms | tok/sec: 1,110,082 | mfu: -1.00 | total time: 2.33m\n",
      "step 00280 (34.77%) | loss: 1.373449 | lrm: 1.00 | dt: 614.09ms | tok/sec: 853,759 | mfu: -1.00 | total time: 2.34m\n",
      "step 00281 (34.89%) | loss: 1.366996 | lrm: 1.00 | dt: 468.83ms | tok/sec: 1,118,301 | mfu: -1.00 | total time: 2.35m\n",
      "step 00282 (35.02%) | loss: 1.358073 | lrm: 1.00 | dt: 542.93ms | tok/sec: 965,660 | mfu: -1.00 | total time: 2.36m\n",
      "step 00283 (35.15%) | loss: 1.366089 | lrm: 1.00 | dt: 538.84ms | tok/sec: 972,988 | mfu: -1.00 | total time: 2.37m\n",
      "step 00284 (35.27%) | loss: 1.374306 | lrm: 1.00 | dt: 469.45ms | tok/sec: 1,116,809 | mfu: -1.00 | total time: 2.38m\n",
      "step 00285 (35.37%) | loss: 1.380248 | lrm: 1.00 | dt: 485.23ms | tok/sec: 1,080,490 | mfu: -1.00 | total time: 2.38m\n",
      "step 00286 (35.48%) | loss: 1.374378 | lrm: 1.00 | dt: 468.55ms | tok/sec: 1,118,951 | mfu: -1.00 | total time: 2.39m\n",
      "step 00287 (35.59%) | loss: 1.384219 | lrm: 1.00 | dt: 544.72ms | tok/sec: 962,490 | mfu: -1.00 | total time: 2.40m\n",
      "step 00288 (35.71%) | loss: 1.390036 | lrm: 1.00 | dt: 466.85ms | tok/sec: 1,123,037 | mfu: -1.00 | total time: 2.41m\n",
      "step 00289 (35.84%) | loss: 1.379598 | lrm: 1.00 | dt: 478.26ms | tok/sec: 1,096,234 | mfu: -1.00 | total time: 2.42m\n",
      "step 00290 (35.96%) | loss: 1.371501 | lrm: 1.00 | dt: 531.49ms | tok/sec: 986,449 | mfu: -1.00 | total time: 2.42m\n",
      "step 00291 (36.09%) | loss: 1.390901 | lrm: 1.00 | dt: 620.24ms | tok/sec: 845,305 | mfu: -1.00 | total time: 2.43m\n",
      "step 00292 (36.22%) | loss: 1.372771 | lrm: 1.00 | dt: 605.17ms | tok/sec: 866,342 | mfu: -1.00 | total time: 2.45m\n",
      "step 00293 (36.34%) | loss: 1.363294 | lrm: 1.00 | dt: 471.39ms | tok/sec: 1,112,215 | mfu: -1.00 | total time: 2.45m\n",
      "step 00294 (36.46%) | loss: 1.376494 | lrm: 1.00 | dt: 482.77ms | tok/sec: 1,085,993 | mfu: -1.00 | total time: 2.46m\n",
      "step 00295 (36.60%) | loss: 1.376800 | lrm: 1.00 | dt: 470.23ms | tok/sec: 1,114,967 | mfu: -1.00 | total time: 2.47m\n",
      "step 00296 (36.73%) | loss: 1.366835 | lrm: 1.00 | dt: 477.45ms | tok/sec: 1,098,110 | mfu: -1.00 | total time: 2.48m\n",
      "step 00297 (36.86%) | loss: 1.361940 | lrm: 1.00 | dt: 536.24ms | tok/sec: 977,719 | mfu: -1.00 | total time: 2.49m\n",
      "step 00298 (37.00%) | loss: 1.370607 | lrm: 1.00 | dt: 536.65ms | tok/sec: 976,972 | mfu: -1.00 | total time: 2.49m\n",
      "step 00299 (37.10%) | loss: 1.363296 | lrm: 1.00 | dt: 536.37ms | tok/sec: 977,482 | mfu: -1.00 | total time: 2.50m\n",
      "step 00300 (37.23%) | loss: 1.363002 | lrm: 1.00 | dt: 468.74ms | tok/sec: 1,118,502 | mfu: -1.00 | total time: 2.51m\n",
      "step 00300 | Validation bpb: 0.4284\n",
      "step 00301 (37.34%) | loss: 1.353422 | lrm: 1.00 | dt: 525.42ms | tok/sec: 997,841 | mfu: -1.00 | total time: 2.52m\n",
      "step 00302 (37.45%) | loss: 1.348556 | lrm: 1.00 | dt: 467.77ms | tok/sec: 1,120,829 | mfu: -1.00 | total time: 2.53m\n",
      "step 00303 (37.58%) | loss: 1.352444 | lrm: 1.00 | dt: 475.58ms | tok/sec: 1,102,409 | mfu: -1.00 | total time: 2.54m\n",
      "step 00304 (37.71%) | loss: 1.348781 | lrm: 1.00 | dt: 476.15ms | tok/sec: 1,101,099 | mfu: -1.00 | total time: 2.54m\n",
      "step 00305 (37.84%) | loss: 1.346935 | lrm: 1.00 | dt: 524.34ms | tok/sec: 999,895 | mfu: -1.00 | total time: 2.55m\n",
      "step 00306 (37.97%) | loss: 1.354352 | lrm: 1.00 | dt: 623.10ms | tok/sec: 841,412 | mfu: -1.00 | total time: 2.56m\n",
      "step 00307 (38.09%) | loss: 1.356755 | lrm: 1.00 | dt: 534.28ms | tok/sec: 981,302 | mfu: -1.00 | total time: 2.57m\n",
      "step 00308 (38.20%) | loss: 1.367309 | lrm: 1.00 | dt: 466.97ms | tok/sec: 1,122,740 | mfu: -1.00 | total time: 2.58m\n",
      "step 00309 (38.31%) | loss: 1.375881 | lrm: 1.00 | dt: 542.70ms | tok/sec: 966,075 | mfu: -1.00 | total time: 2.59m\n",
      "step 00310 (38.44%) | loss: 1.375789 | lrm: 1.00 | dt: 614.84ms | tok/sec: 852,720 | mfu: -1.00 | total time: 2.60m\n",
      "step 00311 (38.54%) | loss: 1.367561 | lrm: 1.00 | dt: 531.80ms | tok/sec: 985,883 | mfu: -1.00 | total time: 2.61m\n",
      "step 00312 (38.66%) | loss: 1.363997 | lrm: 1.00 | dt: 468.46ms | tok/sec: 1,119,163 | mfu: -1.00 | total time: 2.62m\n",
      "step 00313 (38.79%) | loss: 1.353372 | lrm: 1.00 | dt: 554.53ms | tok/sec: 945,469 | mfu: -1.00 | total time: 2.62m\n",
      "step 00314 (38.90%) | loss: 1.362745 | lrm: 1.00 | dt: 467.95ms | tok/sec: 1,120,381 | mfu: -1.00 | total time: 2.63m\n",
      "step 00315 (39.01%) | loss: 1.353990 | lrm: 1.00 | dt: 539.43ms | tok/sec: 971,930 | mfu: -1.00 | total time: 2.64m\n",
      "step 00316 (39.12%) | loss: 1.362448 | lrm: 1.00 | dt: 526.81ms | tok/sec: 995,215 | mfu: -1.00 | total time: 2.65m\n",
      "step 00317 (39.25%) | loss: 1.359707 | lrm: 1.00 | dt: 619.39ms | tok/sec: 846,453 | mfu: -1.00 | total time: 2.66m\n",
      "step 00318 (39.35%) | loss: 1.363977 | lrm: 1.00 | dt: 538.39ms | tok/sec: 973,811 | mfu: -1.00 | total time: 2.67m\n",
      "step 00319 (39.46%) | loss: 1.358658 | lrm: 1.00 | dt: 594.47ms | tok/sec: 881,936 | mfu: -1.00 | total time: 2.68m\n",
      "step 00320 (39.59%) | loss: 1.356551 | lrm: 1.00 | dt: 531.75ms | tok/sec: 985,972 | mfu: -1.00 | total time: 2.69m\n",
      "step 00321 (39.72%) | loss: 1.351219 | lrm: 1.00 | dt: 529.75ms | tok/sec: 989,689 | mfu: -1.00 | total time: 2.70m\n",
      "step 00322 (39.83%) | loss: 1.347753 | lrm: 1.00 | dt: 533.81ms | tok/sec: 982,169 | mfu: -1.00 | total time: 2.71m\n",
      "step 00323 (39.94%) | loss: 1.351392 | lrm: 1.00 | dt: 534.69ms | tok/sec: 980,542 | mfu: -1.00 | total time: 2.72m\n",
      "step 00324 (40.06%) | loss: 1.356004 | lrm: 1.00 | dt: 466.88ms | tok/sec: 1,122,950 | mfu: -1.00 | total time: 2.72m\n",
      "step 00325 (40.19%) | loss: 1.344115 | lrm: 1.00 | dt: 528.77ms | tok/sec: 991,519 | mfu: -1.00 | total time: 2.73m\n",
      "step 00326 (40.31%) | loss: 1.350026 | lrm: 1.00 | dt: 466.80ms | tok/sec: 1,123,143 | mfu: -1.00 | total time: 2.74m\n",
      "step 00327 (40.43%) | loss: 1.363280 | lrm: 1.00 | dt: 548.35ms | tok/sec: 956,118 | mfu: -1.00 | total time: 2.75m\n",
      "step 00328 (40.56%) | loss: 1.362974 | lrm: 1.00 | dt: 465.19ms | tok/sec: 1,127,029 | mfu: -1.00 | total time: 2.76m\n",
      "step 00329 (40.68%) | loss: 1.350794 | lrm: 1.00 | dt: 541.89ms | tok/sec: 967,512 | mfu: -1.00 | total time: 2.77m\n",
      "step 00330 (40.80%) | loss: 1.342899 | lrm: 1.00 | dt: 529.31ms | tok/sec: 990,510 | mfu: -1.00 | total time: 2.77m\n",
      "step 00331 (40.91%) | loss: 1.336548 | lrm: 1.00 | dt: 467.97ms | tok/sec: 1,120,335 | mfu: -1.00 | total time: 2.78m\n",
      "step 00332 (41.02%) | loss: 1.338346 | lrm: 1.00 | dt: 478.17ms | tok/sec: 1,096,444 | mfu: -1.00 | total time: 2.79m\n",
      "step 00333 (41.15%) | loss: 1.328829 | lrm: 1.00 | dt: 475.06ms | tok/sec: 1,103,620 | mfu: -1.00 | total time: 2.80m\n",
      "step 00334 (41.28%) | loss: 1.329947 | lrm: 1.00 | dt: 541.80ms | tok/sec: 967,676 | mfu: -1.00 | total time: 2.81m\n",
      "step 00335 (41.40%) | loss: 1.322311 | lrm: 1.00 | dt: 466.11ms | tok/sec: 1,124,821 | mfu: -1.00 | total time: 2.81m\n",
      "step 00336 (41.54%) | loss: 1.327100 | lrm: 1.00 | dt: 548.44ms | tok/sec: 955,954 | mfu: -1.00 | total time: 2.82m\n",
      "step 00337 (41.68%) | loss: 1.327425 | lrm: 1.00 | dt: 536.29ms | tok/sec: 977,617 | mfu: -1.00 | total time: 2.83m\n",
      "step 00338 (41.79%) | loss: 1.324352 | lrm: 1.00 | dt: 466.33ms | tok/sec: 1,124,278 | mfu: -1.00 | total time: 2.84m\n",
      "step 00339 (41.93%) | loss: 1.323704 | lrm: 1.00 | dt: 479.96ms | tok/sec: 1,092,365 | mfu: -1.00 | total time: 2.85m\n",
      "step 00340 (42.06%) | loss: 1.333981 | lrm: 1.00 | dt: 549.20ms | tok/sec: 954,634 | mfu: -1.00 | total time: 2.86m\n",
      "step 00341 (42.19%) | loss: 1.328509 | lrm: 1.00 | dt: 530.95ms | tok/sec: 987,449 | mfu: -1.00 | total time: 2.87m\n",
      "step 00342 (42.30%) | loss: 1.338544 | lrm: 1.00 | dt: 466.73ms | tok/sec: 1,123,310 | mfu: -1.00 | total time: 2.87m\n",
      "step 00343 (42.43%) | loss: 1.330907 | lrm: 1.00 | dt: 479.95ms | tok/sec: 1,092,383 | mfu: -1.00 | total time: 2.88m\n",
      "step 00344 (42.57%) | loss: 1.324976 | lrm: 1.00 | dt: 541.85ms | tok/sec: 967,593 | mfu: -1.00 | total time: 2.89m\n",
      "step 00345 (42.70%) | loss: 1.320919 | lrm: 1.00 | dt: 468.18ms | tok/sec: 1,119,838 | mfu: -1.00 | total time: 2.90m\n",
      "step 00346 (42.82%) | loss: 1.318818 | lrm: 1.00 | dt: 479.56ms | tok/sec: 1,093,279 | mfu: -1.00 | total time: 2.91m\n",
      "step 00347 (42.94%) | loss: 1.320625 | lrm: 1.00 | dt: 471.58ms | tok/sec: 1,111,777 | mfu: -1.00 | total time: 2.92m\n",
      "step 00348 (43.07%) | loss: 1.317773 | lrm: 1.00 | dt: 476.35ms | tok/sec: 1,100,644 | mfu: -1.00 | total time: 2.92m\n",
      "step 00349 (43.19%) | loss: 1.313510 | lrm: 1.00 | dt: 471.70ms | tok/sec: 1,111,490 | mfu: -1.00 | total time: 2.93m\n",
      "step 00350 (43.31%) | loss: 1.318201 | lrm: 1.00 | dt: 545.86ms | tok/sec: 960,489 | mfu: -1.00 | total time: 2.94m\n",
      "step 00351 (43.44%) | loss: 1.322504 | lrm: 1.00 | dt: 551.52ms | tok/sec: 950,623 | mfu: -1.00 | total time: 2.95m\n",
      "step 00352 (43.57%) | loss: 1.313623 | lrm: 1.00 | dt: 517.48ms | tok/sec: 1,013,152 | mfu: -1.00 | total time: 2.96m\n",
      "step 00353 (43.70%) | loss: 1.314878 | lrm: 1.00 | dt: 535.25ms | tok/sec: 979,515 | mfu: -1.00 | total time: 2.97m\n",
      "step 00354 (43.80%) | loss: 1.338078 | lrm: 1.00 | dt: 467.44ms | tok/sec: 1,121,622 | mfu: -1.00 | total time: 2.97m\n",
      "step 00355 (43.94%) | loss: 1.335650 | lrm: 1.00 | dt: 482.91ms | tok/sec: 1,085,689 | mfu: -1.00 | total time: 2.98m\n",
      "step 00356 (44.05%) | loss: 1.331695 | lrm: 1.00 | dt: 541.56ms | tok/sec: 968,098 | mfu: -1.00 | total time: 2.99m\n",
      "step 00357 (44.15%) | loss: 1.319215 | lrm: 1.00 | dt: 466.84ms | tok/sec: 1,123,046 | mfu: -1.00 | total time: 3.00m\n",
      "step 00358 (44.28%) | loss: 1.303504 | lrm: 1.00 | dt: 534.89ms | tok/sec: 980,177 | mfu: -1.00 | total time: 3.01m\n",
      "step 00359 (44.42%) | loss: 1.299407 | lrm: 1.00 | dt: 465.76ms | tok/sec: 1,125,664 | mfu: -1.00 | total time: 3.02m\n",
      "step 00360 (44.55%) | loss: 1.300923 | lrm: 1.00 | dt: 478.40ms | tok/sec: 1,095,928 | mfu: -1.00 | total time: 3.02m\n",
      "step 00361 (44.68%) | loss: 1.299734 | lrm: 1.00 | dt: 531.35ms | tok/sec: 986,705 | mfu: -1.00 | total time: 3.03m\n",
      "step 00362 (44.82%) | loss: 1.300182 | lrm: 1.00 | dt: 465.60ms | tok/sec: 1,126,038 | mfu: -1.00 | total time: 3.04m\n",
      "step 00363 (44.94%) | loss: 1.298434 | lrm: 1.00 | dt: 478.80ms | tok/sec: 1,095,012 | mfu: -1.00 | total time: 3.05m\n",
      "step 00364 (45.09%) | loss: 1.305278 | lrm: 1.00 | dt: 545.49ms | tok/sec: 961,136 | mfu: -1.00 | total time: 3.06m\n",
      "step 00365 (45.20%) | loss: 1.302621 | lrm: 1.00 | dt: 540.31ms | tok/sec: 970,355 | mfu: -1.00 | total time: 3.07m\n",
      "step 00366 (45.32%) | loss: 1.296157 | lrm: 1.00 | dt: 465.14ms | tok/sec: 1,127,173 | mfu: -1.00 | total time: 3.07m\n",
      "step 00367 (45.45%) | loss: 1.294816 | lrm: 1.00 | dt: 480.48ms | tok/sec: 1,091,177 | mfu: -1.00 | total time: 3.08m\n",
      "step 00368 (45.59%) | loss: 1.307780 | lrm: 1.00 | dt: 473.24ms | tok/sec: 1,107,860 | mfu: -1.00 | total time: 3.09m\n",
      "step 00369 (45.71%) | loss: 1.317780 | lrm: 1.00 | dt: 595.23ms | tok/sec: 880,822 | mfu: -1.00 | total time: 3.10m\n",
      "step 00370 (45.83%) | loss: 1.320529 | lrm: 1.00 | dt: 530.38ms | tok/sec: 988,522 | mfu: -1.00 | total time: 3.11m\n",
      "step 00371 (45.98%) | loss: 1.312213 | lrm: 1.00 | dt: 533.76ms | tok/sec: 982,258 | mfu: -1.00 | total time: 3.12m\n",
      "step 00372 (46.11%) | loss: 1.305470 | lrm: 1.00 | dt: 467.21ms | tok/sec: 1,122,179 | mfu: -1.00 | total time: 3.13m\n",
      "step 00373 (46.24%) | loss: 1.310095 | lrm: 1.00 | dt: 540.90ms | tok/sec: 969,280 | mfu: -1.00 | total time: 3.13m\n",
      "step 00374 (46.37%) | loss: 1.318748 | lrm: 1.00 | dt: 465.41ms | tok/sec: 1,126,507 | mfu: -1.00 | total time: 3.14m\n",
      "step 00375 (46.49%) | loss: 1.318384 | lrm: 1.00 | dt: 601.76ms | tok/sec: 871,251 | mfu: -1.00 | total time: 3.15m\n",
      "step 00376 (46.60%) | loss: 1.323478 | lrm: 1.00 | dt: 534.34ms | tok/sec: 981,195 | mfu: -1.00 | total time: 3.16m\n",
      "step 00377 (46.74%) | loss: 1.318159 | lrm: 1.00 | dt: 540.64ms | tok/sec: 969,754 | mfu: -1.00 | total time: 3.17m\n",
      "step 00378 (46.87%) | loss: 1.329694 | lrm: 1.00 | dt: 595.32ms | tok/sec: 880,689 | mfu: -1.00 | total time: 3.18m\n",
      "step 00379 (47.01%) | loss: 1.333061 | lrm: 1.00 | dt: 468.64ms | tok/sec: 1,118,741 | mfu: -1.00 | total time: 3.19m\n",
      "step 00380 (47.13%) | loss: 1.334996 | lrm: 1.00 | dt: 484.94ms | tok/sec: 1,081,140 | mfu: -1.00 | total time: 3.20m\n",
      "step 00381 (47.26%) | loss: 1.337796 | lrm: 1.00 | dt: 537.67ms | tok/sec: 975,118 | mfu: -1.00 | total time: 3.21m\n",
      "step 00382 (47.39%) | loss: 1.334987 | lrm: 1.00 | dt: 529.93ms | tok/sec: 989,347 | mfu: -1.00 | total time: 3.21m\n",
      "step 00383 (47.51%) | loss: 1.327960 | lrm: 1.00 | dt: 539.75ms | tok/sec: 971,355 | mfu: -1.00 | total time: 3.22m\n",
      "step 00384 (47.64%) | loss: 1.337891 | lrm: 1.00 | dt: 465.46ms | tok/sec: 1,126,397 | mfu: -1.00 | total time: 3.23m\n",
      "step 00385 (47.78%) | loss: 1.348678 | lrm: 1.00 | dt: 481.25ms | tok/sec: 1,089,438 | mfu: -1.00 | total time: 3.24m\n",
      "step 00386 (47.92%) | loss: 1.346603 | lrm: 1.00 | dt: 535.31ms | tok/sec: 979,404 | mfu: -1.00 | total time: 3.25m\n",
      "step 00387 (48.06%) | loss: 1.348768 | lrm: 1.00 | dt: 606.11ms | tok/sec: 865,002 | mfu: -1.00 | total time: 3.26m\n",
      "step 00388 (48.17%) | loss: 1.339115 | lrm: 1.00 | dt: 522.78ms | tok/sec: 1,002,892 | mfu: -1.00 | total time: 3.27m\n",
      "step 00389 (48.29%) | loss: 1.356920 | lrm: 1.00 | dt: 524.21ms | tok/sec: 1,000,140 | mfu: -1.00 | total time: 3.28m\n",
      "step 00390 (48.43%) | loss: 1.350074 | lrm: 1.00 | dt: 527.95ms | tok/sec: 993,069 | mfu: -1.00 | total time: 3.28m\n",
      "step 00391 (48.55%) | loss: 1.343846 | lrm: 1.00 | dt: 543.67ms | tok/sec: 964,355 | mfu: -1.00 | total time: 3.29m\n",
      "step 00392 (48.66%) | loss: 1.344041 | lrm: 1.00 | dt: 539.91ms | tok/sec: 971,074 | mfu: -1.00 | total time: 3.30m\n",
      "step 00393 (48.78%) | loss: 1.345600 | lrm: 1.00 | dt: 529.61ms | tok/sec: 989,956 | mfu: -1.00 | total time: 3.31m\n",
      "step 00394 (48.90%) | loss: 1.348539 | lrm: 1.00 | dt: 606.15ms | tok/sec: 864,940 | mfu: -1.00 | total time: 3.32m\n",
      "step 00395 (49.03%) | loss: 1.350052 | lrm: 1.00 | dt: 467.22ms | tok/sec: 1,122,134 | mfu: -1.00 | total time: 3.33m\n",
      "step 00396 (49.17%) | loss: 1.337807 | lrm: 1.00 | dt: 536.79ms | tok/sec: 976,706 | mfu: -1.00 | total time: 3.34m\n",
      "step 00397 (49.30%) | loss: 1.335183 | lrm: 1.00 | dt: 465.23ms | tok/sec: 1,126,932 | mfu: -1.00 | total time: 3.35m\n",
      "step 00398 (49.44%) | loss: 1.339429 | lrm: 1.00 | dt: 542.37ms | tok/sec: 966,666 | mfu: -1.00 | total time: 3.35m\n",
      "step 00399 (49.55%) | loss: 1.337937 | lrm: 1.00 | dt: 465.72ms | tok/sec: 1,125,749 | mfu: -1.00 | total time: 3.36m\n",
      "step 00400 (49.67%) | loss: 1.349107 | lrm: 1.00 | dt: 610.78ms | tok/sec: 858,394 | mfu: -1.00 | total time: 3.37m\n",
      "step 00401 (49.81%) | loss: 1.344376 | lrm: 1.00 | dt: 528.75ms | tok/sec: 991,561 | mfu: -1.00 | total time: 3.38m\n",
      "step 00402 (49.94%) | loss: 1.335010 | lrm: 1.00 | dt: 603.16ms | tok/sec: 869,231 | mfu: -1.00 | total time: 3.39m\n",
      "step 00403 (50.06%) | loss: 1.322186 | lrm: 1.00 | dt: 527.28ms | tok/sec: 994,326 | mfu: -1.00 | total time: 3.40m\n",
      "step 00404 (50.18%) | loss: 1.331711 | lrm: 1.00 | dt: 527.88ms | tok/sec: 993,204 | mfu: -1.00 | total time: 3.41m\n",
      "step 00405 (50.29%) | loss: 1.317719 | lrm: 1.00 | dt: 530.30ms | tok/sec: 988,656 | mfu: -1.00 | total time: 3.42m\n",
      "step 00406 (50.41%) | loss: 1.318672 | lrm: 1.00 | dt: 468.24ms | tok/sec: 1,119,701 | mfu: -1.00 | total time: 3.43m\n",
      "step 00407 (50.54%) | loss: 1.316224 | lrm: 1.00 | dt: 473.55ms | tok/sec: 1,107,146 | mfu: -1.00 | total time: 3.43m\n",
      "step 00408 (50.65%) | loss: 1.314686 | lrm: 1.00 | dt: 475.09ms | tok/sec: 1,103,554 | mfu: -1.00 | total time: 3.44m\n",
      "step 00409 (50.78%) | loss: 1.316400 | lrm: 1.00 | dt: 540.23ms | tok/sec: 970,498 | mfu: -1.00 | total time: 3.45m\n",
      "step 00410 (50.90%) | loss: 1.310576 | lrm: 1.00 | dt: 468.92ms | tok/sec: 1,118,076 | mfu: -1.00 | total time: 3.46m\n",
      "step 00411 (51.01%) | loss: 1.310149 | lrm: 1.00 | dt: 478.94ms | tok/sec: 1,094,673 | mfu: -1.00 | total time: 3.47m\n",
      "step 00412 (51.13%) | loss: 1.311507 | lrm: 1.00 | dt: 476.61ms | tok/sec: 1,100,035 | mfu: -1.00 | total time: 3.47m\n",
      "step 00413 (51.25%) | loss: 1.309176 | lrm: 1.00 | dt: 472.72ms | tok/sec: 1,109,080 | mfu: -1.00 | total time: 3.48m\n",
      "step 00414 (51.38%) | loss: 1.309370 | lrm: 1.00 | dt: 477.63ms | tok/sec: 1,097,676 | mfu: -1.00 | total time: 3.49m\n",
      "step 00415 (51.49%) | loss: 1.305257 | lrm: 1.00 | dt: 471.86ms | tok/sec: 1,111,120 | mfu: -1.00 | total time: 3.50m\n",
      "step 00416 (51.60%) | loss: 1.300752 | lrm: 1.00 | dt: 476.23ms | tok/sec: 1,100,916 | mfu: -1.00 | total time: 3.51m\n",
      "step 00417 (51.72%) | loss: 1.299332 | lrm: 1.00 | dt: 470.42ms | tok/sec: 1,114,499 | mfu: -1.00 | total time: 3.51m\n",
      "step 00418 (51.83%) | loss: 1.298393 | lrm: 1.00 | dt: 477.13ms | tok/sec: 1,098,832 | mfu: -1.00 | total time: 3.52m\n",
      "step 00419 (51.97%) | loss: 1.292150 | lrm: 1.00 | dt: 473.40ms | tok/sec: 1,107,503 | mfu: -1.00 | total time: 3.53m\n",
      "step 00420 (52.09%) | loss: 1.284988 | lrm: 1.00 | dt: 475.69ms | tok/sec: 1,102,152 | mfu: -1.00 | total time: 3.54m\n",
      "step 00421 (52.19%) | loss: 1.285886 | lrm: 1.00 | dt: 526.66ms | tok/sec: 995,489 | mfu: -1.00 | total time: 3.55m\n",
      "step 00422 (52.33%) | loss: 1.294541 | lrm: 1.00 | dt: 467.76ms | tok/sec: 1,120,848 | mfu: -1.00 | total time: 3.55m\n",
      "step 00423 (52.46%) | loss: 1.293664 | lrm: 1.00 | dt: 477.02ms | tok/sec: 1,099,101 | mfu: -1.00 | total time: 3.56m\n",
      "step 00424 (52.59%) | loss: 1.286093 | lrm: 1.00 | dt: 473.45ms | tok/sec: 1,107,378 | mfu: -1.00 | total time: 3.57m\n",
      "step 00425 (52.71%) | loss: 1.292211 | lrm: 1.00 | dt: 534.14ms | tok/sec: 981,553 | mfu: -1.00 | total time: 3.58m\n",
      "step 00426 (52.81%) | loss: 1.298986 | lrm: 1.00 | dt: 467.53ms | tok/sec: 1,121,406 | mfu: -1.00 | total time: 3.59m\n",
      "step 00427 (52.91%) | loss: 1.297212 | lrm: 1.00 | dt: 480.06ms | tok/sec: 1,092,125 | mfu: -1.00 | total time: 3.59m\n",
      "step 00428 (53.04%) | loss: 1.294095 | lrm: 1.00 | dt: 534.60ms | tok/sec: 980,719 | mfu: -1.00 | total time: 3.60m\n",
      "step 00429 (53.15%) | loss: 1.295230 | lrm: 1.00 | dt: 465.57ms | tok/sec: 1,126,131 | mfu: -1.00 | total time: 3.61m\n",
      "step 00430 (53.27%) | loss: 1.307514 | lrm: 1.00 | dt: 478.33ms | tok/sec: 1,096,072 | mfu: -1.00 | total time: 3.62m\n",
      "step 00431 (53.38%) | loss: 1.296462 | lrm: 1.00 | dt: 472.80ms | tok/sec: 1,108,891 | mfu: -1.00 | total time: 3.63m\n",
      "step 00432 (53.49%) | loss: 1.299061 | lrm: 1.00 | dt: 473.55ms | tok/sec: 1,107,154 | mfu: -1.00 | total time: 3.64m\n",
      "step 00433 (53.61%) | loss: 1.304932 | lrm: 1.00 | dt: 474.22ms | tok/sec: 1,105,574 | mfu: -1.00 | total time: 3.64m\n",
      "step 00434 (53.72%) | loss: 1.300936 | lrm: 1.00 | dt: 472.03ms | tok/sec: 1,110,700 | mfu: -1.00 | total time: 3.65m\n",
      "step 00435 (53.85%) | loss: 1.292463 | lrm: 1.00 | dt: 473.54ms | tok/sec: 1,107,176 | mfu: -1.00 | total time: 3.66m\n",
      "step 00436 (53.96%) | loss: 1.288114 | lrm: 1.00 | dt: 471.60ms | tok/sec: 1,111,718 | mfu: -1.00 | total time: 3.67m\n",
      "step 00437 (54.10%) | loss: 1.292919 | lrm: 1.00 | dt: 474.38ms | tok/sec: 1,105,205 | mfu: -1.00 | total time: 3.67m\n",
      "step 00438 (54.21%) | loss: 1.289550 | lrm: 1.00 | dt: 473.19ms | tok/sec: 1,107,991 | mfu: -1.00 | total time: 3.68m\n",
      "step 00439 (54.33%) | loss: 1.287134 | lrm: 1.00 | dt: 475.28ms | tok/sec: 1,103,125 | mfu: -1.00 | total time: 3.69m\n",
      "step 00440 (54.45%) | loss: 1.293736 | lrm: 1.00 | dt: 471.40ms | tok/sec: 1,112,182 | mfu: -1.00 | total time: 3.70m\n",
      "step 00441 (54.58%) | loss: 1.300806 | lrm: 1.00 | dt: 541.86ms | tok/sec: 967,566 | mfu: -1.00 | total time: 3.71m\n",
      "step 00442 (54.68%) | loss: 1.303017 | lrm: 1.00 | dt: 536.96ms | tok/sec: 976,406 | mfu: -1.00 | total time: 3.72m\n",
      "step 00443 (54.78%) | loss: 1.293997 | lrm: 1.00 | dt: 534.54ms | tok/sec: 980,818 | mfu: -1.00 | total time: 3.72m\n",
      "step 00444 (54.88%) | loss: 1.296265 | lrm: 1.00 | dt: 468.94ms | tok/sec: 1,118,024 | mfu: -1.00 | total time: 3.73m\n",
      "step 00445 (55.02%) | loss: 1.290839 | lrm: 1.00 | dt: 533.09ms | tok/sec: 983,493 | mfu: -1.00 | total time: 3.74m\n",
      "step 00446 (55.13%) | loss: 1.300366 | lrm: 1.00 | dt: 466.77ms | tok/sec: 1,123,219 | mfu: -1.00 | total time: 3.75m\n",
      "step 00447 (55.25%) | loss: 1.293611 | lrm: 1.00 | dt: 477.52ms | tok/sec: 1,097,944 | mfu: -1.00 | total time: 3.76m\n",
      "step 00448 (55.36%) | loss: 1.291630 | lrm: 1.00 | dt: 543.16ms | tok/sec: 965,253 | mfu: -1.00 | total time: 3.77m\n",
      "step 00449 (55.49%) | loss: 1.292091 | lrm: 1.00 | dt: 466.04ms | tok/sec: 1,124,987 | mfu: -1.00 | total time: 3.77m\n",
      "step 00450 (55.60%) | loss: 1.288789 | lrm: 1.00 | dt: 482.54ms | tok/sec: 1,086,521 | mfu: -1.00 | total time: 3.78m\n",
      "step 00450 | Validation bpb: 0.4169\n",
      "step 00451 (55.72%) | loss: 1.296067 | lrm: 1.00 | dt: 465.92ms | tok/sec: 1,125,277 | mfu: -1.00 | total time: 3.79m\n",
      "step 00452 (55.85%) | loss: 1.290058 | lrm: 1.00 | dt: 473.84ms | tok/sec: 1,106,456 | mfu: -1.00 | total time: 3.80m\n",
      "step 00453 (55.95%) | loss: 1.285000 | lrm: 1.00 | dt: 475.22ms | tok/sec: 1,103,253 | mfu: -1.00 | total time: 3.81m\n",
      "step 00454 (56.08%) | loss: 1.285054 | lrm: 1.00 | dt: 470.96ms | tok/sec: 1,113,222 | mfu: -1.00 | total time: 3.81m\n",
      "step 00455 (56.20%) | loss: 1.281280 | lrm: 1.00 | dt: 472.94ms | tok/sec: 1,108,583 | mfu: -1.00 | total time: 3.82m\n",
      "step 00456 (56.32%) | loss: 1.280266 | lrm: 1.00 | dt: 527.94ms | tok/sec: 993,076 | mfu: -1.00 | total time: 3.83m\n",
      "step 00457 (56.46%) | loss: 1.289124 | lrm: 1.00 | dt: 465.52ms | tok/sec: 1,126,243 | mfu: -1.00 | total time: 3.84m\n",
      "step 00458 (56.58%) | loss: 1.293783 | lrm: 1.00 | dt: 479.49ms | tok/sec: 1,093,426 | mfu: -1.00 | total time: 3.85m\n",
      "step 00459 (56.69%) | loss: 1.294803 | lrm: 1.00 | dt: 472.94ms | tok/sec: 1,108,577 | mfu: -1.00 | total time: 3.85m\n",
      "step 00460 (56.81%) | loss: 1.294508 | lrm: 1.00 | dt: 536.11ms | tok/sec: 977,943 | mfu: -1.00 | total time: 3.86m\n",
      "step 00461 (56.94%) | loss: 1.300783 | lrm: 1.00 | dt: 466.04ms | tok/sec: 1,124,973 | mfu: -1.00 | total time: 3.87m\n",
      "step 00462 (57.06%) | loss: 1.303570 | lrm: 1.00 | dt: 532.35ms | tok/sec: 984,855 | mfu: -1.00 | total time: 3.88m\n",
      "step 00463 (57.17%) | loss: 1.305265 | lrm: 1.00 | dt: 534.92ms | tok/sec: 980,128 | mfu: -1.00 | total time: 3.89m\n",
      "step 00464 (57.30%) | loss: 1.301788 | lrm: 1.00 | dt: 525.48ms | tok/sec: 997,725 | mfu: -1.00 | total time: 3.90m\n",
      "step 00465 (57.44%) | loss: 1.295794 | lrm: 1.00 | dt: 530.22ms | tok/sec: 988,817 | mfu: -1.00 | total time: 3.91m\n",
      "step 00466 (57.57%) | loss: 1.297973 | lrm: 1.00 | dt: 533.78ms | tok/sec: 982,223 | mfu: -1.00 | total time: 3.92m\n",
      "step 00467 (57.70%) | loss: 1.306268 | lrm: 1.00 | dt: 531.08ms | tok/sec: 987,215 | mfu: -1.00 | total time: 3.92m\n",
      "step 00468 (57.82%) | loss: 1.307136 | lrm: 1.00 | dt: 527.10ms | tok/sec: 994,668 | mfu: -1.00 | total time: 3.93m\n",
      "step 00469 (57.93%) | loss: 1.301865 | lrm: 1.00 | dt: 465.52ms | tok/sec: 1,126,232 | mfu: -1.00 | total time: 3.94m\n",
      "step 00470 (58.04%) | loss: 1.306690 | lrm: 1.00 | dt: 477.09ms | tok/sec: 1,098,922 | mfu: -1.00 | total time: 3.95m\n",
      "step 00471 (58.15%) | loss: 1.305735 | lrm: 1.00 | dt: 473.33ms | tok/sec: 1,107,655 | mfu: -1.00 | total time: 3.96m\n",
      "step 00472 (58.27%) | loss: 1.299781 | lrm: 1.00 | dt: 471.07ms | tok/sec: 1,112,967 | mfu: -1.00 | total time: 3.96m\n",
      "step 00473 (58.39%) | loss: 1.302735 | lrm: 1.00 | dt: 474.35ms | tok/sec: 1,105,284 | mfu: -1.00 | total time: 3.97m\n",
      "step 00474 (58.51%) | loss: 1.295049 | lrm: 1.00 | dt: 533.39ms | tok/sec: 982,944 | mfu: -1.00 | total time: 3.98m\n",
      "step 00475 (58.64%) | loss: 1.291023 | lrm: 1.00 | dt: 524.45ms | tok/sec: 999,699 | mfu: -1.00 | total time: 3.99m\n",
      "step 00476 (58.77%) | loss: 1.295283 | lrm: 1.00 | dt: 533.57ms | tok/sec: 982,595 | mfu: -1.00 | total time: 4.00m\n",
      "step 00477 (58.89%) | loss: 1.298050 | lrm: 1.00 | dt: 607.08ms | tok/sec: 863,616 | mfu: -1.00 | total time: 4.01m\n",
      "step 00478 (59.02%) | loss: 1.284580 | lrm: 1.00 | dt: 545.01ms | tok/sec: 961,972 | mfu: -1.00 | total time: 4.02m\n",
      "step 00479 (59.13%) | loss: 1.291152 | lrm: 1.00 | dt: 531.39ms | tok/sec: 986,643 | mfu: -1.00 | total time: 4.03m\n",
      "step 00480 (59.28%) | loss: 1.299416 | lrm: 1.00 | dt: 533.11ms | tok/sec: 983,458 | mfu: -1.00 | total time: 4.04m\n",
      "step 00481 (59.39%) | loss: 1.296389 | lrm: 1.00 | dt: 466.16ms | tok/sec: 1,124,706 | mfu: -1.00 | total time: 4.04m\n",
      "step 00482 (59.50%) | loss: 1.292751 | lrm: 1.00 | dt: 530.21ms | tok/sec: 988,832 | mfu: -1.00 | total time: 4.05m\n",
      "step 00483 (59.63%) | loss: 1.311493 | lrm: 1.00 | dt: 524.95ms | tok/sec: 998,743 | mfu: -1.00 | total time: 4.06m\n",
      "step 00484 (59.75%) | loss: 1.301236 | lrm: 1.00 | dt: 465.90ms | tok/sec: 1,125,331 | mfu: -1.00 | total time: 4.07m\n",
      "step 00485 (59.88%) | loss: 1.294111 | lrm: 1.00 | dt: 633.26ms | tok/sec: 827,917 | mfu: -1.00 | total time: 4.08m\n",
      "step 00486 (60.01%) | loss: 1.288808 | lrm: 1.00 | dt: 521.08ms | tok/sec: 1,006,158 | mfu: -1.00 | total time: 4.09m\n",
      "step 00487 (60.13%) | loss: 1.288528 | lrm: 1.00 | dt: 466.81ms | tok/sec: 1,123,128 | mfu: -1.00 | total time: 4.10m\n",
      "step 00488 (60.25%) | loss: 1.280160 | lrm: 1.00 | dt: 478.99ms | tok/sec: 1,094,570 | mfu: -1.00 | total time: 4.10m\n",
      "step 00489 (60.40%) | loss: 1.275157 | lrm: 1.00 | dt: 474.86ms | tok/sec: 1,104,098 | mfu: -1.00 | total time: 4.11m\n",
      "step 00490 (60.52%) | loss: 1.279172 | lrm: 1.00 | dt: 475.26ms | tok/sec: 1,103,153 | mfu: -1.00 | total time: 4.12m\n",
      "step 00491 (60.65%) | loss: 1.286684 | lrm: 1.00 | dt: 540.79ms | tok/sec: 969,483 | mfu: -1.00 | total time: 4.13m\n",
      "step 00492 (60.78%) | loss: 1.294183 | lrm: 1.00 | dt: 466.96ms | tok/sec: 1,122,774 | mfu: -1.00 | total time: 4.14m\n",
      "step 00493 (60.90%) | loss: 1.297246 | lrm: 1.00 | dt: 479.52ms | tok/sec: 1,093,358 | mfu: -1.00 | total time: 4.14m\n",
      "step 00494 (61.04%) | loss: 1.291644 | lrm: 1.00 | dt: 470.33ms | tok/sec: 1,114,725 | mfu: -1.00 | total time: 4.15m\n",
      "step 00495 (61.18%) | loss: 1.294056 | lrm: 1.00 | dt: 532.06ms | tok/sec: 985,400 | mfu: -1.00 | total time: 4.16m\n",
      "step 00496 (61.32%) | loss: 1.291370 | lrm: 1.00 | dt: 466.40ms | tok/sec: 1,124,120 | mfu: -1.00 | total time: 4.17m\n",
      "step 00497 (61.45%) | loss: 1.295624 | lrm: 1.00 | dt: 538.14ms | tok/sec: 974,253 | mfu: -1.00 | total time: 4.18m\n",
      "step 00498 (61.56%) | loss: 1.297453 | lrm: 1.00 | dt: 546.47ms | tok/sec: 959,401 | mfu: -1.00 | total time: 4.19m\n",
      "step 00499 (61.70%) | loss: 1.299693 | lrm: 1.00 | dt: 618.73ms | tok/sec: 847,357 | mfu: -1.00 | total time: 4.20m\n",
      "step 00500 (61.81%) | loss: 1.309739 | lrm: 1.00 | dt: 683.06ms | tok/sec: 767,562 | mfu: -1.00 | total time: 4.21m\n",
      "step 00501 (61.95%) | loss: 1.303043 | lrm: 1.00 | dt: 524.91ms | tok/sec: 998,816 | mfu: -1.00 | total time: 4.22m\n",
      "step 00502 (62.07%) | loss: 1.298893 | lrm: 1.00 | dt: 466.17ms | tok/sec: 1,124,679 | mfu: -1.00 | total time: 4.23m\n",
      "step 00503 (62.19%) | loss: 1.307933 | lrm: 1.00 | dt: 476.82ms | tok/sec: 1,099,553 | mfu: -1.00 | total time: 4.23m\n",
      "step 00504 (62.32%) | loss: 1.313222 | lrm: 1.00 | dt: 553.18ms | tok/sec: 947,771 | mfu: -1.00 | total time: 4.24m\n",
      "step 00505 (62.45%) | loss: 1.304463 | lrm: 1.00 | dt: 615.10ms | tok/sec: 852,360 | mfu: -1.00 | total time: 4.25m\n",
      "step 00506 (62.59%) | loss: 1.304101 | lrm: 1.00 | dt: 533.47ms | tok/sec: 982,783 | mfu: -1.00 | total time: 4.26m\n",
      "step 00507 (62.71%) | loss: 1.300708 | lrm: 1.00 | dt: 537.71ms | tok/sec: 975,036 | mfu: -1.00 | total time: 4.27m\n",
      "step 00508 (62.83%) | loss: 1.302833 | lrm: 1.00 | dt: 533.22ms | tok/sec: 983,258 | mfu: -1.00 | total time: 4.28m\n",
      "step 00509 (62.95%) | loss: 1.299581 | lrm: 1.00 | dt: 530.55ms | tok/sec: 988,194 | mfu: -1.00 | total time: 4.29m\n",
      "step 00510 (63.07%) | loss: 1.292043 | lrm: 1.00 | dt: 533.47ms | tok/sec: 982,782 | mfu: -1.00 | total time: 4.30m\n",
      "step 00511 (63.20%) | loss: 1.293192 | lrm: 1.00 | dt: 529.88ms | tok/sec: 989,447 | mfu: -1.00 | total time: 4.31m\n",
      "step 00512 (63.32%) | loss: 1.294483 | lrm: 1.00 | dt: 696.81ms | tok/sec: 752,412 | mfu: -1.00 | total time: 4.32m\n",
      "step 00513 (63.44%) | loss: 1.291364 | lrm: 1.00 | dt: 685.25ms | tok/sec: 765,100 | mfu: -1.00 | total time: 4.33m\n",
      "step 00514 (63.55%) | loss: 1.301950 | lrm: 1.00 | dt: 614.43ms | tok/sec: 853,295 | mfu: -1.00 | total time: 4.34m\n",
      "step 00515 (63.69%) | loss: 1.307491 | lrm: 1.00 | dt: 610.67ms | tok/sec: 858,538 | mfu: -1.00 | total time: 4.35m\n",
      "step 00516 (63.81%) | loss: 1.312190 | lrm: 1.00 | dt: 634.27ms | tok/sec: 826,603 | mfu: -1.00 | total time: 4.36m\n",
      "step 00517 (63.94%) | loss: 1.319146 | lrm: 1.00 | dt: 536.22ms | tok/sec: 977,748 | mfu: -1.00 | total time: 4.37m\n",
      "step 00518 (64.06%) | loss: 1.311891 | lrm: 1.00 | dt: 466.60ms | tok/sec: 1,123,646 | mfu: -1.00 | total time: 4.38m\n",
      "step 00519 (64.18%) | loss: 1.303122 | lrm: 1.00 | dt: 543.07ms | tok/sec: 965,417 | mfu: -1.00 | total time: 4.39m\n",
      "step 00520 (64.29%) | loss: 1.307459 | lrm: 1.00 | dt: 467.88ms | tok/sec: 1,120,552 | mfu: -1.00 | total time: 4.39m\n",
      "step 00521 (64.41%) | loss: 1.303520 | lrm: 1.00 | dt: 543.36ms | tok/sec: 964,907 | mfu: -1.00 | total time: 4.40m\n",
      "step 00522 (64.52%) | loss: 1.305999 | lrm: 1.00 | dt: 614.97ms | tok/sec: 852,537 | mfu: -1.00 | total time: 4.41m\n",
      "step 00523 (64.64%) | loss: 1.306121 | lrm: 1.00 | dt: 468.17ms | tok/sec: 1,119,867 | mfu: -1.00 | total time: 4.42m\n",
      "step 00524 (64.77%) | loss: 1.300823 | lrm: 1.00 | dt: 544.32ms | tok/sec: 963,197 | mfu: -1.00 | total time: 4.43m\n",
      "step 00525 (64.91%) | loss: 1.295550 | lrm: 1.00 | dt: 533.72ms | tok/sec: 982,321 | mfu: -1.00 | total time: 4.44m\n",
      "step 00526 (65.03%) | loss: 1.299356 | lrm: 1.00 | dt: 467.77ms | tok/sec: 1,120,814 | mfu: -1.00 | total time: 4.45m\n",
      "step 00527 (65.17%) | loss: 1.298715 | lrm: 1.00 | dt: 548.37ms | tok/sec: 956,092 | mfu: -1.00 | total time: 4.46m\n",
      "step 00528 (65.29%) | loss: 1.292808 | lrm: 1.00 | dt: 529.31ms | tok/sec: 990,518 | mfu: -1.00 | total time: 4.46m\n",
      "step 00529 (65.42%) | loss: 1.290660 | lrm: 1.00 | dt: 468.65ms | tok/sec: 1,118,714 | mfu: -1.00 | total time: 4.47m\n",
      "step 00530 (65.54%) | loss: 1.291835 | lrm: 1.00 | dt: 613.22ms | tok/sec: 854,982 | mfu: -1.00 | total time: 4.48m\n",
      "step 00531 (65.65%) | loss: 1.296413 | lrm: 1.00 | dt: 610.68ms | tok/sec: 858,531 | mfu: -1.00 | total time: 4.49m\n",
      "step 00532 (65.78%) | loss: 1.290801 | lrm: 1.00 | dt: 466.75ms | tok/sec: 1,123,280 | mfu: -1.00 | total time: 4.50m\n",
      "step 00533 (65.89%) | loss: 1.304938 | lrm: 1.00 | dt: 479.71ms | tok/sec: 1,092,938 | mfu: -1.00 | total time: 4.51m\n",
      "step 00534 (66.04%) | loss: 1.304742 | lrm: 1.00 | dt: 472.68ms | tok/sec: 1,109,171 | mfu: -1.00 | total time: 4.52m\n",
      "step 00535 (66.17%) | loss: 1.305344 | lrm: 1.00 | dt: 473.89ms | tok/sec: 1,106,349 | mfu: -1.00 | total time: 4.52m\n",
      "step 00536 (66.28%) | loss: 1.314246 | lrm: 1.00 | dt: 617.97ms | tok/sec: 848,406 | mfu: -1.00 | total time: 4.53m\n",
      "step 00537 (66.40%) | loss: 1.312436 | lrm: 1.00 | dt: 527.32ms | tok/sec: 994,256 | mfu: -1.00 | total time: 4.54m\n",
      "step 00538 (66.54%) | loss: 1.313647 | lrm: 1.00 | dt: 522.81ms | tok/sec: 1,002,820 | mfu: -1.00 | total time: 4.55m\n",
      "step 00539 (66.67%) | loss: 1.313590 | lrm: 1.00 | dt: 533.56ms | tok/sec: 982,615 | mfu: -1.00 | total time: 4.56m\n",
      "step 00540 (66.80%) | loss: 1.304114 | lrm: 1.00 | dt: 465.59ms | tok/sec: 1,126,076 | mfu: -1.00 | total time: 4.57m\n",
      "step 00541 (66.93%) | loss: 1.301052 | lrm: 1.00 | dt: 470.97ms | tok/sec: 1,113,205 | mfu: -1.00 | total time: 4.58m\n",
      "step 00542 (67.06%) | loss: 1.292454 | lrm: 1.00 | dt: 534.78ms | tok/sec: 980,376 | mfu: -1.00 | total time: 4.59m\n",
      "step 00543 (67.16%) | loss: 1.296889 | lrm: 1.00 | dt: 521.19ms | tok/sec: 1,005,938 | mfu: -1.00 | total time: 4.59m\n",
      "step 00544 (67.28%) | loss: 1.284395 | lrm: 1.00 | dt: 623.28ms | tok/sec: 841,177 | mfu: -1.00 | total time: 4.60m\n",
      "step 00545 (67.42%) | loss: 1.283435 | lrm: 1.00 | dt: 541.07ms | tok/sec: 968,988 | mfu: -1.00 | total time: 4.61m\n",
      "step 00546 (67.53%) | loss: 1.279291 | lrm: 1.00 | dt: 536.96ms | tok/sec: 976,393 | mfu: -1.00 | total time: 4.62m\n",
      "step 00547 (67.67%) | loss: 1.279706 | lrm: 1.00 | dt: 526.36ms | tok/sec: 996,064 | mfu: -1.00 | total time: 4.63m\n",
      "step 00548 (67.80%) | loss: 1.276585 | lrm: 1.00 | dt: 464.65ms | tok/sec: 1,128,350 | mfu: -1.00 | total time: 4.64m\n",
      "step 00549 (67.94%) | loss: 1.283906 | lrm: 1.00 | dt: 539.40ms | tok/sec: 971,975 | mfu: -1.00 | total time: 4.65m\n",
      "step 00550 (68.08%) | loss: 1.272882 | lrm: 1.00 | dt: 582.70ms | tok/sec: 899,756 | mfu: -1.00 | total time: 4.66m\n",
      "step 00551 (68.21%) | loss: 1.280104 | lrm: 1.00 | dt: 466.16ms | tok/sec: 1,124,706 | mfu: -1.00 | total time: 4.67m\n",
      "step 00552 (68.34%) | loss: 1.290099 | lrm: 1.00 | dt: 478.03ms | tok/sec: 1,096,764 | mfu: -1.00 | total time: 4.67m\n",
      "step 00553 (68.46%) | loss: 1.281157 | lrm: 1.00 | dt: 535.82ms | tok/sec: 978,480 | mfu: -1.00 | total time: 4.68m\n",
      "step 00554 (68.60%) | loss: 1.286391 | lrm: 1.00 | dt: 467.96ms | tok/sec: 1,120,377 | mfu: -1.00 | total time: 4.69m\n",
      "step 00555 (68.73%) | loss: 1.289082 | lrm: 1.00 | dt: 533.97ms | tok/sec: 981,874 | mfu: -1.00 | total time: 4.70m\n",
      "step 00556 (68.85%) | loss: 1.291650 | lrm: 1.00 | dt: 465.72ms | tok/sec: 1,125,756 | mfu: -1.00 | total time: 4.71m\n",
      "step 00557 (68.98%) | loss: 1.291661 | lrm: 1.00 | dt: 473.92ms | tok/sec: 1,106,289 | mfu: -1.00 | total time: 4.71m\n",
      "step 00558 (69.11%) | loss: 1.301819 | lrm: 1.00 | dt: 473.21ms | tok/sec: 1,107,935 | mfu: -1.00 | total time: 4.72m\n",
      "step 00559 (69.22%) | loss: 1.309024 | lrm: 1.00 | dt: 473.85ms | tok/sec: 1,106,444 | mfu: -1.00 | total time: 4.73m\n",
      "step 00560 (69.34%) | loss: 1.311867 | lrm: 1.00 | dt: 473.11ms | tok/sec: 1,108,182 | mfu: -1.00 | total time: 4.74m\n",
      "step 00561 (69.48%) | loss: 1.316777 | lrm: 1.00 | dt: 625.78ms | tok/sec: 837,812 | mfu: -1.00 | total time: 4.75m\n",
      "step 00562 (69.62%) | loss: 1.317113 | lrm: 1.00 | dt: 531.80ms | tok/sec: 985,868 | mfu: -1.00 | total time: 4.76m\n",
      "step 00563 (69.73%) | loss: 1.311579 | lrm: 1.00 | dt: 533.09ms | tok/sec: 983,494 | mfu: -1.00 | total time: 4.77m\n",
      "step 00564 (69.86%) | loss: 1.309029 | lrm: 1.00 | dt: 464.65ms | tok/sec: 1,128,347 | mfu: -1.00 | total time: 4.77m\n",
      "step 00565 (69.98%) | loss: 1.299394 | lrm: 1.00 | dt: 542.02ms | tok/sec: 967,280 | mfu: -1.00 | total time: 4.78m\n",
      "step 00566 (70.10%) | loss: 1.296377 | lrm: 1.00 | dt: 464.63ms | tok/sec: 1,128,395 | mfu: -1.00 | total time: 4.79m\n",
      "step 00567 (70.23%) | loss: 1.300217 | lrm: 1.00 | dt: 479.76ms | tok/sec: 1,092,814 | mfu: -1.00 | total time: 4.80m\n",
      "step 00568 (70.36%) | loss: 1.304150 | lrm: 1.00 | dt: 472.47ms | tok/sec: 1,109,685 | mfu: -1.00 | total time: 4.81m\n",
      "step 00569 (70.51%) | loss: 1.303430 | lrm: 1.00 | dt: 620.52ms | tok/sec: 844,918 | mfu: -1.00 | total time: 4.82m\n",
      "step 00570 (70.62%) | loss: 1.296190 | lrm: 1.00 | dt: 617.07ms | tok/sec: 849,643 | mfu: -1.00 | total time: 4.83m\n",
      "step 00571 (70.72%) | loss: 1.287355 | lrm: 1.00 | dt: 468.57ms | tok/sec: 1,118,922 | mfu: -1.00 | total time: 4.84m\n",
      "step 00572 (70.84%) | loss: 1.300121 | lrm: 1.00 | dt: 480.22ms | tok/sec: 1,091,760 | mfu: -1.00 | total time: 4.84m\n",
      "step 00573 (70.97%) | loss: 1.300471 | lrm: 1.00 | dt: 470.76ms | tok/sec: 1,113,711 | mfu: -1.00 | total time: 4.85m\n",
      "step 00574 (71.08%) | loss: 1.298063 | lrm: 1.00 | dt: 476.37ms | tok/sec: 1,100,581 | mfu: -1.00 | total time: 4.86m\n",
      "step 00575 (71.19%) | loss: 1.304670 | lrm: 1.00 | dt: 474.68ms | tok/sec: 1,104,515 | mfu: -1.00 | total time: 4.87m\n",
      "step 00576 (71.30%) | loss: 1.304389 | lrm: 1.00 | dt: 535.64ms | tok/sec: 978,807 | mfu: -1.00 | total time: 4.88m\n",
      "step 00577 (71.42%) | loss: 1.299402 | lrm: 1.00 | dt: 466.52ms | tok/sec: 1,123,829 | mfu: -1.00 | total time: 4.88m\n",
      "step 00578 (71.55%) | loss: 1.292024 | lrm: 1.00 | dt: 478.35ms | tok/sec: 1,096,023 | mfu: -1.00 | total time: 4.89m\n",
      "step 00579 (71.69%) | loss: 1.293113 | lrm: 1.00 | dt: 474.83ms | tok/sec: 1,104,155 | mfu: -1.00 | total time: 4.90m\n",
      "step 00580 (71.82%) | loss: 1.301557 | lrm: 1.00 | dt: 475.91ms | tok/sec: 1,101,648 | mfu: -1.00 | total time: 4.91m\n",
      "step 00581 (71.95%) | loss: 1.296312 | lrm: 1.00 | dt: 539.08ms | tok/sec: 972,569 | mfu: -1.00 | total time: 4.92m\n",
      "step 00582 (72.07%) | loss: 1.298416 | lrm: 1.00 | dt: 534.61ms | tok/sec: 980,694 | mfu: -1.00 | total time: 4.93m\n",
      "step 00583 (72.18%) | loss: 1.304377 | lrm: 1.00 | dt: 538.17ms | tok/sec: 974,208 | mfu: -1.00 | total time: 4.93m\n",
      "step 00584 (72.29%) | loss: 1.310128 | lrm: 1.00 | dt: 547.09ms | tok/sec: 958,315 | mfu: -1.00 | total time: 4.94m\n",
      "step 00585 (72.43%) | loss: 1.298865 | lrm: 1.00 | dt: 465.03ms | tok/sec: 1,127,435 | mfu: -1.00 | total time: 4.95m\n",
      "step 00586 (72.55%) | loss: 1.296011 | lrm: 1.00 | dt: 547.83ms | tok/sec: 957,032 | mfu: -1.00 | total time: 4.96m\n",
      "step 00587 (72.67%) | loss: 1.293252 | lrm: 1.00 | dt: 532.98ms | tok/sec: 983,690 | mfu: -1.00 | total time: 4.97m\n",
      "step 00588 (72.81%) | loss: 1.304699 | lrm: 1.00 | dt: 467.01ms | tok/sec: 1,122,658 | mfu: -1.00 | total time: 4.98m\n",
      "step 00589 (72.92%) | loss: 1.313018 | lrm: 1.00 | dt: 532.10ms | tok/sec: 985,320 | mfu: -1.00 | total time: 4.99m\n",
      "step 00590 (73.05%) | loss: 1.314462 | lrm: 1.00 | dt: 538.15ms | tok/sec: 974,243 | mfu: -1.00 | total time: 4.99m\n",
      "step 00591 (73.19%) | loss: 1.316577 | lrm: 1.00 | dt: 527.90ms | tok/sec: 993,153 | mfu: -1.00 | total time: 5.00m\n",
      "step 00592 (73.31%) | loss: 1.303981 | lrm: 1.00 | dt: 534.77ms | tok/sec: 980,394 | mfu: -1.00 | total time: 5.01m\n",
      "step 00593 (73.42%) | loss: 1.303025 | lrm: 1.00 | dt: 537.51ms | tok/sec: 975,401 | mfu: -1.00 | total time: 5.02m\n",
      "step 00594 (73.54%) | loss: 1.314762 | lrm: 1.00 | dt: 467.68ms | tok/sec: 1,121,040 | mfu: -1.00 | total time: 5.03m\n",
      "step 00595 (73.67%) | loss: 1.315223 | lrm: 1.00 | dt: 477.33ms | tok/sec: 1,098,377 | mfu: -1.00 | total time: 5.04m\n",
      "step 00596 (73.77%) | loss: 1.327560 | lrm: 1.00 | dt: 471.11ms | tok/sec: 1,112,878 | mfu: -1.00 | total time: 5.04m\n",
      "step 00597 (73.93%) | loss: 1.328968 | lrm: 1.00 | dt: 475.98ms | tok/sec: 1,101,492 | mfu: -1.00 | total time: 5.05m\n",
      "step 00598 (74.05%) | loss: 1.333021 | lrm: 1.00 | dt: 470.51ms | tok/sec: 1,114,304 | mfu: -1.00 | total time: 5.06m\n",
      "step 00599 (74.16%) | loss: 1.329064 | lrm: 1.00 | dt: 536.63ms | tok/sec: 977,008 | mfu: -1.00 | total time: 5.07m\n",
      "step 00600 (74.26%) | loss: 1.321007 | lrm: 1.00 | dt: 532.07ms | tok/sec: 985,381 | mfu: -1.00 | total time: 5.08m\n",
      "step 00600 | Validation bpb: 0.4097\n",
      "step 00601 (74.39%) | loss: 1.317131 | lrm: 1.00 | dt: 549.61ms | tok/sec: 953,927 | mfu: -1.00 | total time: 5.09m\n",
      "step 00602 (74.50%) | loss: 1.308639 | lrm: 1.00 | dt: 470.06ms | tok/sec: 1,115,352 | mfu: -1.00 | total time: 5.10m\n",
      "step 00603 (74.62%) | loss: 1.313549 | lrm: 1.00 | dt: 480.05ms | tok/sec: 1,092,163 | mfu: -1.00 | total time: 5.10m\n",
      "step 00604 (74.72%) | loss: 1.302663 | lrm: 1.00 | dt: 472.99ms | tok/sec: 1,108,463 | mfu: -1.00 | total time: 5.11m\n",
      "step 00605 (74.84%) | loss: 1.304126 | lrm: 1.00 | dt: 474.06ms | tok/sec: 1,105,955 | mfu: -1.00 | total time: 5.12m\n",
      "step 00606 (74.98%) | loss: 1.318566 | lrm: 1.00 | dt: 471.82ms | tok/sec: 1,111,207 | mfu: -1.00 | total time: 5.13m\n",
      "step 00607 (75.12%) | loss: 1.310253 | lrm: 1.00 | dt: 471.34ms | tok/sec: 1,112,341 | mfu: -1.00 | total time: 5.14m\n",
      "step 00608 (75.23%) | loss: 1.291959 | lrm: 1.00 | dt: 472.29ms | tok/sec: 1,110,101 | mfu: -1.00 | total time: 5.14m\n",
      "step 00609 (75.34%) | loss: 1.287857 | lrm: 1.00 | dt: 470.46ms | tok/sec: 1,114,405 | mfu: -1.00 | total time: 5.15m\n",
      "step 00610 (75.45%) | loss: 1.292777 | lrm: 1.00 | dt: 473.33ms | tok/sec: 1,107,664 | mfu: -1.00 | total time: 5.16m\n",
      "step 00611 (75.58%) | loss: 1.285638 | lrm: 1.00 | dt: 472.68ms | tok/sec: 1,109,176 | mfu: -1.00 | total time: 5.17m\n",
      "step 00612 (75.70%) | loss: 1.273699 | lrm: 1.00 | dt: 472.18ms | tok/sec: 1,110,349 | mfu: -1.00 | total time: 5.17m\n",
      "step 00613 (75.83%) | loss: 1.265621 | lrm: 1.00 | dt: 470.60ms | tok/sec: 1,114,081 | mfu: -1.00 | total time: 5.18m\n",
      "step 00614 (75.93%) | loss: 1.267945 | lrm: 1.00 | dt: 472.05ms | tok/sec: 1,110,654 | mfu: -1.00 | total time: 5.19m\n",
      "step 00615 (76.08%) | loss: 1.267921 | lrm: 1.00 | dt: 470.90ms | tok/sec: 1,113,377 | mfu: -1.00 | total time: 5.20m\n",
      "step 00616 (76.19%) | loss: 1.270037 | lrm: 1.00 | dt: 470.94ms | tok/sec: 1,113,286 | mfu: -1.00 | total time: 5.21m\n",
      "step 00617 (76.31%) | loss: 1.271436 | lrm: 1.00 | dt: 539.06ms | tok/sec: 972,589 | mfu: -1.00 | total time: 5.21m\n",
      "step 00618 (76.44%) | loss: 1.274245 | lrm: 1.00 | dt: 466.29ms | tok/sec: 1,124,380 | mfu: -1.00 | total time: 5.22m\n",
      "step 00619 (76.55%) | loss: 1.283437 | lrm: 1.00 | dt: 477.57ms | tok/sec: 1,097,823 | mfu: -1.00 | total time: 5.23m\n",
      "step 00620 (76.68%) | loss: 1.293633 | lrm: 1.00 | dt: 471.71ms | tok/sec: 1,111,458 | mfu: -1.00 | total time: 5.24m\n",
      "step 00621 (76.79%) | loss: 1.289469 | lrm: 1.00 | dt: 476.17ms | tok/sec: 1,101,055 | mfu: -1.00 | total time: 5.25m\n",
      "step 00622 (76.92%) | loss: 1.287642 | lrm: 1.00 | dt: 472.57ms | tok/sec: 1,109,451 | mfu: -1.00 | total time: 5.25m\n",
      "step 00623 (77.03%) | loss: 1.296010 | lrm: 1.00 | dt: 474.74ms | tok/sec: 1,104,364 | mfu: -1.00 | total time: 5.26m\n",
      "step 00624 (77.16%) | loss: 1.290249 | lrm: 1.00 | dt: 473.43ms | tok/sec: 1,107,432 | mfu: -1.00 | total time: 5.27m\n",
      "step 00625 (77.27%) | loss: 1.293415 | lrm: 1.00 | dt: 472.57ms | tok/sec: 1,109,437 | mfu: -1.00 | total time: 5.28m\n",
      "step 00626 (77.39%) | loss: 1.284668 | lrm: 1.00 | dt: 472.39ms | tok/sec: 1,109,856 | mfu: -1.00 | total time: 5.29m\n",
      "step 00627 (77.50%) | loss: 1.279524 | lrm: 1.00 | dt: 474.17ms | tok/sec: 1,105,689 | mfu: -1.00 | total time: 5.29m\n",
      "step 00628 (77.64%) | loss: 1.283179 | lrm: 1.00 | dt: 471.50ms | tok/sec: 1,111,953 | mfu: -1.00 | total time: 5.30m\n",
      "step 00629 (77.77%) | loss: 1.281612 | lrm: 1.00 | dt: 472.15ms | tok/sec: 1,110,436 | mfu: -1.00 | total time: 5.31m\n",
      "step 00630 (77.90%) | loss: 1.283117 | lrm: 1.00 | dt: 550.54ms | tok/sec: 952,312 | mfu: -1.00 | total time: 5.32m\n",
      "step 00631 (78.02%) | loss: 1.274191 | lrm: 1.00 | dt: 532.76ms | tok/sec: 984,101 | mfu: -1.00 | total time: 5.33m\n",
      "step 00632 (78.13%) | loss: 1.277218 | lrm: 1.00 | dt: 535.46ms | tok/sec: 979,134 | mfu: -1.00 | total time: 5.34m\n",
      "step 00633 (78.25%) | loss: 1.273645 | lrm: 1.00 | dt: 466.13ms | tok/sec: 1,124,765 | mfu: -1.00 | total time: 5.34m\n",
      "step 00634 (78.37%) | loss: 1.266338 | lrm: 1.00 | dt: 477.59ms | tok/sec: 1,097,773 | mfu: -1.00 | total time: 5.35m\n",
      "step 00635 (78.49%) | loss: 1.262527 | lrm: 1.00 | dt: 471.91ms | tok/sec: 1,110,987 | mfu: -1.00 | total time: 5.36m\n",
      "step 00636 (78.61%) | loss: 1.264862 | lrm: 1.00 | dt: 476.66ms | tok/sec: 1,099,924 | mfu: -1.00 | total time: 5.37m\n",
      "step 00637 (78.72%) | loss: 1.267675 | lrm: 1.00 | dt: 472.14ms | tok/sec: 1,110,456 | mfu: -1.00 | total time: 5.38m\n",
      "step 00638 (78.84%) | loss: 1.266862 | lrm: 1.00 | dt: 472.47ms | tok/sec: 1,109,679 | mfu: -1.00 | total time: 5.38m\n",
      "step 00639 (78.95%) | loss: 1.261468 | lrm: 1.00 | dt: 473.84ms | tok/sec: 1,106,476 | mfu: -1.00 | total time: 5.39m\n",
      "step 00640 (79.08%) | loss: 1.262477 | lrm: 1.00 | dt: 472.96ms | tok/sec: 1,108,536 | mfu: -1.00 | total time: 5.40m\n",
      "step 00641 (79.19%) | loss: 1.268572 | lrm: 1.00 | dt: 474.68ms | tok/sec: 1,104,502 | mfu: -1.00 | total time: 5.41m\n",
      "step 00642 (79.33%) | loss: 1.276472 | lrm: 1.00 | dt: 470.99ms | tok/sec: 1,113,151 | mfu: -1.00 | total time: 5.42m\n",
      "step 00643 (79.47%) | loss: 1.274022 | lrm: 1.00 | dt: 545.93ms | tok/sec: 960,361 | mfu: -1.00 | total time: 5.42m\n",
      "step 00644 (79.58%) | loss: 1.272048 | lrm: 1.00 | dt: 466.44ms | tok/sec: 1,124,019 | mfu: -1.00 | total time: 5.43m\n",
      "step 00645 (79.69%) | loss: 1.270205 | lrm: 1.00 | dt: 549.62ms | tok/sec: 953,902 | mfu: -1.00 | total time: 5.44m\n",
      "step 00646 (79.84%) | loss: 1.266672 | lrm: 1.00 | dt: 538.78ms | tok/sec: 973,096 | mfu: -1.00 | total time: 5.45m\n",
      "step 00647 (79.95%) | loss: 1.268244 | lrm: 1.00 | dt: 531.24ms | tok/sec: 986,918 | mfu: -1.00 | total time: 5.46m\n",
      "step 00648 (80.07%) | loss: 1.263656 | lrm: 1.00 | dt: 466.83ms | tok/sec: 1,123,086 | mfu: -1.00 | total time: 5.47m\n",
      "step 00649 (80.18%) | loss: 1.258830 | lrm: 0.99 | dt: 481.72ms | tok/sec: 1,088,372 | mfu: -1.00 | total time: 5.47m\n",
      "step 00650 (80.30%) | loss: 1.252199 | lrm: 0.99 | dt: 472.08ms | tok/sec: 1,110,586 | mfu: -1.00 | total time: 5.48m\n",
      "step 00651 (80.41%) | loss: 1.249886 | lrm: 0.98 | dt: 478.40ms | tok/sec: 1,095,928 | mfu: -1.00 | total time: 5.49m\n",
      "step 00652 (80.54%) | loss: 1.241798 | lrm: 0.97 | dt: 470.85ms | tok/sec: 1,113,498 | mfu: -1.00 | total time: 5.50m\n",
      "step 00653 (80.66%) | loss: 1.246329 | lrm: 0.97 | dt: 474.02ms | tok/sec: 1,106,045 | mfu: -1.00 | total time: 5.51m\n",
      "step 00654 (80.79%) | loss: 1.249982 | lrm: 0.96 | dt: 539.18ms | tok/sec: 972,372 | mfu: -1.00 | total time: 5.52m\n",
      "step 00655 (80.91%) | loss: 1.257687 | lrm: 0.95 | dt: 467.78ms | tok/sec: 1,120,793 | mfu: -1.00 | total time: 5.52m\n",
      "step 00656 (81.03%) | loss: 1.252724 | lrm: 0.95 | dt: 479.34ms | tok/sec: 1,093,780 | mfu: -1.00 | total time: 5.53m\n",
      "step 00657 (81.15%) | loss: 1.246997 | lrm: 0.94 | dt: 538.81ms | tok/sec: 973,054 | mfu: -1.00 | total time: 5.54m\n",
      "step 00658 (81.27%) | loss: 1.257573 | lrm: 0.94 | dt: 534.85ms | tok/sec: 980,260 | mfu: -1.00 | total time: 5.55m\n",
      "step 00659 (81.39%) | loss: 1.260873 | lrm: 0.93 | dt: 466.40ms | tok/sec: 1,124,104 | mfu: -1.00 | total time: 5.56m\n",
      "step 00660 (81.52%) | loss: 1.257482 | lrm: 0.92 | dt: 477.69ms | tok/sec: 1,097,559 | mfu: -1.00 | total time: 5.56m\n",
      "step 00661 (81.65%) | loss: 1.255800 | lrm: 0.92 | dt: 469.71ms | tok/sec: 1,116,197 | mfu: -1.00 | total time: 5.57m\n",
      "step 00662 (81.77%) | loss: 1.251302 | lrm: 0.91 | dt: 476.65ms | tok/sec: 1,099,943 | mfu: -1.00 | total time: 5.58m\n",
      "step 00663 (81.90%) | loss: 1.252930 | lrm: 0.90 | dt: 471.03ms | tok/sec: 1,113,067 | mfu: -1.00 | total time: 5.59m\n",
      "step 00664 (82.03%) | loss: 1.268847 | lrm: 0.90 | dt: 472.87ms | tok/sec: 1,108,742 | mfu: -1.00 | total time: 5.60m\n",
      "step 00665 (82.14%) | loss: 1.278134 | lrm: 0.89 | dt: 473.13ms | tok/sec: 1,108,136 | mfu: -1.00 | total time: 5.60m\n",
      "step 00666 (82.27%) | loss: 1.271844 | lrm: 0.89 | dt: 474.80ms | tok/sec: 1,104,236 | mfu: -1.00 | total time: 5.61m\n",
      "step 00667 (82.40%) | loss: 1.275257 | lrm: 0.88 | dt: 473.99ms | tok/sec: 1,106,119 | mfu: -1.00 | total time: 5.62m\n",
      "step 00668 (82.52%) | loss: 1.272896 | lrm: 0.87 | dt: 470.11ms | tok/sec: 1,115,240 | mfu: -1.00 | total time: 5.63m\n",
      "step 00669 (82.62%) | loss: 1.295430 | lrm: 0.87 | dt: 545.62ms | tok/sec: 960,894 | mfu: -1.00 | total time: 5.64m\n",
      "step 00670 (82.75%) | loss: 1.292525 | lrm: 0.86 | dt: 533.14ms | tok/sec: 983,400 | mfu: -1.00 | total time: 5.65m\n",
      "step 00671 (82.87%) | loss: 1.294756 | lrm: 0.86 | dt: 531.33ms | tok/sec: 986,743 | mfu: -1.00 | total time: 5.65m\n",
      "step 00672 (82.99%) | loss: 1.294830 | lrm: 0.85 | dt: 466.21ms | tok/sec: 1,124,566 | mfu: -1.00 | total time: 5.66m\n",
      "step 00673 (83.11%) | loss: 1.279760 | lrm: 0.84 | dt: 481.43ms | tok/sec: 1,089,025 | mfu: -1.00 | total time: 5.67m\n",
      "step 00674 (83.22%) | loss: 1.269268 | lrm: 0.84 | dt: 472.64ms | tok/sec: 1,109,275 | mfu: -1.00 | total time: 5.68m\n",
      "step 00675 (83.34%) | loss: 1.248378 | lrm: 0.83 | dt: 476.35ms | tok/sec: 1,100,646 | mfu: -1.00 | total time: 5.69m\n",
      "step 00676 (83.46%) | loss: 1.250980 | lrm: 0.83 | dt: 538.71ms | tok/sec: 973,224 | mfu: -1.00 | total time: 5.70m\n",
      "step 00677 (83.59%) | loss: 1.254368 | lrm: 0.82 | dt: 532.20ms | tok/sec: 985,131 | mfu: -1.00 | total time: 5.70m\n",
      "step 00678 (83.71%) | loss: 1.260930 | lrm: 0.81 | dt: 530.12ms | tok/sec: 989,006 | mfu: -1.00 | total time: 5.71m\n",
      "step 00679 (83.84%) | loss: 1.268166 | lrm: 0.81 | dt: 466.15ms | tok/sec: 1,124,728 | mfu: -1.00 | total time: 5.72m\n",
      "step 00680 (83.95%) | loss: 1.272261 | lrm: 0.80 | dt: 481.93ms | tok/sec: 1,087,893 | mfu: -1.00 | total time: 5.73m\n",
      "step 00681 (84.06%) | loss: 1.272173 | lrm: 0.80 | dt: 472.61ms | tok/sec: 1,109,343 | mfu: -1.00 | total time: 5.74m\n",
      "step 00682 (84.20%) | loss: 1.275695 | lrm: 0.79 | dt: 477.99ms | tok/sec: 1,096,853 | mfu: -1.00 | total time: 5.74m\n",
      "step 00683 (84.31%) | loss: 1.269367 | lrm: 0.78 | dt: 536.54ms | tok/sec: 977,166 | mfu: -1.00 | total time: 5.75m\n",
      "step 00684 (84.43%) | loss: 1.273081 | lrm: 0.78 | dt: 466.75ms | tok/sec: 1,123,281 | mfu: -1.00 | total time: 5.76m\n",
      "step 00685 (84.54%) | loss: 1.276883 | lrm: 0.77 | dt: 479.76ms | tok/sec: 1,092,814 | mfu: -1.00 | total time: 5.77m\n",
      "step 00686 (84.66%) | loss: 1.277611 | lrm: 0.77 | dt: 470.98ms | tok/sec: 1,113,177 | mfu: -1.00 | total time: 5.78m\n",
      "step 00687 (84.76%) | loss: 1.276311 | lrm: 0.76 | dt: 480.18ms | tok/sec: 1,091,862 | mfu: -1.00 | total time: 5.79m\n",
      "step 00688 (84.88%) | loss: 1.300669 | lrm: 0.76 | dt: 540.82ms | tok/sec: 969,437 | mfu: -1.00 | total time: 5.79m\n",
      "step 00689 (85.01%) | loss: 1.286982 | lrm: 0.75 | dt: 467.18ms | tok/sec: 1,122,231 | mfu: -1.00 | total time: 5.80m\n",
      "step 00690 (85.15%) | loss: 1.282462 | lrm: 0.74 | dt: 485.81ms | tok/sec: 1,079,200 | mfu: -1.00 | total time: 5.81m\n",
      "step 00691 (85.26%) | loss: 1.275988 | lrm: 0.74 | dt: 469.14ms | tok/sec: 1,117,551 | mfu: -1.00 | total time: 5.82m\n",
      "step 00692 (85.37%) | loss: 1.285287 | lrm: 0.73 | dt: 477.48ms | tok/sec: 1,098,034 | mfu: -1.00 | total time: 5.83m\n",
      "step 00693 (85.50%) | loss: 1.290032 | lrm: 0.72 | dt: 547.27ms | tok/sec: 958,011 | mfu: -1.00 | total time: 5.84m\n",
      "step 00694 (85.63%) | loss: 1.291385 | lrm: 0.72 | dt: 531.92ms | tok/sec: 985,656 | mfu: -1.00 | total time: 5.84m\n",
      "step 00695 (85.77%) | loss: 1.298252 | lrm: 0.71 | dt: 468.07ms | tok/sec: 1,120,109 | mfu: -1.00 | total time: 5.85m\n",
      "step 00696 (85.88%) | loss: 1.302863 | lrm: 0.71 | dt: 480.28ms | tok/sec: 1,091,628 | mfu: -1.00 | total time: 5.86m\n",
      "step 00697 (86.02%) | loss: 1.291195 | lrm: 0.70 | dt: 470.59ms | tok/sec: 1,114,096 | mfu: -1.00 | total time: 5.87m\n",
      "step 00698 (86.16%) | loss: 1.294104 | lrm: 0.69 | dt: 543.41ms | tok/sec: 964,819 | mfu: -1.00 | total time: 5.88m\n",
      "step 00699 (86.29%) | loss: 1.297959 | lrm: 0.69 | dt: 528.81ms | tok/sec: 991,454 | mfu: -1.00 | total time: 5.89m\n",
      "step 00700 (86.41%) | loss: 1.293498 | lrm: 0.68 | dt: 468.63ms | tok/sec: 1,118,766 | mfu: -1.00 | total time: 5.89m\n",
      "step 00701 (86.51%) | loss: 1.283041 | lrm: 0.67 | dt: 477.44ms | tok/sec: 1,098,124 | mfu: -1.00 | total time: 5.90m\n",
      "step 00702 (86.65%) | loss: 1.280007 | lrm: 0.67 | dt: 469.60ms | tok/sec: 1,116,459 | mfu: -1.00 | total time: 5.91m\n",
      "step 00703 (86.79%) | loss: 1.273675 | lrm: 0.66 | dt: 543.15ms | tok/sec: 965,273 | mfu: -1.00 | total time: 5.92m\n",
      "step 00704 (86.90%) | loss: 1.274071 | lrm: 0.65 | dt: 534.25ms | tok/sec: 981,353 | mfu: -1.00 | total time: 5.93m\n",
      "step 00705 (87.03%) | loss: 1.278181 | lrm: 0.65 | dt: 467.21ms | tok/sec: 1,122,160 | mfu: -1.00 | total time: 5.93m\n",
      "step 00706 (87.15%) | loss: 1.275883 | lrm: 0.64 | dt: 480.91ms | tok/sec: 1,090,204 | mfu: -1.00 | total time: 5.94m\n",
      "step 00707 (87.27%) | loss: 1.276480 | lrm: 0.64 | dt: 470.28ms | tok/sec: 1,114,836 | mfu: -1.00 | total time: 5.95m\n",
      "step 00708 (87.39%) | loss: 1.272023 | lrm: 0.63 | dt: 479.22ms | tok/sec: 1,094,052 | mfu: -1.00 | total time: 5.96m\n",
      "step 00709 (87.49%) | loss: 1.266653 | lrm: 0.63 | dt: 472.19ms | tok/sec: 1,110,330 | mfu: -1.00 | total time: 5.97m\n",
      "step 00710 (87.62%) | loss: 1.256642 | lrm: 0.62 | dt: 472.61ms | tok/sec: 1,109,340 | mfu: -1.00 | total time: 5.97m\n",
      "step 00711 (87.74%) | loss: 1.252811 | lrm: 0.61 | dt: 472.14ms | tok/sec: 1,110,457 | mfu: -1.00 | total time: 5.98m\n",
      "step 00712 (87.85%) | loss: 1.251188 | lrm: 0.61 | dt: 471.35ms | tok/sec: 1,112,316 | mfu: -1.00 | total time: 5.99m\n",
      "step 00713 (87.97%) | loss: 1.256026 | lrm: 0.60 | dt: 474.90ms | tok/sec: 1,104,005 | mfu: -1.00 | total time: 6.00m\n",
      "step 00714 (88.10%) | loss: 1.253407 | lrm: 0.60 | dt: 472.21ms | tok/sec: 1,110,275 | mfu: -1.00 | total time: 6.01m\n",
      "step 00715 (88.23%) | loss: 1.260450 | lrm: 0.59 | dt: 475.18ms | tok/sec: 1,103,344 | mfu: -1.00 | total time: 6.01m\n",
      "step 00716 (88.36%) | loss: 1.252923 | lrm: 0.58 | dt: 473.35ms | tok/sec: 1,107,608 | mfu: -1.00 | total time: 6.02m\n",
      "step 00717 (88.49%) | loss: 1.250301 | lrm: 0.58 | dt: 474.61ms | tok/sec: 1,104,674 | mfu: -1.00 | total time: 6.03m\n",
      "step 00718 (88.62%) | loss: 1.252243 | lrm: 0.57 | dt: 539.33ms | tok/sec: 972,108 | mfu: -1.00 | total time: 6.04m\n",
      "step 00719 (88.73%) | loss: 1.244215 | lrm: 0.56 | dt: 467.87ms | tok/sec: 1,120,585 | mfu: -1.00 | total time: 6.05m\n",
      "step 00720 (88.85%) | loss: 1.248041 | lrm: 0.56 | dt: 480.35ms | tok/sec: 1,091,464 | mfu: -1.00 | total time: 6.05m\n",
      "step 00721 (88.96%) | loss: 1.244062 | lrm: 0.55 | dt: 471.88ms | tok/sec: 1,111,057 | mfu: -1.00 | total time: 6.06m\n",
      "step 00722 (89.10%) | loss: 1.245639 | lrm: 0.55 | dt: 475.70ms | tok/sec: 1,102,145 | mfu: -1.00 | total time: 6.07m\n",
      "step 00723 (89.22%) | loss: 1.242011 | lrm: 0.54 | dt: 542.68ms | tok/sec: 966,113 | mfu: -1.00 | total time: 6.08m\n",
      "step 00724 (89.34%) | loss: 1.244352 | lrm: 0.53 | dt: 533.42ms | tok/sec: 982,875 | mfu: -1.00 | total time: 6.09m\n",
      "step 00725 (89.45%) | loss: 1.233984 | lrm: 0.53 | dt: 468.32ms | tok/sec: 1,119,506 | mfu: -1.00 | total time: 6.10m\n",
      "step 00726 (89.56%) | loss: 1.237334 | lrm: 0.52 | dt: 480.34ms | tok/sec: 1,091,500 | mfu: -1.00 | total time: 6.10m\n",
      "step 00727 (89.67%) | loss: 1.245082 | lrm: 0.52 | dt: 470.24ms | tok/sec: 1,114,929 | mfu: -1.00 | total time: 6.11m\n",
      "step 00728 (89.81%) | loss: 1.256744 | lrm: 0.51 | dt: 553.54ms | tok/sec: 947,146 | mfu: -1.00 | total time: 6.12m\n",
      "step 00729 (89.92%) | loss: 1.248342 | lrm: 0.50 | dt: 465.80ms | tok/sec: 1,125,555 | mfu: -1.00 | total time: 6.13m\n",
      "step 00730 (90.05%) | loss: 1.255661 | lrm: 0.50 | dt: 478.22ms | tok/sec: 1,096,327 | mfu: -1.00 | total time: 6.14m\n",
      "step 00731 (90.18%) | loss: 1.261060 | lrm: 0.49 | dt: 472.94ms | tok/sec: 1,108,574 | mfu: -1.00 | total time: 6.14m\n",
      "step 00732 (90.31%) | loss: 1.258410 | lrm: 0.48 | dt: 539.99ms | tok/sec: 970,926 | mfu: -1.00 | total time: 6.15m\n",
      "step 00733 (90.43%) | loss: 1.256838 | lrm: 0.48 | dt: 468.01ms | tok/sec: 1,120,254 | mfu: -1.00 | total time: 6.16m\n",
      "step 00734 (90.56%) | loss: 1.249633 | lrm: 0.47 | dt: 478.67ms | tok/sec: 1,095,304 | mfu: -1.00 | total time: 6.17m\n",
      "step 00735 (90.69%) | loss: 1.262596 | lrm: 0.47 | dt: 471.52ms | tok/sec: 1,111,919 | mfu: -1.00 | total time: 6.18m\n",
      "step 00736 (90.83%) | loss: 1.258622 | lrm: 0.46 | dt: 478.71ms | tok/sec: 1,095,202 | mfu: -1.00 | total time: 6.19m\n",
      "step 00737 (90.96%) | loss: 1.252002 | lrm: 0.45 | dt: 550.22ms | tok/sec: 952,873 | mfu: -1.00 | total time: 6.19m\n",
      "step 00738 (91.08%) | loss: 1.253393 | lrm: 0.45 | dt: 465.26ms | tok/sec: 1,126,878 | mfu: -1.00 | total time: 6.20m\n",
      "step 00739 (91.19%) | loss: 1.246624 | lrm: 0.44 | dt: 479.65ms | tok/sec: 1,093,072 | mfu: -1.00 | total time: 6.21m\n",
      "step 00740 (91.31%) | loss: 1.235466 | lrm: 0.43 | dt: 539.89ms | tok/sec: 971,098 | mfu: -1.00 | total time: 6.22m\n",
      "step 00741 (91.42%) | loss: 1.234559 | lrm: 0.43 | dt: 468.08ms | tok/sec: 1,120,073 | mfu: -1.00 | total time: 6.23m\n",
      "step 00742 (91.55%) | loss: 1.232930 | lrm: 0.42 | dt: 482.13ms | tok/sec: 1,087,443 | mfu: -1.00 | total time: 6.23m\n",
      "step 00743 (91.70%) | loss: 1.238549 | lrm: 0.42 | dt: 469.88ms | tok/sec: 1,115,781 | mfu: -1.00 | total time: 6.24m\n",
      "step 00744 (91.81%) | loss: 1.236476 | lrm: 0.41 | dt: 475.92ms | tok/sec: 1,101,625 | mfu: -1.00 | total time: 6.25m\n",
      "step 00745 (91.93%) | loss: 1.233585 | lrm: 0.40 | dt: 475.08ms | tok/sec: 1,103,584 | mfu: -1.00 | total time: 6.26m\n",
      "step 00746 (92.05%) | loss: 1.227560 | lrm: 0.40 | dt: 472.73ms | tok/sec: 1,109,065 | mfu: -1.00 | total time: 6.27m\n",
      "step 00747 (92.17%) | loss: 1.230381 | lrm: 0.39 | dt: 541.02ms | tok/sec: 969,078 | mfu: -1.00 | total time: 6.28m\n",
      "step 00748 (92.29%) | loss: 1.219646 | lrm: 0.39 | dt: 466.40ms | tok/sec: 1,124,113 | mfu: -1.00 | total time: 6.28m\n",
      "step 00749 (92.40%) | loss: 1.225389 | lrm: 0.38 | dt: 481.22ms | tok/sec: 1,089,494 | mfu: -1.00 | total time: 6.29m\n",
      "step 00750 (92.51%) | loss: 1.229165 | lrm: 0.37 | dt: 471.94ms | tok/sec: 1,110,914 | mfu: -1.00 | total time: 6.30m\n",
      "step 00750 | Validation bpb: 0.3984\n",
      "step 00751 (92.63%) | loss: 1.224991 | lrm: 0.37 | dt: 466.54ms | tok/sec: 1,123,768 | mfu: -1.00 | total time: 6.31m\n",
      "step 00752 (92.76%) | loss: 1.246129 | lrm: 0.36 | dt: 469.76ms | tok/sec: 1,116,081 | mfu: -1.00 | total time: 6.31m\n",
      "step 00753 (92.88%) | loss: 1.243243 | lrm: 0.36 | dt: 475.31ms | tok/sec: 1,103,033 | mfu: -1.00 | total time: 6.32m\n",
      "step 00754 (93.00%) | loss: 1.244942 | lrm: 0.35 | dt: 469.16ms | tok/sec: 1,117,500 | mfu: -1.00 | total time: 6.33m\n",
      "step 00755 (93.11%) | loss: 1.239824 | lrm: 0.34 | dt: 538.39ms | tok/sec: 973,805 | mfu: -1.00 | total time: 6.34m\n",
      "step 00756 (93.25%) | loss: 1.229491 | lrm: 0.34 | dt: 465.51ms | tok/sec: 1,126,261 | mfu: -1.00 | total time: 6.35m\n",
      "step 00757 (93.37%) | loss: 1.229391 | lrm: 0.33 | dt: 554.04ms | tok/sec: 946,306 | mfu: -1.00 | total time: 6.36m\n",
      "step 00758 (93.47%) | loss: 1.219080 | lrm: 0.33 | dt: 465.68ms | tok/sec: 1,125,846 | mfu: -1.00 | total time: 6.36m\n",
      "step 00759 (93.59%) | loss: 1.216588 | lrm: 0.32 | dt: 478.36ms | tok/sec: 1,096,018 | mfu: -1.00 | total time: 6.37m\n",
      "step 00760 (93.72%) | loss: 1.224571 | lrm: 0.31 | dt: 471.79ms | tok/sec: 1,111,262 | mfu: -1.00 | total time: 6.38m\n",
      "step 00761 (93.85%) | loss: 1.229895 | lrm: 0.31 | dt: 475.59ms | tok/sec: 1,102,388 | mfu: -1.00 | total time: 6.39m\n",
      "step 00762 (93.96%) | loss: 1.221160 | lrm: 0.30 | dt: 624.13ms | tok/sec: 840,025 | mfu: -1.00 | total time: 6.40m\n",
      "step 00763 (94.07%) | loss: 1.225269 | lrm: 0.30 | dt: 466.65ms | tok/sec: 1,123,505 | mfu: -1.00 | total time: 6.41m\n",
      "step 00764 (94.19%) | loss: 1.224056 | lrm: 0.29 | dt: 472.59ms | tok/sec: 1,109,388 | mfu: -1.00 | total time: 6.41m\n",
      "step 00765 (94.30%) | loss: 1.222914 | lrm: 0.29 | dt: 543.66ms | tok/sec: 964,359 | mfu: -1.00 | total time: 6.42m\n",
      "step 00766 (94.43%) | loss: 1.222611 | lrm: 0.28 | dt: 530.33ms | tok/sec: 988,603 | mfu: -1.00 | total time: 6.43m\n",
      "step 00767 (94.57%) | loss: 1.224804 | lrm: 0.27 | dt: 532.66ms | tok/sec: 984,288 | mfu: -1.00 | total time: 6.44m\n",
      "step 00768 (94.69%) | loss: 1.214873 | lrm: 0.27 | dt: 531.81ms | tok/sec: 985,859 | mfu: -1.00 | total time: 6.45m\n",
      "step 00769 (94.81%) | loss: 1.210039 | lrm: 0.26 | dt: 543.17ms | tok/sec: 965,244 | mfu: -1.00 | total time: 6.46m\n",
      "step 00770 (94.94%) | loss: 1.216682 | lrm: 0.25 | dt: 549.83ms | tok/sec: 953,544 | mfu: -1.00 | total time: 6.47m\n",
      "step 00771 (95.05%) | loss: 1.214478 | lrm: 0.25 | dt: 524.32ms | tok/sec: 999,929 | mfu: -1.00 | total time: 6.48m\n",
      "step 00772 (95.18%) | loss: 1.218483 | lrm: 0.24 | dt: 466.80ms | tok/sec: 1,123,164 | mfu: -1.00 | total time: 6.48m\n",
      "step 00773 (95.30%) | loss: 1.222725 | lrm: 0.24 | dt: 478.30ms | tok/sec: 1,096,149 | mfu: -1.00 | total time: 6.49m\n",
      "step 00774 (95.42%) | loss: 1.214922 | lrm: 0.23 | dt: 472.01ms | tok/sec: 1,110,755 | mfu: -1.00 | total time: 6.50m\n",
      "step 00775 (95.56%) | loss: 1.220031 | lrm: 0.22 | dt: 472.41ms | tok/sec: 1,109,803 | mfu: -1.00 | total time: 6.51m\n",
      "step 00776 (95.68%) | loss: 1.217860 | lrm: 0.22 | dt: 535.83ms | tok/sec: 978,459 | mfu: -1.00 | total time: 6.52m\n",
      "step 00777 (95.78%) | loss: 1.211609 | lrm: 0.21 | dt: 466.64ms | tok/sec: 1,123,548 | mfu: -1.00 | total time: 6.52m\n",
      "step 00778 (95.91%) | loss: 1.209064 | lrm: 0.20 | dt: 479.77ms | tok/sec: 1,092,795 | mfu: -1.00 | total time: 6.53m\n",
      "step 00779 (96.03%) | loss: 1.206016 | lrm: 0.20 | dt: 545.33ms | tok/sec: 961,408 | mfu: -1.00 | total time: 6.54m\n",
      "step 00780 (96.14%) | loss: 1.211814 | lrm: 0.19 | dt: 464.70ms | tok/sec: 1,128,225 | mfu: -1.00 | total time: 6.55m\n",
      "step 00781 (96.28%) | loss: 1.231561 | lrm: 0.19 | dt: 533.29ms | tok/sec: 983,113 | mfu: -1.00 | total time: 6.56m\n",
      "step 00782 (96.39%) | loss: 1.227707 | lrm: 0.18 | dt: 465.98ms | tok/sec: 1,125,134 | mfu: -1.00 | total time: 6.57m\n",
      "step 00783 (96.52%) | loss: 1.223338 | lrm: 0.17 | dt: 530.48ms | tok/sec: 988,321 | mfu: -1.00 | total time: 6.58m\n",
      "step 00784 (96.65%) | loss: 1.221186 | lrm: 0.17 | dt: 466.68ms | tok/sec: 1,123,435 | mfu: -1.00 | total time: 6.58m\n",
      "step 00785 (96.78%) | loss: 1.226021 | lrm: 0.16 | dt: 540.42ms | tok/sec: 970,144 | mfu: -1.00 | total time: 6.59m\n",
      "step 00786 (96.89%) | loss: 1.230169 | lrm: 0.16 | dt: 467.54ms | tok/sec: 1,121,374 | mfu: -1.00 | total time: 6.60m\n",
      "step 00787 (97.02%) | loss: 1.217840 | lrm: 0.15 | dt: 541.11ms | tok/sec: 968,912 | mfu: -1.00 | total time: 6.61m\n",
      "step 00788 (97.15%) | loss: 1.221070 | lrm: 0.14 | dt: 466.11ms | tok/sec: 1,124,806 | mfu: -1.00 | total time: 6.62m\n",
      "step 00789 (97.28%) | loss: 1.224236 | lrm: 0.14 | dt: 477.89ms | tok/sec: 1,097,099 | mfu: -1.00 | total time: 6.62m\n",
      "step 00790 (97.41%) | loss: 1.221091 | lrm: 0.13 | dt: 471.05ms | tok/sec: 1,113,008 | mfu: -1.00 | total time: 6.63m\n",
      "step 00791 (97.55%) | loss: 1.218586 | lrm: 0.12 | dt: 536.71ms | tok/sec: 976,856 | mfu: -1.00 | total time: 6.64m\n",
      "step 00792 (97.67%) | loss: 1.226717 | lrm: 0.12 | dt: 534.13ms | tok/sec: 981,566 | mfu: -1.00 | total time: 6.65m\n",
      "step 00793 (97.79%) | loss: 1.218754 | lrm: 0.11 | dt: 466.92ms | tok/sec: 1,122,875 | mfu: -1.00 | total time: 6.66m\n",
      "step 00794 (97.92%) | loss: 1.226166 | lrm: 0.10 | dt: 479.68ms | tok/sec: 1,092,990 | mfu: -1.00 | total time: 6.67m\n",
      "step 00795 (98.04%) | loss: 1.217390 | lrm: 0.10 | dt: 470.34ms | tok/sec: 1,114,708 | mfu: -1.00 | total time: 6.67m\n",
      "step 00796 (98.16%) | loss: 1.217533 | lrm: 0.09 | dt: 479.94ms | tok/sec: 1,092,402 | mfu: -1.00 | total time: 6.68m\n",
      "step 00797 (98.30%) | loss: 1.217931 | lrm: 0.09 | dt: 471.87ms | tok/sec: 1,111,097 | mfu: -1.00 | total time: 6.69m\n",
      "step 00798 (98.41%) | loss: 1.215258 | lrm: 0.08 | dt: 473.97ms | tok/sec: 1,106,174 | mfu: -1.00 | total time: 6.70m\n",
      "step 00799 (98.53%) | loss: 1.210837 | lrm: 0.07 | dt: 471.30ms | tok/sec: 1,112,439 | mfu: -1.00 | total time: 6.71m\n",
      "step 00800 (98.64%) | loss: 1.212448 | lrm: 0.07 | dt: 474.10ms | tok/sec: 1,105,861 | mfu: -1.00 | total time: 6.71m\n",
      "step 00801 (98.78%) | loss: 1.214125 | lrm: 0.06 | dt: 552.83ms | tok/sec: 948,363 | mfu: -1.00 | total time: 6.72m\n",
      "step 00802 (98.91%) | loss: 1.207010 | lrm: 0.05 | dt: 527.44ms | tok/sec: 994,023 | mfu: -1.00 | total time: 6.73m\n",
      "step 00803 (99.04%) | loss: 1.210230 | lrm: 0.05 | dt: 466.56ms | tok/sec: 1,123,738 | mfu: -1.00 | total time: 6.74m\n",
      "step 00804 (99.18%) | loss: 1.211243 | lrm: 0.04 | dt: 484.27ms | tok/sec: 1,082,629 | mfu: -1.00 | total time: 6.75m\n",
      "step 00805 (99.31%) | loss: 1.230007 | lrm: 0.03 | dt: 469.80ms | tok/sec: 1,115,987 | mfu: -1.00 | total time: 6.76m\n",
      "step 00806 (99.44%) | loss: 1.232932 | lrm: 0.03 | dt: 477.73ms | tok/sec: 1,097,451 | mfu: -1.00 | total time: 6.76m\n",
      "step 00807 (99.58%) | loss: 1.228554 | lrm: 0.02 | dt: 545.18ms | tok/sec: 961,681 | mfu: -1.00 | total time: 6.77m\n",
      "step 00808 (99.71%) | loss: 1.230171 | lrm: 0.01 | dt: 467.03ms | tok/sec: 1,122,607 | mfu: -1.00 | total time: 6.78m\n",
      "step 00809 (99.85%) | loss: 1.223937 | lrm: 0.01 | dt: 480.01ms | tok/sec: 1,092,246 | mfu: -1.00 | total time: 6.79m\n",
      "step 00809 | Validation bpb: 0.3947\n",
      "[W1119 13:15:42.054043091 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "saved model to /home/ubuntu/mynanochat/mid_checkpoints/d20/model_000809.pt\n",
      "[W1119 13:15:42.253257148 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1119 13:15:42.254425247 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1119 13:15:42.255118289 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1119 13:15:42.261507596 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "saved metadata to /home/ubuntu/mynanochat/mid_checkpoints/d20/meta_000809.json\n",
      "saved optimizer to /home/ubuntu/mynanochat/mid_checkpoints/d20/optim_000809_rank0.pt\n",
      "Peak memory usage: 75422.02MiB\n",
      "Total training time: 6.79m\n",
      "Minimum validation bpb: 0.3947\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading history steps 86-86, summary, console lines 824-830 (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                step â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/dt â–â–ƒâ–â–â–ƒâ–â–â–ˆâ–ˆâ–…â–ƒâ–â–…â–ƒâ–â–†â–â–†â–ƒâ–„â–â–â–â–â–ƒâ–â–ˆâ–ƒâ–†â–â–â–â–„â–â–â–‚â–â–‚â–â–„\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss â–ˆâ–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–ƒâ–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/lrm â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–…â–„â–ƒâ–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/tok_per_sec â–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–â–â–„â–ˆâ–ƒâ–…â–ˆâ–‡â–…â–ƒâ–ˆâ–…â–‡â–ˆâ–‡â–‡â–…â–‡â–‡â–ƒâ–„â–‡â–…â–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–…â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val/bpb â–ˆâ–‚â–‚â–‚â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                step 809\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: total_training_time 407.27086\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/dt 0.4741\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 1.21245\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/lrm 0.06793\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/tok_per_sec 1105861\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val/bpb 0.39469\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mchallenge-28-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat/runs/hiwg9hf8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ericsilberstein-self/my-nanochat\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251119_130651-hiwg9hf8/logs\u001b[0m\n",
      "[W1119 13:15:51.081676247 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1119 13:15:51.089072704 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n",
      "[W1119 13:15:51.230440317 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_mid_train -- --model_tag=d20 --run=challenge-28-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662c0eab-8684-4ca7-b624-cd61bcbf8415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9242091f-e1e3-45d8-afd3-8117dce4bc29",
   "metadata": {},
   "source": [
    "^ The starting bpb validation is way off of the final from the training, this seems bad.\n",
    "\n",
    "End of base training: step 21400 | Validation bpb: 0.8135\n",
    "\n",
    "Start of this: step 00000 | Validation bpb: 0.6856"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec48fd01-9466-4970-a9fb-fbaf6b12b21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11a8a1ff-a287-49f9-a6dc-68b82be03bc9",
   "metadata": {},
   "source": [
    "Chat eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42db5a9e-9c6d-4fab-a872-6f0689a48345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1119 13:16:40.589000 251134 torch/distributed/run.py:803] \n",
      "W1119 13:16:40.589000 251134 torch/distributed/run.py:803] *****************************************\n",
      "W1119 13:16:40.589000 251134 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1119 13:16:40.589000 251134 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/mid_checkpoints/d20 with step 809\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "README.md: 9.00kB [00:00, 31.0MB/s]\n",
      "ARC-Easy/train-00000-of-00001.parquet: 100%|â–ˆ| 331k/331k [00:00<00:00, 1.21MB/s]\n",
      "ARC-Easy/test-00000-of-00001.parquet: 100%|â–ˆâ–ˆ| 346k/346k [00:00<00:00, 1.82MB/s]\n",
      "ARC-Easy/validation-00000-of-00001.parqu(â€¦): 100%|â–ˆ| 86.1k/86.1k [00:00<00:00, 5\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 2251/2251 [00:00<00:00, 407606.02 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2376/2376 [00:00<00:00, 649482.94 examples/s]\n",
      "Generating validation split: 100%|â–ˆ| 570/570 [00:00<00:00, 301634.28 examples/s]\n",
      "final: 1017/2376 (42.80%)\n",
      "ARC-Easy accuracy: 42.80%\n",
      "ARC-Challenge/train-00000-of-00001.parqu(â€¦): 100%|â–ˆ| 190k/190k [00:00<00:00, 812\n",
      "ARC-Challenge/test-00000-of-00001.parque(â€¦): 100%|â–ˆ| 204k/204k [00:00<00:00, 838\n",
      "ARC-Challenge/validation-00000-of-00001.(â€¦): 100%|â–ˆ| 55.7k/55.7k [00:00<00:00, 4\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 1119/1119 [00:00<00:00, 241655.14 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1172/1172 [00:00<00:00, 385184.48 examples/s]\n",
      "Generating validation split: 100%|â–ˆ| 299/299 [00:00<00:00, 169289.54 examples/s]\n",
      "final: 367/1172 (31.31%)\n",
      "ARC-Challenge accuracy: 31.31%\n",
      "final: 4568/14042 (32.53%)\n",
      "MMLU accuracy: 32.53%\n",
      "\u001b[KRank 6 | 3/165 (1.82%)]\n",
      "\u001b[KRank 4 | 3/165 (1.82%)]\n",
      "\u001b[KRank 5 | 2/165 (1.21%)]\n",
      "\u001b[KRank 2 | 6/165 (3.64%)]\n",
      "\u001b[KRank 1 | 6/165 (3.64%)]\n",
      "\u001b[KRank 0 | 7/165 (4.24%)]\n",
      "\u001b[KRank 7 | 2/164 (1.22%)]\n",
      "\u001b[KRank 3 | 8/165 (4.85%)]\n",
      "==================================================\n",
      "final: 37/1319 (2.81%)\n",
      "GSM8K accuracy: 2.81%\n",
      "README.md: 6.52kB [00:00, 21.4MB/s]\n",
      "openai_humaneval/test-00000-of-00001.par(â€¦): 100%|â–ˆ| 83.9k/83.9k [00:00<00:00, 3\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:00<00:00, 47718.76 examples/s]\n",
      "\u001b[KRank 5 | 3/20 (15.00%)]\n",
      "\u001b[KRank 4 | 1/20 (5.00%)]]\n",
      "\u001b[KRank 6 | 3/20 (15.00%)]\n",
      "\u001b[KRank 1 | 1/21 (4.76%)]]\n",
      "\u001b[KRank 2 | 3/21 (14.29%)]\n",
      "\u001b[KRank 3 | 0/21 (0.00%)]\n",
      "\u001b[KRank 7 | 3/20 (15.00%)]\n",
      "\u001b[KRank 0 | 2/21 (9.52%)]]\n",
      "==================================================\n",
      "final: 16/164 (9.76%)\n",
      "HumanEval accuracy: 9.76%\n",
      "\u001b[KRank 5 | 29/32 (90.62%)]]\n",
      "\u001b[KRank 4 | 30/32 (93.75%)]]\n",
      "\u001b[KRank 1 | 30/32 (93.75%)]]\n",
      "\u001b[KRank 6 | 32/32 (100.00%)]\n",
      "\u001b[KRank 2 | 31/32 (96.88%)]]\n",
      "\u001b[KRank 3 | 32/32 (100.00%)]\n",
      "\u001b[KRank 7 | 32/32 (100.00%)]\n",
      "\u001b[KRank 0 | 32/32 (100.00%)]\n",
      "==================================================\n",
      "final: 248/256 (96.88%)\n",
      "SpellingBee accuracy: 96.88%\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- --source=mid --model-tag=d20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e76c3-76a5-4954-96f6-98defc1c59d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9a4a760-2805-48a9-a5dd-3bb4dd95d7a0",
   "metadata": {},
   "source": [
    "Also run limited chat evals on base and mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88e42e91-a08d-4302-972f-c0c50c9b67e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1119 13:24:59.014000 254422 torch/distributed/run.py:803] \n",
      "W1119 13:24:59.014000 254422 torch/distributed/run.py:803] *****************************************\n",
      "W1119 13:24:59.014000 254422 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1119 13:24:59.014000 254422 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/base_checkpoints/d20 with step 21400\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "final: 25/100 (25.00%)\n",
      "ARC-Easy accuracy: 25.00%\n",
      "final: 30/100 (30.00%)\n",
      "ARC-Challenge accuracy: 30.00%\n",
      "final: 28/100 (28.00%)\n",
      "MMLU accuracy: 28.00%\n",
      "\u001b[KRank 3 | 0/13 (0.00%)]\n",
      "\u001b[KRank 1 | 0/13 (0.00%)]\n",
      "\u001b[KRank 0 | 0/13 (0.00%)]\n",
      "\u001b[KRank 4 | 0/12 (0.00%)]\n",
      "\u001b[KRank 6 | 0/12 (0.00%)]\n",
      "\u001b[KRank 7 | 0/12 (0.00%)]\n",
      "\u001b[KRank 5 | 0/12 (0.00%)]\n",
      "\u001b[KRank 2 | 0/13 (0.00%)]\n",
      "==================================================\n",
      "final: 0/100 (0.00%)\n",
      "GSM8K accuracy: 0.00%\n",
      "\u001b[KRank 6 | 0/12 (0.00%)]\n",
      "\u001b[KRank 4 | 0/12 (0.00%)]\n",
      "\u001b[KRank 2 | 0/13 (0.00%)]\n",
      "\u001b[KRank 1 | 0/13 (0.00%)]\n",
      "\u001b[KRank 7 | 0/12 (0.00%)]\n",
      "\u001b[KRank 5 | 0/12 (0.00%)]\n",
      "\u001b[KRank 3 | 0/13 (0.00%)]\n",
      "\u001b[KRank 0 | 0/13 (0.00%)]\n",
      "==================================================\n",
      "final: 0/100 (0.00%)\n",
      "HumanEval accuracy: 0.00%\n",
      "\u001b[KRank 4 | 0/12 (0.00%)]\n",
      "\u001b[KRank 6 | 0/12 (0.00%)]\n",
      "\u001b[KRank 2 | 0/13 (0.00%)]\n",
      "\u001b[KRank 3 | 0/13 (0.00%)]\n",
      "\u001b[KRank 1 | 0/13 (0.00%)]\n",
      "\u001b[KRank 7 | 0/12 (0.00%)]\n",
      "\u001b[KRank 5 | 0/12 (0.00%)]\n",
      "\u001b[KRank 0 | 0/13 (0.00%)]\n",
      "==================================================\n",
      "final: 0/100 (0.00%)\n",
      "SpellingBee accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- --source=base --model-tag=d20 --max-problems=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d8825f-50f9-4be2-a844-fe4de903da5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f541b64d-b44e-432e-83e8-55da2230e14a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1119 13:27:52.132000 256185 torch/distributed/run.py:803] \n",
      "W1119 13:27:52.132000 256185 torch/distributed/run.py:803] *****************************************\n",
      "W1119 13:27:52.132000 256185 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1119 13:27:52.132000 256185 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/mid_checkpoints/d20 with step 809\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "final: 39/100 (39.00%)\n",
      "ARC-Easy accuracy: 39.00%\n",
      "final: 33/100 (33.00%)\n",
      "ARC-Challenge accuracy: 33.00%\n",
      "final: 31/100 (31.00%)\n",
      "MMLU accuracy: 31.00%\n",
      "\u001b[KRank 7 | 0/12 (0.00%)]]\n",
      "\u001b[KRank 0 | 0/13 (0.00%)]\n",
      "\u001b[KRank 5 | 0/12 (0.00%)]\n",
      "\u001b[KRank 6 | 0/12 (0.00%)]\n",
      "\u001b[KRank 3 | 0/13 (0.00%)]\n",
      "\u001b[KRank 2 | 1/13 (7.69%)]\n",
      "\u001b[KRank 4 | 0/12 (0.00%)]\n",
      "\u001b[KRank 1 | 0/13 (0.00%)]\n",
      "==================================================\n",
      "final: 1/100 (1.00%)\n",
      "GSM8K accuracy: 1.00%\n",
      "\u001b[KRank 4 | 1/12 (8.33%)]]\n",
      "\u001b[KRank 7 | 1/12 (8.33%)]]\n",
      "\u001b[KRank 0 | 1/13 (7.69%)]\n",
      "\u001b[KRank 6 | 1/12 (8.33%)]\n",
      "\u001b[KRank 5 | 1/12 (8.33%)]\n",
      "\u001b[KRank 2 | 2/13 (15.38%)]\n",
      "\u001b[KRank 3 | 0/13 (0.00%)]\n",
      "\u001b[KRank 1 | 0/13 (0.00%)]\n",
      "==================================================\n",
      "final: 7/100 (7.00%)\n",
      "HumanEval accuracy: 7.00%\n",
      "\u001b[KRank 7 | 12/12 (100.00%)]\n",
      "\u001b[KRank 4 | 11/12 (91.67%)]\n",
      "\u001b[KRank 2 | 13/13 (100.00%)]\n",
      "\u001b[KRank 3 | 13/13 (100.00%)]\n",
      "\u001b[KRank 0 | 13/13 (100.00%)]\n",
      "\u001b[KRank 5 | 10/12 (83.33%)]]\n",
      "\u001b[KRank 6 | 12/12 (100.00%)]\n",
      "\u001b[KRank 1 | 13/13 (100.00%)]\n",
      "==================================================\n",
      "final: 97/100 (97.00%)\n",
      "SpellingBee accuracy: 97.00%\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_chat_eval -- --source=mid --model-tag=d20 --max-problems=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4876f88c-107f-46de-800a-e957ec3c1955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ee620c0-35eb-47a4-afaf-c48dfeb78ae4",
   "metadata": {},
   "source": [
    "Do the base eval on the mid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4db03321-2435-41ef-9625-eefb4fa5a152",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1119 13:29:45.629000 257849 torch/distributed/run.py:803] \n",
      "W1119 13:29:45.629000 257849 torch/distributed/run.py:803] *****************************************\n",
      "W1119 13:29:45.629000 257849 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1119 13:29:45.629000 257849 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/mid_checkpoints/d20 with step 809\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.4374 | centered: 0.2498 | time: 13.45s\n",
      "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.0671 | centered: 0.0671 | time: 3.09s\n",
      "Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.4806 | centered: 0.4806 | time: 26.67s\n",
      "Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.6090 | centered: 0.4787 | time: 4.08s\n",
      "Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.3157 | centered: 0.0876 | time: 1.95s\n",
      "Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.6300 | centered: 0.2600 | time: 0.18s\n",
      "Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.2351 | centered: 0.0438 | time: 2.14s\n",
      "Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.6616 | centered: 0.3232 | time: 3.22s\n",
      "Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.3000 | centered: 0.0667 | time: 0.72s\n",
      "Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.3497 | centered: 0.3497 | time: 6.57s\n",
      "Evaluating: hellaswag (10-shot, type: multiple_choice)... accuracy: 0.4303 | centered: 0.2404 | time: 25.88s\n",
      "Evaluating: winograd (0-shot, type: schema)... accuracy: 0.6154 | centered: 0.2308 | time: 0.37s\n",
      "Evaluating: winogrande (0-shot, type: schema)... accuracy: 0.5470 | centered: 0.0939 | time: 1.52s\n",
      "Evaluating: bigbench_dyck_languages (10-shot, type: language_modeling)... accuracy: 0.1330 | centered: 0.1330 | time: 1.66s\n",
      "Evaluating: agi_eval_lsat_ar (3-shot, type: multiple_choice)... accuracy: 0.2435 | centered: 0.0543 | time: 0.57s\n",
      "Evaluating: bigbench_cs_algorithms (10-shot, type: language_modeling)... accuracy: 0.4295 | centered: 0.4295 | time: 1.99s\n",
      "Evaluating: bigbench_operators (10-shot, type: language_modeling)... accuracy: 0.1429 | centered: 0.1429 | time: 0.35s\n",
      "Evaluating: bigbench_repeat_copy_logic (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.05s\n",
      "Evaluating: squad (10-shot, type: language_modeling)... accuracy: 0.2487 | centered: 0.2487 | time: 21.71s\n",
      "Evaluating: coqa (0-shot, type: language_modeling)... accuracy: 0.1919 | centered: 0.1919 | time: 11.34s\n",
      "Evaluating: boolq (10-shot, type: multiple_choice)... accuracy: 0.5713 | centered: -0.1283 | time: 8.15s\n",
      "Evaluating: bigbench_language_identification (10-shot, type: multiple_choice)... accuracy: 0.2501 | centered: 0.1750 | time: 41.31s\n",
      "CORE metric: 0.1918\n",
      "centered results:\n",
      "{\n",
      "    \"hellaswag_zeroshot\": 0.24981741110483804,\n",
      "    \"jeopardy\": 0.06707604974508286,\n",
      "    \"bigbench_qa_wikidata\": 0.48063579201698303,\n",
      "    \"arc_easy\": 0.47867560386657715,\n",
      "    \"arc_challenge\": 0.08759951591491699,\n",
      "    \"copa\": 0.25999999046325684,\n",
      "    \"commonsense_qa\": 0.04381655156612395,\n",
      "    \"piqa\": 0.3231773376464844,\n",
      "    \"openbook_qa\": 0.0666666825612386,\n",
      "    \"lambada_openai\": 0.3496991991996765,\n",
      "    \"hellaswag\": 0.24039034048716226,\n",
      "    \"winograd\": 0.23076927661895752,\n",
      "    \"winogrande\": 0.09392261505126953,\n",
      "    \"bigbench_dyck_languages\": 0.13300000131130219,\n",
      "    \"agi_eval_lsat_ar\": 0.05434781685471533,\n",
      "    \"bigbench_cs_algorithms\": 0.42954543232917786,\n",
      "    \"bigbench_operators\": 0.1428571492433548,\n",
      "    \"bigbench_repeat_copy_logic\": 0.0,\n",
      "    \"squad\": 0.24872279167175293,\n",
      "    \"coqa\": 0.19190780818462372,\n",
      "    \"boolq\": -0.1282793785396375,\n",
      "    \"bigbench_language_identification\": 0.17502748821959374\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_base_eval -- --source=mid --model-tag=d20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b62c8b-8d38-48c8-9150-33e7331987cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac83f8e5-6de9-4ee7-9f7c-5a1024c8043b",
   "metadata": {},
   "source": [
    "Repeat the base eval on the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81c5b55a-1ce7-4e41-9013-87d0aff4c14c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1119 13:33:21.170000 481715 torch/distributed/run.py:803] \n",
      "W1119 13:33:21.170000 481715 torch/distributed/run.py:803] *****************************************\n",
      "W1119 13:33:21.170000 481715 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1119 13:33:21.170000 481715 torch/distributed/run.py:803] *****************************************\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Autodetected device type: cuda\n",
      "/home/ubuntu/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "loading the model from /home/ubuntu/mynanochat/base_checkpoints/d20 with step 21400\n",
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n",
      "Evaluating: hellaswag_zeroshot (0-shot, type: multiple_choice)... accuracy: 0.4425 | centered: 0.2567 | time: 13.65s\n",
      "Evaluating: jeopardy (10-shot, type: language_modeling)... accuracy: 0.1186 | centered: 0.1186 | time: 3.22s\n",
      "Evaluating: bigbench_qa_wikidata (10-shot, type: language_modeling)... accuracy: 0.5366 | centered: 0.5366 | time: 26.73s\n",
      "Evaluating: arc_easy (10-shot, type: multiple_choice)... accuracy: 0.6469 | centered: 0.5292 | time: 4.13s\n",
      "Evaluating: arc_challenge (10-shot, type: multiple_choice)... accuracy: 0.3515 | centered: 0.1354 | time: 1.95s\n",
      "Evaluating: copa (0-shot, type: multiple_choice)... accuracy: 0.6900 | centered: 0.3800 | time: 0.18s\n",
      "Evaluating: commonsense_qa (10-shot, type: multiple_choice)... accuracy: 0.2301 | centered: 0.0377 | time: 2.13s\n",
      "Evaluating: piqa (10-shot, type: multiple_choice)... accuracy: 0.6795 | centered: 0.3591 | time: 3.17s\n",
      "Evaluating: openbook_qa (0-shot, type: multiple_choice)... accuracy: 0.3600 | centered: 0.1467 | time: 0.71s\n",
      "Evaluating: lambada_openai (0-shot, type: language_modeling)... accuracy: 0.3745 | centered: 0.3745 | time: 6.63s\n",
      "Evaluating: hellaswag (10-shot, type: multiple_choice)... accuracy: 0.4471 | centered: 0.2628 | time: 25.85s\n",
      "Evaluating: winograd (0-shot, type: schema)... accuracy: 0.6410 | centered: 0.2821 | time: 0.35s\n",
      "Evaluating: winogrande (0-shot, type: schema)... accuracy: 0.5280 | centered: 0.0560 | time: 1.55s\n",
      "Evaluating: bigbench_dyck_languages (10-shot, type: language_modeling)... accuracy: 0.1020 | centered: 0.1020 | time: 1.68s\n",
      "Evaluating: agi_eval_lsat_ar (3-shot, type: multiple_choice)... accuracy: 0.2217 | centered: 0.0272 | time: 0.58s\n",
      "Evaluating: bigbench_cs_algorithms (10-shot, type: language_modeling)... accuracy: 0.3583 | centered: 0.3583 | time: 2.00s\n",
      "Evaluating: bigbench_operators (10-shot, type: language_modeling)... accuracy: 0.1667 | centered: 0.1667 | time: 0.35s\n",
      "Evaluating: bigbench_repeat_copy_logic (10-shot, type: language_modeling)... accuracy: 0.0000 | centered: 0.0000 | time: 0.05s\n",
      "Evaluating: squad (10-shot, type: language_modeling)... accuracy: 0.2345 | centered: 0.2345 | time: 21.66s\n",
      "Evaluating: coqa (0-shot, type: language_modeling)... accuracy: 0.1813 | centered: 0.1813 | time: 11.45s\n",
      "Evaluating: boolq (10-shot, type: multiple_choice)... accuracy: 0.5070 | centered: -0.2973 | time: 8.18s\n",
      "Evaluating: bigbench_language_identification (10-shot, type: multiple_choice)... accuracy: 0.2530 | centered: 0.1782 | time: 41.42s\n",
      "CORE metric: 0.2012\n",
      "centered results:\n",
      "{\n",
      "    \"hellaswag_zeroshot\": 0.25672173500061035,\n",
      "    \"jeopardy\": 0.11856400221586227,\n",
      "    \"bigbench_qa_wikidata\": 0.5365877747535706,\n",
      "    \"arc_easy\": 0.5291806856791178,\n",
      "    \"arc_challenge\": 0.13538110256195068,\n",
      "    \"copa\": 0.3799999952316284,\n",
      "    \"commonsense_qa\": 0.03767403215169905,\n",
      "    \"piqa\": 0.35908591747283936,\n",
      "    \"openbook_qa\": 0.14666668574015299,\n",
      "    \"lambada_openai\": 0.3745391070842743,\n",
      "    \"hellaswag\": 0.26282942295074463,\n",
      "    \"winograd\": 0.28205132484436035,\n",
      "    \"winogrande\": 0.05603790283203125,\n",
      "    \"bigbench_dyck_languages\": 0.10200000554323196,\n",
      "    \"agi_eval_lsat_ar\": 0.027173891663551317,\n",
      "    \"bigbench_cs_algorithms\": 0.3583333194255829,\n",
      "    \"bigbench_operators\": 0.1666666716337204,\n",
      "    \"bigbench_repeat_copy_logic\": 0.0,\n",
      "    \"squad\": 0.2345316857099533,\n",
      "    \"coqa\": 0.18126018345355988,\n",
      "    \"boolq\": -0.2972798786665264,\n",
      "    \"bigbench_language_identification\": 0.1782178120775716\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 -m scripts.my_base_eval -- --source=base --model-tag=d20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b1ad9b-3ff3-4ed5-9ba2-f3c00e4579d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0d48807-c594-47b5-bec4-cd07e969083d",
   "metadata": {},
   "source": [
    "### Back on laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b6bc0a-d584-428c-a902-75117e3f7db0",
   "metadata": {},
   "source": [
    "Did those runs above, terminated the 8xH100 machine, now back on my mac. (Spent 0.82 hours on the machine and spent $19.53.)\n",
    "\n",
    "Copied the model files to `files_copied_from_servers/challenge-28-midtrain-d20`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca83214-c20d-4ca6-a9bc-ff3e4c872991",
   "metadata": {},
   "source": [
    "Things to figure out after the run:\n",
    "\n",
    "1) Why was final val bpb in base training (done in challenge 25) 0.8135 but the starting one in this mid training run was 0.6856?\n",
    "   \n",
    "2) Why was the final CORE metric in base training 0.2084 but the one I measured here on the same model was 0.2012?\n",
    "\n",
    "3) Can SpellingBee accuracy really be 96.88%?\n",
    "\n",
    "Will investigate these in `investigate-questions.ipynb`\n",
    "\n",
    "Other observations:\n",
    "\n",
    "- The training took around 9 minutes\n",
    "  \n",
    "- The training seemed to behave judging by the training loss and bpb validation (graphs pasted below)\n",
    "\n",
    "- I ran the base eval on the base model twice and both times it gave CORE = 0.2012 (expected them to match, sanity check)\n",
    "\n",
    "- I ran the base eval on the mid model and it gave CORE = 0.0198, a decrease (is this expected?)\n",
    "\n",
    "- I ran a short version of the chat eval on base and mid and saw:\n",
    "    - ARC-Easy accuracy: 25.00% -> 39.00%\n",
    "    - ARC-Challenge accuracy: 30.00% -> 33.00%\n",
    "    - MMLU accuracy: 28.00% -> 31.00%\n",
    "    - GSM8K accuracy: 0.00% -> 1.00%\n",
    "    - HumanEval accuracy: 0.00% -> 7.00%\n",
    "    - SpellingBee accuracy: 0.00% -> 97.00%"
   ]
  },
  {
   "attachments": {
    "634d2136-c143-44a3-ab50-94bebd38b749.png": {
     "image/png": "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"
    },
    "976c15fb-c639-4cba-9e47-bf7de02b749f.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "ccb8ed1a-f3eb-439a-bfa3-d29efa9b24dc",
   "metadata": {},
   "source": [
    "![image.png](attachment:634d2136-c143-44a3-ab50-94bebd38b749.png)\n",
    "\n",
    "![image.png](attachment:976c15fb-c639-4cba-9e47-bf7de02b749f.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f4052-c310-4fd8-ae23-326cad6def69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3017cda9-d1eb-45cc-a834-8790a291339a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
