{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e029249-3c35-40c4-b8cf-16f82c3c010f",
   "metadata": {},
   "source": [
    "My goal is to add the bytes-per-bit loss eval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe04ec-b997-4ee4-b567-f6d71cfd9c54",
   "metadata": {},
   "source": [
    "### Cache mapping from token id to number of bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d5f7b0-8264-4c19-a381-034a2769fd4a",
   "metadata": {},
   "source": [
    "I can see from the code I'll now need this but I skipped it when training the tokenizer. For now I'll do it right here in this notebook. Later I should move the token training and this into a `my_tok_train.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733be52c-fd6a-446b-99ce-2ca5a0b144eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "import os\n",
    "import torch\n",
    "from my_nanochat.my_tokenizer import get_tokenizer, get_token_bytes\n",
    "from my_nanochat.my_common import get_base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9bb715a-98b1-47b7-b721-82fbac09cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439fd6b6-d0d8-4919-9451-d07972c2d1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<bos>'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_special_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f7c90c-b543-46c2-843a-f0aab8a28654",
   "metadata": {},
   "source": [
    "^ This is the only special token for now because I didn't add the other ones yet. He adds many others but they only are needed later for finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e8406c-a283-4e8a-a035-91bc9d8b306e",
   "metadata": {},
   "source": [
    "Copying from the bottom of [tok_train.py](https://github.com/karpathy/nanochat/blob/master/scripts/tok_train.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f963af0-f843-49c5-b7cd-4144d57dc31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved token_bytes to /Users/ericsilberstein/.cache/my_nanochat/token_bytes.pt\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "special_set = set(tokenizer.get_special_tokens())\n",
    "token_strings = [tokenizer.decode([token_id]) for token_id in range(vocab_size)]\n",
    "token_bytes = []\n",
    "for token_id in range(vocab_size):\n",
    "    token_str = token_strings[token_id]\n",
    "    if token_str in special_set:\n",
    "        token_bytes.append(0)\n",
    "    else:\n",
    "        id_bytes = len(token_str.encode(\"utf-8\"))\n",
    "        token_bytes.append(id_bytes)\n",
    "token_bytes = torch.tensor(token_bytes, dtype=torch.int32, device=\"cpu\")\n",
    "token_bytes_path = os.path.join(get_base_dir(), \"token_bytes.pt\")\n",
    "torch.save(token_bytes, token_bytes_path)\n",
    "print(f\"Saved token_bytes to {token_bytes_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bdffa23-4941-4727-80bd-27c6fb5331bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E'] -> [1, 1, 1, 1, 1]\n",
      "[' all', ' U', ' cont', ' dis', ' comp'] -> [4, 2, 5, 4, 5]\n",
      "[' ubiquitous', 'Rich', ' tails', ' faults', ' disrupted'] -> [11, 4, 6, 7, 10]\n"
     ]
    }
   ],
   "source": [
    "for i in [65, 500, 20_000]:\n",
    "    print(f\"{token_strings[i:i+5]} -> {token_bytes[i:i+5].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb5cd1fb-c9d9-4d0a-a3af-f938b515c8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2204624\n",
      "drwxr-xr-x  3 ericsilberstein  staff    96B Nov 11 11:02 \u001b[34mbase_checkpoints\u001b[m\u001b[m\n",
      "-rw-r--r--  1 ericsilberstein  staff   826K Oct 28 16:02 my-tokenizer.pkl\n",
      "-rw-r--r--  1 ericsilberstein  staff    90M Oct 28 10:53 shard_00000.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff    90M Oct 28 10:53 shard_00001.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff    89M Oct 28 10:53 shard_00002.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff    89M Oct 28 10:53 shard_00003.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff    91M Oct 28 10:53 shard_00004.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff    89M Oct 28 10:53 shard_00005.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff    89M Oct 28 10:53 shard_00006.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff    89M Oct 28 10:53 shard_00007.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff    89M Oct 28 10:53 shard_00008.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff    89M Oct 28 10:53 shard_00009.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff    90M Nov  1 07:21 shard_00010.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff    90M Nov  1 07:22 shard_00011.parquet\n",
      "-rw-r--r--  1 ericsilberstein  staff   258K Nov 11 13:34 token_bytes.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {get_base_dir()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7623a65-5d51-439c-9bc2-2cb2a723f8ac",
   "metadata": {},
   "source": [
    "Also add `get_token_bytes()` to `my_tokenizer.py` to load the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b01f2a2e-a33e-4415-b817-328412c0b463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11,  4,  6,  7, 10], dtype=torch.int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_token_bytes()[20_000:20_005]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74668d48-d4e3-4f65-9087-63710c171df2",
   "metadata": {},
   "source": [
    "### Add ability to load a model\n",
    "\n",
    "To try out evaluation code I'll need a model. I could just instantiate a new one, but I might as well now add `load_checkpoint()` and `build_model()` to `my_checkpoint_manager.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4083ee51-c02e-4ceb-98ac-cdc409d55a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_nanochat.my_checkpoint_manager import load_checkpoint, build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d3cf43-24f7-47d2-9a8f-f34d130f76f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.join(get_base_dir(), \"base_checkpoints\", \"d4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9ffa7da-daa1-4b17-9927-2628422d9a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data, optimizer_data, meta_data = load_checkpoint(checkpoint_dir, step=10, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39286070-cecd-429a-92d0-1c4782f2db87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26,\n",
       " collections.OrderedDict,\n",
       " odict_keys(['transformer.wte.weight', 'transformer.h.0.attn.c_q.weight', 'transformer.h.0.attn.c_k.weight', 'transformer.h.0.attn.c_v.weight', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.1.attn.c_q.weight', 'transformer.h.1.attn.c_k.weight', 'transformer.h.1.attn.c_v.weight', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.2.attn.c_q.weight', 'transformer.h.2.attn.c_k.weight', 'transformer.h.2.attn.c_v.weight', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.3.attn.c_q.weight', 'transformer.h.3.attn.c_k.weight', 'transformer.h.3.attn.c_v.weight', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.weight', 'lm_head.weight']))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_data), type(model_data), model_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91ff8f0a-f3f3-4e31-a224-8b7b784b1185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step': 10,\n",
       " 'val_bpb': -1,\n",
       " 'model_config': {'sequence_len': 128,\n",
       "  'vocab_size': 65537,\n",
       "  'n_layer': 4,\n",
       "  'n_head': 2,\n",
       "  'n_kv_head': 2,\n",
       "  'n_embd': 256},\n",
       " 'user_config': {'device_type': '',\n",
       "  'depth': 4,\n",
       "  'max_seq_len': 128,\n",
       "  'num_iterations': 10,\n",
       "  'device_batch_size': 1,\n",
       "  'total_batch_size': 128,\n",
       "  'embedding_lr': 0.2,\n",
       "  'unembedding_lr': 0.004,\n",
       "  'weight_decay': 0.0,\n",
       "  'matrix_lr': 0.02,\n",
       "  'grad_clip': 1.0,\n",
       "  'warmup_ratio': 0.0,\n",
       "  'warmdown_ratio': 0.2,\n",
       "  'final_lr_frac': 0.0,\n",
       "  'eval_every': 250,\n",
       "  'eval_tokens': 10485760,\n",
       "  'core_metric_every': 2000,\n",
       "  'core_metric_max_per_task': 500,\n",
       "  'sample_every': 2000,\n",
       "  'model_tag': ''},\n",
       " 'device_batch_size': 1,\n",
       " 'max_seq_len': 128}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b8b9047-7f42-4233-91be-3ecb8e963a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with config: {'sequence_len': 128, 'vocab_size': 65537, 'n_layer': 4, 'n_head': 2, 'n_kv_head': 2, 'n_embd': 256}\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = os.path.join(get_base_dir(), \"base_checkpoints\", \"d4\")\n",
    "model, tokenizer, meta_data = build_model(checkpoint_dir, step=10, device=torch.get_default_device(), phase=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a9daa38-7e97-45c8-841e-64ab1dc2c69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(65537, 256)\n",
       "    (h): ModuleList(\n",
       "      (0-3): 4 x Block(\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8718467f-d63f-4d5e-a5cf-a62102740536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65537"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70131218-1ba9-4a8c-acfe-47d676520ae2",
   "metadata": {},
   "source": [
    "### Evaluate BPB\n",
    "\n",
    "Understand the code in [loss_eval.py](https://github.com/karpathy/nanochat/blob/master/nanochat/loss_eval.py)\n",
    "\n",
    "He says:\n",
    "\n",
    "```\n",
    "    Instead of the naive 'mean loss', this function returns the bits per byte (bpb),\n",
    "    which is a tokenization vocab size-indepedent metric, meaning you are still comparing\n",
    "    apples:apples if you change the vocab size. The way this works is that instead of just\n",
    "    calculating the average loss as usual, you calculate the sum loss, and indepependently\n",
    "    also the sum bytes (of all the target tokens), and divide. This normalizes the loss by\n",
    "    the number of bytes that the target tokens represent.\n",
    "\n",
    "    The added complexity is so that:\n",
    "    1) All \"normal\" tokens are normalized by the length of the token in bytes\n",
    "    2) No special tokens (e.g. <|bos|>) are included in the metric - they are masked out.\n",
    "    3) No actively masked tokens (using ignore_index of e.g. -1) are included in the metric.\n",
    "\n",
    "    In addition to evaluate_loss, we need the token_bytes tensor:\n",
    "    It is a 1D tensor of shape (vocab_size,), indicating the number of bytes for\n",
    "    each token id, or 0 if the token is to not be counted (e.g. special tokens).\n",
    "```\n",
    "\n",
    "The function signature is `def evaluate_bpb(model, batches, steps, token_bytes)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a6325781-4f5a-447b-baff-ccab3bf2d7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_nanochat.my_checkpoint_manager import build_model\n",
    "from my_nanochat.my_tokenizer import get_token_bytes\n",
    "from my_nanochat.my_dataloader import tokenizing_distributed_data_loader\n",
    "import torch.distributed as dist\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92c6a34-02ab-4b5a-a32b-25b5d98018ae",
   "metadata": {},
   "source": [
    "First get everything I'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ba575201-991e-4483-8899-85837e6027b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with config: {'sequence_len': 128, 'vocab_size': 65537, 'n_layer': 4, 'n_head': 2, 'n_kv_head': 2, 'n_embd': 256}\n"
     ]
    }
   ],
   "source": [
    "device = torch.get_default_device()\n",
    "checkpoint_dir = os.path.join(get_base_dir(), \"base_checkpoints\", \"d4\")\n",
    "model, tokenizer, meta_data = build_model(checkpoint_dir, step=10, device=device, phase=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "39d336d3-ed9f-49eb-8b06-a7c2f56c44ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = tokenizing_distributed_data_loader(\n",
    "    2, # so we can more easily see the batch dimension instead of meta_data[\"device_batch_size\"],\n",
    "    meta_data[\"max_seq_len\"],\n",
    "    split=\"val\",\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a9f7ef41-8e1b-408d-af3b-1b80a09ea041",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_bytes = get_token_bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1e4fec45-6693-4d07-83eb-2c05fd507310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for here\n",
    "# these are eval_steps, how many steps we need to get through the number of tokens we want to use for eval\n",
    "steps = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2869ae3-c9a6-4618-b581-7e1b31431407",
   "metadata": {},
   "source": [
    "Now step through the `evaluate_bpb()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "47a6db06-786a-4fed-8536-4821e22a27a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nat per token seems to be loss per token but don't really understand it\n",
    "total_nats = torch.tensor(0.0, dtype=torch.float32, device=model.get_device())\n",
    "total_bytes = torch.tensor(0.0, dtype=torch.int64, device=model.get_device())\n",
    "batch_iter = iter(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "987b1896-0531-4869-a674-fb0d841faa1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "158ee701-ed24-4e17-8ad8-6131f3a1466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#will be in a loop: for _ in range(steps):\n",
    "x, y = next(batch_iter)\n",
    "loss2d = model(x, y, loss_reduction='none') # (B, T) <-- believe wrong, already flat\n",
    "loss2d = loss2d.view(-1) # flatten\n",
    "y = y.view(-1) # flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "86b556a2-2149-4307-bb7c-58f83249f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (y.int() < 0).any():\n",
    "    # he comments:\n",
    "    # slightly more complex code path if some target tokens are ignore_index (e.g. -1)\n",
    "    # any target token < 0 is to be ignored: do NOT index token_bytes with negatives\n",
    "    valid = y >= 0\n",
    "    y_safe = torch.where(valid, y, torch.zeros_like(y)) # invalids will be 0\n",
    "    num_bytes2d = torch.where(         # not sure why called num_bytes2d\n",
    "        valid,\n",
    "        token_bytes[y_safe],\n",
    "        torch.zeros_like(y, dtype=token_bytes.dtype))\n",
    "    total_nats += (loss2d * (num_bytes2d > 0)).sum() # sums loss excluding invalid y\n",
    "    total_bytes += num_bytes2d.sum()\n",
    "else:\n",
    "    num_bytes2d = token_bytes[y]\n",
    "    total_nats += (loss2d * (num_bytes2d > 0)).sum()\n",
    "    total_bytes += num_bytes2d.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ac1bf592-dc70-4d9b-97f2-b7ac3b2f004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearly there...now some DDP stuff\n",
    "world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
    "if world_size > 1:\n",
    "    dist.all_reduce(total_nats, op=dist.ReduceOp.SUM)\n",
    "    dist.all_reduce(total_bytes, op=dist.ReduceOp.SUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "48fd3479-2436-451b-8fdf-75b334c9ab8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.929899219834058"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move both to cpu, calculate bpb and return\n",
    "total_nats = total_nats.item()\n",
    "total_bytes = total_bytes.item()\n",
    "if total_bytes == 0:\n",
    "    bpb = float('inf')\n",
    "bpb = total_nats / (math.log(2) * total_bytes)\n",
    "bpb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ba236-a8a9-48b9-953d-f5e8a98ce2ce",
   "metadata": {},
   "source": [
    "What's the intuition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "26e721e6-bf03-4346-9441-3dee00b818b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2593.397216796875, 1277)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_nats, total_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0c9bff2f-f6e9-4d92-b93c-4b49d4571146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.929899219834058"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so in this case:\n",
    "total_nats / (math.log(2) * total_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "deda2957-b82f-4cdc-946b-132dfc9433b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.540429891429462"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but let's say these tokens on average had fewer bytes, say more were for single letters and short words\n",
    "2643 / (math.log(2) * (total_bytes - 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c47540-7885-4c76-ba84-a6a3f1231165",
   "metadata": {},
   "source": [
    "TODO: come back to better understand this including what a nat is, why this is useful, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a64c619f-276c-41ec-81af-aa4ca3a3209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it into the function:\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_bpb(model, batches, steps, token_bytes):\n",
    "    total_nats = torch.tensor(0.0, dtype=torch.float32, device=model.get_device())\n",
    "    total_bytes = torch.tensor(0.0, dtype=torch.int64, device=model.get_device())\n",
    "    batch_iter = iter(batches)\n",
    "    for _ in range(steps):\n",
    "        x, y = next(batch_iter)\n",
    "        loss2d = model(x, y, loss_reduction='none') # (B, T) <-- believe wrong, already flat\n",
    "        loss2d = loss2d.view(-1) # flatten\n",
    "        y = y.view(-1) # flatten\n",
    "        if (y.int() < 0).any():\n",
    "            # he comments:\n",
    "            # slightly more complex code path if some target tokens are ignore_index (e.g. -1)\n",
    "            # any target token < 0 is to be ignored: do NOT index token_bytes with negatives\n",
    "            valid = y >= 0\n",
    "            y_safe = torch.where(valid, y, torch.zeros_like(y)) # invalids will be 0\n",
    "            num_bytes2d = torch.where(         # not sure why called num_bytes2d\n",
    "                valid,\n",
    "                token_bytes[y_safe],\n",
    "                torch.zeros_like(y, dtype=token_bytes.dtype))\n",
    "            total_nats += (loss2d * (num_bytes2d > 0)).sum() # sums loss excluding invalid y\n",
    "            total_bytes += num_bytes2d.sum()\n",
    "        else:\n",
    "            num_bytes2d = token_bytes[y]\n",
    "            total_nats += (loss2d * (num_bytes2d > 0)).sum()\n",
    "            total_bytes += num_bytes2d.sum()\n",
    "    # nearly there...now some DDP stuff\n",
    "    world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
    "    if world_size > 1:\n",
    "        dist.all_reduce(total_nats, op=dist.ReduceOp.SUM)\n",
    "        dist.all_reduce(total_bytes, op=dist.ReduceOp.SUM)\n",
    "    # move both to cpu, calculate bpb and return\n",
    "    total_nats = total_nats.item()\n",
    "    total_bytes = total_bytes.item()\n",
    "    if total_bytes == 0:\n",
    "        return float('inf')\n",
    "    bpb = total_nats / (math.log(2) * total_bytes)\n",
    "    return bpb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb793b25-c221-4d8e-89ae-577cd7e5e119",
   "metadata": {},
   "source": [
    "check if it matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "34395d76-a652-4138-ba90-8c93f1ede08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.929899219834058"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = tokenizing_distributed_data_loader(\n",
    "    2, # so we can more easily see the batch dimension instead of meta_data[\"device_batch_size\"],\n",
    "    meta_data[\"max_seq_len\"],\n",
    "    split=\"val\",\n",
    "    device=device)\n",
    "evaluate_bpb(model, batches, steps, token_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8953fe9-5bb5-4cb4-93ba-c360f647125e",
   "metadata": {},
   "source": [
    "move it to `my_loss_eval.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1b3df870-ec58-4970-90c7-fc997f0d11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_nanochat.my_loss_eval import evaluate_bpb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "903dd775-dda1-43a4-ae54-3dfaccdcfc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_nanochat.my_loss_eval'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_bpb.__module__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8001b3a4-f5cc-4a5d-ba95-7528120b94ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.929899219834058"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = tokenizing_distributed_data_loader(\n",
    "    2, # so we can more easily see the batch dimension instead of meta_data[\"device_batch_size\"],\n",
    "    meta_data[\"max_seq_len\"],\n",
    "    split=\"val\",\n",
    "    device=device)\n",
    "evaluate_bpb(model, batches, steps, token_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8307cf6c-37c8-45b7-8df3-4cf863927ba4",
   "metadata": {},
   "source": [
    "### Add evaluate BPB to training\n",
    "\n",
    "Update `my_base_train.py`, now try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7603735c-1f14-445f-81a9-34564c2b5736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONPATH\"] = \"../my_nanochat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a7c9a029-5437-47a7-ba45-4d2e3861845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overriding depth = 4\n",
      "overriding max_seq_len = 128\n",
      "overriding device_batch_size = 1\n",
      "overriding num_iterations = 20\n",
      "overriding total_batch_size = 128\n",
      "overriding eval_every = 5\n",
      "overriding eval_tokens = 1280\n",
      "user_config: {'device_type': '', 'depth': 4, 'max_seq_len': 128, 'num_iterations': 20, 'device_batch_size': 1, 'total_batch_size': 128, 'embedding_lr': 0.2, 'unembedding_lr': 0.004, 'weight_decay': 0.0, 'matrix_lr': 0.02, 'grad_clip': 1.0, 'warmup_ratio': 0.0, 'warmdown_ratio': 0.2, 'final_lr_frac': 0.0, 'eval_every': 5, 'eval_tokens': 1280, 'core_metric_every': 2000, 'core_metric_max_per_task': 500, 'sample_every': 2000, 'model_tag': ''}\n",
      "Autodetected device type: mps\n",
      "Vocab size: 65,537\n",
      "num_layers: 4\n",
      "model_dim: 256\n",
      "num_heads: 2\n",
      "num_kv_heads: 2\n",
      "Tokens / micro-batch / rank: 1 x 128 = 128\n",
      "Tokens / micro-batch: 128\n",
      "Total batch size 128 => gradient accumulation steps: 1\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(65537, 256)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x Block(\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
      ")\n",
      "Number of parameters: 36,700,672\n",
      "Using user-provided number of iterations: 20\n",
      "Total number of training tokens: 2,560\n",
      "tokens : param ratio: 0.00 (he has note that Chinchilla is ~20)\n",
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n",
      "x.shape is [1, 128], y.shape is [1, 128] -- should match\n",
      "step 00000 | Validation bpb: 3.6896\n",
      "step 00000/00020 (0.00%) | loss: 11.090370 | grad norm: 1.6017 | lrm: 1.00 | dt: 724.51ms | tok/sec: 176 | mfu: -1.00 | total time: 0.00m\n",
      "step 00001/00020 (5.00%) | loss: 11.069742 | grad norm: 81.2766 | lrm: 1.00 | dt: 66.23ms | tok/sec: 1,932 | mfu: -1.00 | total time: 0.00m\n",
      "step 00002/00020 (10.00%) | loss: 11.009183 | grad norm: 47.9126 | lrm: 1.00 | dt: 67.93ms | tok/sec: 1,884 | mfu: -1.00 | total time: 0.00m\n",
      "step 00003/00020 (15.00%) | loss: 10.928947 | grad norm: 125.5656 | lrm: 1.00 | dt: 70.68ms | tok/sec: 1,810 | mfu: -1.00 | total time: 0.00m\n",
      "step 00004/00020 (20.00%) | loss: 10.883070 | grad norm: 85.3734 | lrm: 1.00 | dt: 71.76ms | tok/sec: 1,783 | mfu: -1.00 | total time: 0.00m\n",
      "step 00005 | Validation bpb: 3.6136\n",
      "step 00005/00020 (25.00%) | loss: 10.852453 | grad norm: 108.2018 | lrm: 1.00 | dt: 76.26ms | tok/sec: 1,678 | mfu: -1.00 | total time: 0.00m\n",
      "step 00006/00020 (30.00%) | loss: 10.680094 | grad norm: 114.1231 | lrm: 1.00 | dt: 72.88ms | tok/sec: 1,756 | mfu: -1.00 | total time: 0.00m\n",
      "step 00007/00020 (35.00%) | loss: 10.595155 | grad norm: 260.4925 | lrm: 1.00 | dt: 71.89ms | tok/sec: 1,780 | mfu: -1.00 | total time: 0.00m\n",
      "step 00008/00020 (40.00%) | loss: 10.478086 | grad norm: 300.8248 | lrm: 1.00 | dt: 69.75ms | tok/sec: 1,835 | mfu: -1.00 | total time: 0.00m\n",
      "step 00009/00020 (45.00%) | loss: 10.282728 | grad norm: 201.4155 | lrm: 1.00 | dt: 75.51ms | tok/sec: 1,695 | mfu: -1.00 | total time: 0.00m\n",
      "step 00010 | Validation bpb: 3.3604\n",
      "step 00010/00020 (50.00%) | loss: 10.085369 | grad norm: 176.4012 | lrm: 1.00 | dt: 74.17ms | tok/sec: 1,725 | mfu: -1.00 | total time: 0.00m\n",
      "step 00011/00020 (55.00%) | loss: 9.878459 | grad norm: 244.7091 | lrm: 1.00 | dt: 74.95ms | tok/sec: 1,707 | mfu: -1.00 | total time: 0.00m\n",
      "step 00012/00020 (60.00%) | loss: 9.842812 | grad norm: 156.1121 | lrm: 1.00 | dt: 78.59ms | tok/sec: 1,628 | mfu: -1.00 | total time: 0.00m\n",
      "step 00013/00020 (65.00%) | loss: 9.784685 | grad norm: 93.8882 | lrm: 1.00 | dt: 79.66ms | tok/sec: 1,606 | mfu: -1.00 | total time: 0.00m\n",
      "step 00014/00020 (70.00%) | loss: 9.761606 | grad norm: 104.2607 | lrm: 1.00 | dt: 82.70ms | tok/sec: 1,547 | mfu: -1.00 | total time: 0.01m\n",
      "step 00015 | Validation bpb: 3.2332\n",
      "step 00015/00020 (75.00%) | loss: 9.674745 | grad norm: 96.0506 | lrm: 1.00 | dt: 90.65ms | tok/sec: 1,412 | mfu: -1.00 | total time: 0.01m\n",
      "step 00016/00020 (80.00%) | loss: 9.521922 | grad norm: 64.4122 | lrm: 1.00 | dt: 67.37ms | tok/sec: 1,899 | mfu: -1.00 | total time: 0.01m\n",
      "step 00017/00020 (85.00%) | loss: 9.393893 | grad norm: 74.3078 | lrm: 0.75 | dt: 66.86ms | tok/sec: 1,914 | mfu: -1.00 | total time: 0.01m\n",
      "step 00018/00020 (90.00%) | loss: 9.247991 | grad norm: 74.5192 | lrm: 0.50 | dt: 71.35ms | tok/sec: 1,793 | mfu: -1.00 | total time: 0.01m\n",
      "step 00019/00020 (95.00%) | loss: 9.071944 | grad norm: 93.5189 | lrm: 0.25 | dt: 72.32ms | tok/sec: 1,769 | mfu: -1.00 | total time: 0.01m\n",
      "step 00020 | Validation bpb: 3.1045\n",
      "TODO evaluate CORE metric\n",
      "TODO sample\n",
      "saved model to /Users/ericsilberstein/.cache/my_nanochat/base_checkpoints/d4/model_000020.pt\n",
      "saved optimizer to /Users/ericsilberstein/.cache/my_nanochat/base_checkpoints/d4/model_000020.pt\n",
      "saved metadata to /Users/ericsilberstein/.cache/my_nanochat/base_checkpoints/d4/meta_000020.json\n",
      "Peak memory usage: 0.00MiB\n",
      "Total training time: 0.01m\n",
      "Minimum validation bpb: 3.1045\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.my_base_train \\\n",
    "    --depth=4 \\\n",
    "    --max_seq_len=128 \\\n",
    "    --device_batch_size=1 \\\n",
    "    --num_iterations=20 \\\n",
    "    --total_batch_size=128 \\\n",
    "    --eval_every=5 \\\n",
    "    --eval_tokens=1280"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d562a1c-f666-4fff-a988-132b09eb4a09",
   "metadata": {},
   "source": [
    "^ See the validation bpb printed on every 5th step and the min printed at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5aeae4-3181-4126-bfac-48ce485e3e6b",
   "metadata": {},
   "source": [
    "Code added as part of this challenge:\n",
    "\n",
    "- Added `build_model()` to `my_checkpoint_manager.py` so can load saved checkpoints\n",
    "\n",
    "- Added `my_loss_eval.py` where the BPB stuff lives\n",
    "\n",
    "- Added `get_token_bytes()` to `my_tokenizer.py` to read the saved token ids -> byte count file **but did not** yet move the code to compute the mapping out of this notebook\n",
    "\n",
    "- Updated `my_base_train.py` to do BPB eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d809ae-d575-4984-9c64-28a617c44646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
