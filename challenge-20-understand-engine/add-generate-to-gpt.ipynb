{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5958e157-b964-4134-aa3a-bec4f8127b42",
   "metadata": {},
   "source": [
    "This is not the main notebook in this challenge. Start with `understand-engine.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d1fa86-2705-46ee-9750-a597116de63e",
   "metadata": {},
   "source": [
    "### Add `generate()` to `gpt.py`\n",
    "\n",
    "This is what he calls the \"naive autoregressive streaming inference.\" It doesn't use kv cache. It's similar to what I was doing by hand in earlier challenges with a few options like top_k and temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8103a85-f1ae-4cb5-9e96-1cf7ed02dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from my_nanochat.my_common import get_base_dir\n",
    "from my_nanochat.my_checkpoint_manager import build_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee30348-ee0e-4bdb-a412-91ad225133a9",
   "metadata": {},
   "source": [
    "#### Snippets of code to help follow the code in `generate()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89d6377-aa4e-46c3-beb8-5bf5cdef1578",
   "metadata": {},
   "source": [
    "top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d176ddc-4030-4d19-a448-2813cc2b3f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4741,  0.3539,  0.8900,  1.4903, -0.6973,  1.5847, -0.8399,  1.8785,\n",
       "         -0.2817,  2.0971],\n",
       "        [-0.5197,  0.9771, -1.7021, -0.0772, -1.1148,  0.0348,  1.7367,  0.9720,\n",
       "         -0.0634,  0.0729]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.randn((2,5,10)) # (B, T, V)\n",
    "logits = logits[:,-1,:]\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1245d528-6eda-487c-9926-a6fa6021f8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0971, 1.8785, 1.5847],\n",
       "        [1.7367, 0.9771, 0.9720]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v, _ = torch.topk(logits, 3)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8eb2a45-78f0-4e8d-9339-efcbbf79dba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5847],\n",
       "        [0.9720]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fbbfd5f-501e-4a8f-b4b0-b8b60bfd89ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -inf,   -inf,   -inf,   -inf,   -inf, 1.5847,   -inf, 1.8785,   -inf,\n",
       "         2.0971],\n",
       "        [  -inf, 0.9771,   -inf,   -inf,   -inf,   -inf, 1.7367, 0.9720,   -inf,\n",
       "           -inf]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6635404a-306a-44e6-9419-1ba64fc6ea3d",
   "metadata": {},
   "source": [
    "temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b440c081-2560-40f6-b532-8a10fd983335",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "966be33d-77fb-47d9-92c1-254e88b308fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -inf,   -inf,   -inf,   -inf,   -inf, 1.5847,   -inf, 1.8785,   -inf,\n",
       "         2.0971],\n",
       "        [  -inf, 0.9771,   -inf,   -inf,   -inf,   -inf, 1.7367, 0.9720,   -inf,\n",
       "           -inf]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bd5c25f-fff8-4bc5-9aa0-78892b550819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -inf,   -inf,   -inf,   -inf,   -inf, 1.9809,   -inf, 2.3481,   -inf,\n",
       "         2.6213],\n",
       "        [  -inf, 1.2214,   -inf,   -inf,   -inf,   -inf, 2.1709, 1.2150,   -inf,\n",
       "           -inf]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = logits / temperature; logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f384ac2-9c45-4b6f-bb1f-fc3039cc245b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2304, 0.0000, 0.3326, 0.0000,\n",
       "         0.4371],\n",
       "        [0.0000, 0.2184, 0.0000, 0.0000, 0.0000, 0.0000, 0.5645, 0.2170, 0.0000,\n",
       "         0.0000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = F.softmax(logits, dim=-1); probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fcd706-c8b1-444c-8a24-bd6c5f8e28ed",
   "metadata": {},
   "source": [
    "Compare a temp of 0.8 and 0.1. At a lower temps, probs gets more \"concentrated\" on the likely stuff. The higher the temp, the more likely we'll choose an unlikely token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef982384-45e0-402d-8f50-bb5a9a7edcc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4688, 0.5312])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.tensor([.3, .4]) / 0.8, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c796155-045b-413d-9354-811a46d3aaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2689, 0.7311])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.tensor([.3, .4]) / 0.1, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc011fcb-f6be-468c-9c37-6a41a3782da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "277fdaea-9ebf-4bd4-9c58-642b5994ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = torch.Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1e170db-bbb9-4d69-8e9a-62aad173729e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9],\n",
       "        [1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multinomial(probs, num_samples=1, generator=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0fa1ac3-50fc-4efe-8eef-583ef64bc869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7],\n",
       "        [1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multinomial(probs, num_samples=1, generator=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb4e07d-6081-4719-b077-905c1e3d69f7",
   "metadata": {},
   "source": [
    "#### Try `generate()` just added to `my_gpt.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2709baf-3d8e-4d58-8181-0787c16ba413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with config: {'sequence_len': 128, 'vocab_size': 65537, 'n_layer': 4, 'n_head': 2, 'n_kv_head': 2, 'n_embd': 256}\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = os.path.join(get_base_dir(), \"base_checkpoints\", \"d4\")\n",
    "model, tokenizer, meta_data = build_model(checkpoint_dir, step=10, device=torch.get_default_device(), phase=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2add5cf-b300-40ad-ae81-9d57c40cc123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[65536, 28466]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_tokens = tokenizer.encode('Hello', prepend=tokenizer.get_bos_token_id())\n",
    "prompt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8562f6df-21f8-4754-90e5-d484b2fce294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[65536, 28466, 49458, 331, 28461, 46644, 3247]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = []\n",
    "tokens.extend(prompt_tokens)\n",
    "for token in model.generate(prompt_tokens, max_tokens=5):\n",
    "    tokens.append(token)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "747a56bd-f1e6-4523-b0a2-b13b6700b1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos>Hello dependant on proudly fringes carry'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b6b654e-3ec6-4fd3-8a36-5770e40dbab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_it(**kwargs):\n",
    "    tokens = []\n",
    "    tokens.extend(prompt_tokens)\n",
    "    for token in model.generate(prompt_tokens, max_tokens=5, **kwargs):\n",
    "        tokens.append(token)\n",
    "    print(f\"{tokenizer.decode(tokens)} -- {kwargs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "027c150f-b484-43e0-bb3c-bfd6a249d844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Hello dependant on proudly fringes carry -- {}\n",
      "<bos>Hello assumptions Auss unoccupiedibial inactive -- {'seed': 43}\n",
      "<bos>Helloandy enchocr)|| inverter -- {'seed': 44}\n",
      "<bos>Hello most most most most most -- {'temperature': 0}\n",
      "<bos>Hello most most most most most -- {'temperature': 0, 'seed': 43}\n",
      "<bos>Hello most most most most most -- {'temperature': 0, 'seed': 44}\n",
      "<bos>Hello most most freight by most -- {'top_k': 5}\n",
      "<bos>Hello freight most most by freight -- {'top_k': 5, 'seed': 43}\n",
      "<bos>Hello most freight freight demand transport -- {'top_k': 5, 'seed': 44}\n",
      "<bos>Hello most most most most most -- {'top_k': 1}\n"
     ]
    }
   ],
   "source": [
    "try_it()\n",
    "try_it(seed=43)\n",
    "try_it(seed=44)\n",
    "try_it(temperature=0)\n",
    "try_it(temperature=0, seed=43)\n",
    "try_it(temperature=0, seed=44)\n",
    "try_it(top_k=5)\n",
    "try_it(top_k=5, seed=43)\n",
    "try_it(top_k=5, seed=44)\n",
    "try_it(top_k=1) # expect same as temperature = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
