{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdab72c7-6279-4e6f-aad2-915d3b6e872a",
   "metadata": {},
   "source": [
    "### Try trained model on other hardware\n",
    "\n",
    "As long as the trained d20 model can fit on MPS / mac / RTX4000 I believe it should work, at least up to a certain sequence length because the KV cache will need to keep getting bigger and bigger. \n",
    "\n",
    "Total parms is 560,988,160. Say they all took 4 bytes, which they don't, that's ~2 GiB which is well under the RTX4000 where we had 7+ GiB memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cb1d27a-19fd-4f04-873f-1befeccd861d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.08984375"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "560_988_160 * 4 / 1024 ** 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07df50cd-9116-4805-b02e-141b67a4276a",
   "metadata": {},
   "source": [
    "#### Let's try MPS on mac first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b3256b5-2819-4f11-bd50-fa4572d96a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_common import get_base_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d683d2fa-e7e1-493e-8feb-d3677196c350",
   "metadata": {},
   "source": [
    "Copied model files to the right place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa38b43-6f97-402c-b257-7cd288ee54ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4851336\n",
      "-rw-r--r--  1 ericsilberstein  staff   847B Nov 16 14:35 meta_021400.json\n",
      "-rw-r--r--  1 ericsilberstein  staff   1.9G Nov 16 14:35 model_021400.pt\n",
      "-rw-r--r--  1 ericsilberstein  staff   389M Nov 16 14:35 optim_021400_rank0.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {get_base_dir()}/base_checkpoints/d20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eb2d3a-f8ea-46e0-93b5-096dc6d91180",
   "metadata": {},
   "source": [
    "Copied tokenizer files to the right place (later, forgot to do this at first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f7af977-0d25-4e76-abec-5e22a7b1a593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 ericsilberstein  staff   826K Nov 16 15:51 /Users/ericsilberstein/.cache/my_nanochat/my-tokenizer.pkl\n",
      "-rw-r--r--  1 ericsilberstein  staff   258K Nov 16 15:51 /Users/ericsilberstein/.cache/my_nanochat/token_bytes.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {get_base_dir()}/*token*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "592e353c-73c5-49a9-8fb3-df03da500015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from my_nanochat.my_common import get_base_dir, autodetect_device_type\n",
    "from my_nanochat.my_checkpoint_manager import build_model\n",
    "device_type = autodetect_device_type()\n",
    "device = torch.device(device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4da8f3c-2536-4602-819e-1b786f9bb4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with config: {'sequence_len': 2048, 'vocab_size': 65536, 'n_layer': 20, 'n_head': 10, 'n_kv_head': 10, 'n_embd': 1280}\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = os.path.join(get_base_dir(), \"base_checkpoints\", \"d20\")\n",
    "model, tokenizer, meta_data = build_model(checkpoint_dir, step=21400, device=device, phase=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92760c2c-2679-4606-a03a-ed34aae3e607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7223a794-f379-4c38-8e47-92c5c04dcfc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step': 21400,\n",
       " 'val_bpb': -1,\n",
       " 'model_config': {'sequence_len': 2048,\n",
       "  'vocab_size': 65536,\n",
       "  'n_layer': 20,\n",
       "  'n_head': 10,\n",
       "  'n_kv_head': 10,\n",
       "  'n_embd': 1280},\n",
       " 'user_config': {'run': 'challenge-25-4',\n",
       "  'device_type': '',\n",
       "  'depth': 20,\n",
       "  'max_seq_len': 2048,\n",
       "  'num_iterations': -1,\n",
       "  'target_param_data_ratio': 20,\n",
       "  'device_batch_size': 32,\n",
       "  'total_batch_size': 524288,\n",
       "  'embedding_lr': 0.2,\n",
       "  'unembedding_lr': 0.004,\n",
       "  'weight_decay': 0.0,\n",
       "  'matrix_lr': 0.02,\n",
       "  'grad_clip': 1.0,\n",
       "  'warmup_ratio': 0.0,\n",
       "  'warmdown_ratio': 0.2,\n",
       "  'final_lr_frac': 0.0,\n",
       "  'eval_every': 250,\n",
       "  'eval_tokens': 10485760,\n",
       "  'core_metric_every': 2000,\n",
       "  'core_metric_max_per_task': 500,\n",
       "  'sample_every': 2000,\n",
       "  'model_tag': ''},\n",
       " 'device_batch_size': 32,\n",
       " 'max_seq_len': 2048}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09a386a2-c0b2-4b9d-be38-4713a783c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"The chemical symbol of gold is\",\n",
    "    \"If yesterday was Friday, then tomorrow will be\",\n",
    "    \"The opposite of hot is\",\n",
    "    \"The planets of the solar system are:\",\n",
    "    \"My favorite color is\",\n",
    "    \"If 5*x + 3 = 13, then x is\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb427bb-c128-442a-a628-3c858fa1fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_nanochat.my_engine import Engine\n",
    "engine = Engine(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43544527-b7ae-4255-8d56-c83e818908d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|bos|>The capital of France is Paris. It is the largest city in France and\n",
      "<|bos|>The chemical symbol of gold is Au. It is a soft, malleable, ductile\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be Monday. If tomorrow is Monday, then tomorrow will\n",
      "<|bos|>The opposite of hot is cold. The opposite of cold is hot. The\n",
      "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter,\n",
      "<|bos|>My favorite color is red. It is the color of blood, of\n",
      "<|bos|>If 5*x + 3 = 13, then x is a factor of 5. If 5*\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    tokens = tokenizer.encode(prompt, prepend=tokenizer.get_bos_token_id())\n",
    "    sample, _ = engine.generate_batch(tokens, num_samples=1, max_tokens=10, temperature=0)\n",
    "    print(tokenizer.decode(sample[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8cf51c-5d7a-4d71-b14b-860bb01bf86e",
   "metadata": {},
   "source": [
    "Do these match the sample from the final step during training?\n",
    "```\n",
    "<|bos|>The capital of France is Paris. It is the largest city in France and\n",
    "<|bos|>The chemical symbol of gold is Au. It is a soft, malleable, ductile\n",
    "<|bos|>If yesterday was Friday, then tomorrow will be Monday. If tomorrow is Monday, then tomorrow will\n",
    "<|bos|>The opposite of hot is cold. The opposite of cold is hot. The\n",
    "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter,\n",
    "<|bos|>My favorite color is red. It is the color of the blood of\n",
    "<|bos|>If 5*x + 3 = 13, then x is a factor of 5. If 5*\n",
    "```\n",
    "\n",
    "Yes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30c0d9c-b25f-4c8d-8a66-9923c3714830",
   "metadata": {},
   "source": [
    "For fun, generate a few longer samples for each prompt with a higher temperature and."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4d76353-1b75-4bdc-8cc9-24727c2a312a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|bos|>The capital of France is Paris, and it is the second largest city in the country. It is also the most populous city\n",
      "<|bos|>The capital of France is Paris, and the capital of France is Paris. Paris is the capital of France. Paris is the\n",
      "<|bos|>The capital of France is Paris. It is a city that is known for its beautiful architecture, its food, its music,\n",
      "<|bos|>The capital of France is Paris and it is the largest city in the country. The city is also the most populous city in\n",
      "<|bos|>The capital of France is Paris. It is the largest city in France and the second largest city in Europe. It is the\n",
      "\n",
      "<|bos|>The chemical symbol of gold is Au, and it is a soft, malleable, ductile, and ductilely reactive metal. It is\n",
      "<|bos|>The chemical symbol of gold is Au. It is a soft, malleable metal that is a bright yellow color. Gold is the most\n",
      "<|bos|>The chemical symbol of gold is Au. The atomic number of gold is 79. The atomic weight of gold is 199\n",
      "<|bos|>The chemical symbol of gold is Au and it is a soft, silvery metal. Gold is a very soft metal and it is also\n",
      "<|bos|>The chemical symbol of gold is Au. It is a soft, malleable, ductile metal that is malleable and ductile at room temperature.\n",
      "\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be Monday, and so on.\n",
      "If you are a student, then you should be able to use this\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be Monday, and so on. We can also say that a day is a unit of time. The\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be Monday. If tomorrow is Monday, then tomorrow will be Wednesday. If tomorrow is Wednesday, then tomorrow\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be Monday, and so on. The only thing that will change is the order of the days. If\n",
      "<|bos|>If yesterday was Friday, then tomorrow will be Monday. If yesterday was Saturday, then tomorrow will be Sunday. If yesterday was Monday, then tomorrow\n",
      "\n",
      "<|bos|>The opposite of hot is cold. The opposite of cold is hot. The opposite of hot is cold. The opposite of cold\n",
      "<|bos|>The opposite of hot is cold. The opposite of cold is hot. The opposite of hot is cold. The opposite of hot\n",
      "<|bos|>The opposite of hot is cold. The opposite of cold is hot. The opposite of hot is cold. The opposite of hot\n",
      "<|bos|>The opposite of hot is cold. That is, hot is cold and cold is hot. The opposite of hot is cold.\n",
      "<|bos|>The opposite of hot is cold. The opposite of cold is hot.\n",
      "The opposite of hot is cold. The opposite of hot\n",
      "\n",
      "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, and Pluto. The\n",
      "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, and Pluto. The\n",
      "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, and Pluto. The\n",
      "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, and Pluto. The\n",
      "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, and Pluto. The\n",
      "\n",
      "<|bos|>My favorite color is red. It is the color of blood, of life, of love, of blood, of blood\n",
      "<|bos|>My favorite color is red. I know that I have a hard time seeing the color red. I have a hard time\n",
      "<|bos|>My favorite color is red. I love red. I love red. I love red. Red is my favorite color.\n",
      "<|bos|>My favorite color is red. That’s why I love it so much. I love the way it makes me feel.\n",
      "<|bos|>My favorite color is red. It’s the color of blood, of course, but it’s also the color of the\n",
      "\n",
      "<|bos|>If 5*x + 3 = 13, then x is a square number. If 3*x + 5 = 13, then x is a\n",
      "<|bos|>If 5*x + 3 = 13, then x is a factor of 2. If 5*x + 3 = 10, then x\n",
      "<|bos|>If 5*x + 3 = 13, then x is a multiple of 5. If x is a multiple of 3, then x is a multiple\n",
      "<|bos|>If 5*x + 3 = 13, then x is a factor of 4. If 5*x + 3 = 19, then x\n",
      "<|bos|>If 5*x + 3 = 13, then x is a factor of 3. If 5*x + 3 = 13, then x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    tokens = tokenizer.encode(prompt, prepend=tokenizer.get_bos_token_id())\n",
    "    samples, _ = engine.generate_batch(tokens, num_samples=5, max_tokens=20, temperature=0.3)\n",
    "    for sample in samples:\n",
    "        print(tokenizer.decode(sample))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd34aa-416d-46de-80e5-bcdc84dae21d",
   "metadata": {},
   "source": [
    "Can we get it to OOM? Probably not just by max tokens because it will hit the assert on 10x max seq length for precomputed sin/cos before OOM, but perhaps by asking for lots of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0627614b-4433-4b9b-bd29-83b0a4cbf7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|bos|>The capital of France is Paris, and it is the second largest city in the country. It is also the most populous city in France. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the south of France, and it is located in the region of the Loire River. The city is located in the north of France, and it is located in the region of the Loire River. The city is located in the north of France, and it is located in the region of the Loire River. The city is located in the north of France, and it is located in the region of the Loire River. The city is located in the north of France, and\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(prompts[0], prepend=tokenizer.get_bos_token_id())\n",
    "samples, _ = engine.generate_batch(tokens, num_samples=1, max_tokens=1000, temperature=0.3)\n",
    "print(tokenizer.decode(samples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb908b67-7728-4295-a215-e5c4243825b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m samples, _ \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20479\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:186\u001b[0m, in \u001b[0;36mEngine.generate_batch\u001b[0;34m(self, tokens, num_samples, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m masks \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)]\n\u001b[1;32m    185\u001b[0m completed \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m] \u001b[38;5;241m*\u001b[39m num_samples\n\u001b[0;32m--> 186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token_column, token_masks \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(tokens, num_samples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (token, mask) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(token_column, token_masks)):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m completed[i]:\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:59\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 59\u001b[0m                 response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:153\u001b[0m, in \u001b[0;36mEngine.generate\u001b[0;34m(self, tokens, num_samples, max_tokens, temperature, top_k, seed)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# TODO I ADDED THIS ASSERT because I'm curious when it could ever not be 1\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m--> 153\u001b[0m     next_ids \u001b[38;5;241m=\u001b[39m \u001b[43msample_next_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, 1)\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     sampled_tokens \u001b[38;5;241m=\u001b[39m next_ids[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Process each row\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:92\u001b[0m, in \u001b[0;36msample_next_token\u001b[0;34m(logits, rng, temperature, top_k)\u001b[0m\n\u001b[1;32m     90\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m/\u001b[39m temperature\n\u001b[1;32m     91\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "samples, _ = engine.generate_batch(tokens, num_samples=1, max_tokens=20479, temperature=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451a42ad-9c12-4a77-bcae-8fcb9d6849fc",
   "metadata": {},
   "source": [
    "It's been maybe 10 minutes and it's still going. Going to cancel and use the non batch version so can watch progress and see if it's even worth waiting. My guess is at first the times per token generated will be consistent because the self attention stuff is just one part of all the operations, but as the sequence grows, the non-self-attention stuff stays constant but the self attention will dominate as the KV cache grows and it needs to do those big matrix multiplications. But will it slow linearly or with say the square of something?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03494e07-09e9-4fd4-90eb-80caf491ac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2015cf2b-5eec-436e-a49e-8949eccb7265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s 0\n",
      "6s 100\n",
      "5s 200\n",
      "5s 300\n",
      "5s 400\n",
      "6s 500\n",
      "6s 600\n",
      "6s 700\n",
      "8s 800\n",
      "7s 900\n",
      "7s 1000\n",
      "8s 1100\n",
      "12s 1200\n",
      "8s 1300\n",
      "7s 1400\n",
      "20s 1500\n",
      "11s 1600\n",
      "9s 1700\n",
      "9s 1800\n",
      "8s 1900\n",
      "8s 2000\n",
      "9s 2100\n",
      "8s 2200\n",
      "8s 2300\n",
      "11s 2400\n",
      "9s 2500\n",
      "9s 2600\n",
      "10s 2700\n",
      "9s 2800\n",
      "10s 2900\n",
      "10s 3000\n",
      "10s 3100\n",
      "10s 3200\n",
      "10s 3300\n",
      "10s 3400\n",
      "10s 3500\n",
      "11s 3600\n",
      "11s 3700\n",
      "11s 3800\n",
      "11s 3900\n",
      "11s 4000\n",
      "12s 4100\n",
      "12s 4200\n",
      "12s 4300\n",
      "12s 4400\n",
      "12s 4500\n",
      "13s 4600\n",
      "13s 4700\n",
      "13s 4800\n",
      "13s 4900\n",
      "13s 5000\n",
      "14s 5100\n",
      "14s 5200\n",
      "13s 5300\n",
      "14s 5400\n",
      "15s 5500\n",
      "14s 5600\n",
      "14s 5700\n",
      "14s 5800\n",
      "15s 5900\n",
      "15s 6000\n",
      "15s 6100\n",
      "15s 6200\n",
      "15s 6300\n",
      "15s 6400\n",
      "15s 6500\n",
      "15s 6600\n",
      "16s 6700\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (token_column, token_masks) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(engine\u001b[38;5;241m.\u001b[39mgenerate(tokens, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20479\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      4\u001b[0m         t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:59\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 59\u001b[0m                 response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:153\u001b[0m, in \u001b[0;36mEngine.generate\u001b[0;34m(self, tokens, num_samples, max_tokens, temperature, top_k, seed)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# TODO I ADDED THIS ASSERT because I'm curious when it could ever not be 1\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m--> 153\u001b[0m     next_ids \u001b[38;5;241m=\u001b[39m \u001b[43msample_next_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, 1)\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     sampled_tokens \u001b[38;5;241m=\u001b[39m next_ids[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Process each row\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:92\u001b[0m, in \u001b[0;36msample_next_token\u001b[0;34m(logits, rng, temperature, top_k)\u001b[0m\n\u001b[1;32m     90\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m/\u001b[39m temperature\n\u001b[1;32m     91\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "for i, (token_column, token_masks) in enumerate(engine.generate(tokens, num_samples=1, max_tokens=20479, temperature=0.3)):\n",
    "    if i % 100 == 0:\n",
    "        t1 = time.time()\n",
    "        delta = t1 - t0\n",
    "        t0 = t1\n",
    "        print(f\"{(delta):.0f}s {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3cd34c-28e0-4ada-9644-769e01074738",
   "metadata": {},
   "source": [
    "I'm surprised it's not slowing faster. But not sure how much it's worth digging into this and how mac and MPS works around this stuff. Going to interrupt and try many samples for fun and then move to trying things on RTX4000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a434d3ed-ba6e-4c42-bd53-e6454bb390af",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Invalid buffer size: 47.74 GiB",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (token_column, token_masks) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(engine\u001b[38;5;241m.\u001b[39mgenerate(tokens, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      4\u001b[0m         t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:38\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 38\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:127\u001b[0m, in \u001b[0;36mEngine.generate\u001b[0;34m(self, tokens, num_samples, max_tokens, temperature, top_k, seed)\u001b[0m\n\u001b[1;32m    125\u001b[0m kv_length_hint \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m+\u001b[39m max_tokens) \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msequence_len\n\u001b[1;32m    126\u001b[0m kv_cache_decode \u001b[38;5;241m=\u001b[39m KVCache(batch_size\u001b[38;5;241m=\u001b[39mnum_samples, seq_len\u001b[38;5;241m=\u001b[39mkv_length_hint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkv_model_kwargs)\n\u001b[0;32m--> 127\u001b[0m \u001b[43mkv_cache_decode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkv_cache_prefill\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m kv_cache_prefill\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# 3) initalize row states for each sample\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:33\u001b[0m, in \u001b[0;36mKVCache.prefill\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# init\u001b[39;00m\n\u001b[1;32m     32\u001b[0m dtype, device \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mkv_cache\u001b[38;5;241m.\u001b[39mdtype, other\u001b[38;5;241m.\u001b[39mkv_cache\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_cache \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# copy data\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_cache[:, :, :, :, :other\u001b[38;5;241m.\u001b[39mpos, :] \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mkv_cache\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Invalid buffer size: 47.74 GiB"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "for i, (token_column, token_masks) in enumerate(engine.generate(tokens, num_samples=50, max_tokens=5000, temperature=0.3)):\n",
    "    if i % 100 == 0:\n",
    "        t1 = time.time()\n",
    "        delta = t1 - t0\n",
    "        t0 = t1\n",
    "        print(f\"{(delta):.0f}s {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c3deba-d28a-4c12-8e48-fbf6825298d9",
   "metadata": {},
   "source": [
    "^ interesting, failed immediately when first allocating the KV cache. Try with fewer samples. This also makes me think that either it will fail right away, or it will fail at one of the points where it needs to grow the cache. Not really sure about that though and also not sure what \"Invalid buffer size\" is vs. OOM. Also prob better to restart kernel and then repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77607caa-003f-437d-938e-79558144bff6",
   "metadata": {},
   "source": [
    "-- restarted kernel and ran appropriate cells above --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "722155c5-8279-46c6-a4ef-f146893cb858",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Invalid buffer size: 47.74 GiB",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompts[\u001b[38;5;241m0\u001b[39m], prepend\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mget_bos_token_id())\n\u001b[1;32m      2\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (token_column, token_masks) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(engine\u001b[38;5;241m.\u001b[39mgenerate(tokens, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      5\u001b[0m         t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:38\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 38\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:127\u001b[0m, in \u001b[0;36mEngine.generate\u001b[0;34m(self, tokens, num_samples, max_tokens, temperature, top_k, seed)\u001b[0m\n\u001b[1;32m    125\u001b[0m kv_length_hint \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m+\u001b[39m max_tokens) \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msequence_len\n\u001b[1;32m    126\u001b[0m kv_cache_decode \u001b[38;5;241m=\u001b[39m KVCache(batch_size\u001b[38;5;241m=\u001b[39mnum_samples, seq_len\u001b[38;5;241m=\u001b[39mkv_length_hint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkv_model_kwargs)\n\u001b[0;32m--> 127\u001b[0m \u001b[43mkv_cache_decode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkv_cache_prefill\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m kv_cache_prefill\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# 3) initalize row states for each sample\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:33\u001b[0m, in \u001b[0;36mKVCache.prefill\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# init\u001b[39;00m\n\u001b[1;32m     32\u001b[0m dtype, device \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mkv_cache\u001b[38;5;241m.\u001b[39mdtype, other\u001b[38;5;241m.\u001b[39mkv_cache\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_cache \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# copy data\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_cache[:, :, :, :, :other\u001b[38;5;241m.\u001b[39mpos, :] \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mkv_cache\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Invalid buffer size: 47.74 GiB"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(prompts[0], prepend=tokenizer.get_bos_token_id())\n",
    "t0 = time.time()\n",
    "for i, (token_column, token_masks) in enumerate(engine.generate(tokens, num_samples=50, max_tokens=5000, temperature=0.3)):\n",
    "    if i % 100 == 0:\n",
    "        t1 = time.time()\n",
    "        delta = t1 - t0\n",
    "        t0 = t1\n",
    "        print(f\"{(delta):.0f}s {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f6cf2-4b12-4e96-a58d-ef312a671eec",
   "metadata": {},
   "source": [
    "-- restarted kernel and ran appropriate cells above --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ee209f9-1a2f-4fd9-af19-e40f8fbb0f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s 0\n",
      "7s 100\n",
      "7s 200\n",
      "8s 300\n",
      "9s 400\n",
      "9s 500\n",
      "10s 600\n",
      "10s 700\n",
      "11s 800\n",
      "13s 900\n",
      "13s 1000\n",
      "22s 1100\n",
      "18s 1200\n",
      "16s 1300\n",
      "16s 1400\n",
      "17s 1500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompts[\u001b[38;5;241m0\u001b[39m], prepend\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mget_bos_token_id())\n\u001b[1;32m      2\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (token_column, token_masks) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(engine\u001b[38;5;241m.\u001b[39mgenerate(tokens, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      5\u001b[0m         t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:59\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 59\u001b[0m                 response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:153\u001b[0m, in \u001b[0;36mEngine.generate\u001b[0;34m(self, tokens, num_samples, max_tokens, temperature, top_k, seed)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# TODO I ADDED THIS ASSERT because I'm curious when it could ever not be 1\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m--> 153\u001b[0m     next_ids \u001b[38;5;241m=\u001b[39m \u001b[43msample_next_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, 1)\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     sampled_tokens \u001b[38;5;241m=\u001b[39m next_ids[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Process each row\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:92\u001b[0m, in \u001b[0;36msample_next_token\u001b[0;34m(logits, rng, temperature, top_k)\u001b[0m\n\u001b[1;32m     90\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m/\u001b[39m temperature\n\u001b[1;32m     91\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(prompts[0], prepend=tokenizer.get_bos_token_id())\n",
    "t0 = time.time()\n",
    "for i, (token_column, token_masks) in enumerate(engine.generate(tokens, num_samples=5, max_tokens=5000, temperature=0.3)):\n",
    "    if i % 100 == 0:\n",
    "        t1 = time.time()\n",
    "        delta = t1 - t0\n",
    "        t0 = t1\n",
    "        print(f\"{(delta):.0f}s {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc202e1a-b611-419e-8e9c-930b5d22a822",
   "metadata": {},
   "source": [
    "Interrupted. Perhaps come back to this later. Move on to trying on RTX4000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdfa463-2137-471b-afc1-6de17b2b48fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6d4aff-30cd-4585-a87c-5819d7f9462d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa3d9a-0020-4131-8018-eee9e1fca1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ce53c-1ba1-4ba7-8e12-9cf136b5c8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9981b977-d2f2-44c9-a5d4-8adce9573874",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 14.89 GiB, other allocations: 3.98 MiB, max allowed: 18.13 GiB). Tried to allocate 4.77 GiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (token_column, token_masks) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(engine\u001b[38;5;241m.\u001b[39mgenerate(tokens, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      4\u001b[0m         t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:38\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 38\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:127\u001b[0m, in \u001b[0;36mEngine.generate\u001b[0;34m(self, tokens, num_samples, max_tokens, temperature, top_k, seed)\u001b[0m\n\u001b[1;32m    125\u001b[0m kv_length_hint \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m+\u001b[39m max_tokens) \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msequence_len\n\u001b[1;32m    126\u001b[0m kv_cache_decode \u001b[38;5;241m=\u001b[39m KVCache(batch_size\u001b[38;5;241m=\u001b[39mnum_samples, seq_len\u001b[38;5;241m=\u001b[39mkv_length_hint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkv_model_kwargs)\n\u001b[0;32m--> 127\u001b[0m \u001b[43mkv_cache_decode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkv_cache_prefill\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m kv_cache_prefill\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# 3) initalize row states for each sample\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ericsilberstein1-repos/nanogpt-learning/challenge-25-pretrain-d20/../my_nanochat/my_nanochat/my_engine.py:33\u001b[0m, in \u001b[0;36mKVCache.prefill\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# init\u001b[39;00m\n\u001b[1;32m     32\u001b[0m dtype, device \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mkv_cache\u001b[38;5;241m.\u001b[39mdtype, other\u001b[38;5;241m.\u001b[39mkv_cache\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_cache \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# copy data\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_cache[:, :, :, :, :other\u001b[38;5;241m.\u001b[39mpos, :] \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mkv_cache\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 14.89 GiB, other allocations: 3.98 MiB, max allowed: 18.13 GiB). Tried to allocate 4.77 GiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "for i, (token_column, token_masks) in enumerate(engine.generate(tokens, num_samples=5, max_tokens=5000, temperature=0.3)):\n",
    "    if i % 100 == 0:\n",
    "        t1 = time.time()\n",
    "        delta = t1 - t0\n",
    "        t0 = t1\n",
    "        print(f\"{(delta):.0f}s {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd74c019-0c53-4dda-9b08-1d4eae204254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bb9a54-487f-4b0f-bb08-e9ec413b4b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7082da6f-d328-4b9a-ba08-c63a69dd3e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(model.generate(tokenizer.encode(\"First take a right on Main St.\", prepend=tokenizer.get_bos_token_id()), max_tokens=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "025ed9bd-fca2-4b8d-a493-a5fe2034fea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46612fe-0213-45ad-9ee1-118d5dfdd553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
