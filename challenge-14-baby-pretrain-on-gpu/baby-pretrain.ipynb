{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1393b7b-957b-41b6-ae94-b43b64a98dde",
   "metadata": {},
   "source": [
    "This is starting as a copy of the `baby-pretrain` notebook from challenge 13 but I will run it with a GPU. Look at `getting-ready.ipynb` in this folder first, then once on GPU machine run `train-tokenizer.ipynb` then run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84c42a9-3a3b-4eb7-ad9c-7efb24ec124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_gpt import GPTConfig, GPT\n",
    "import my_nanochat.my_tokenizer\n",
    "from my_nanochat.my_dataset import text_iterator\n",
    "from my_nanochat.my_dataloader import tokenizing_distributed_data_loader\n",
    "from my_nanochat.my_tokenizer import MyTokenizer\n",
    "from my_nanochat.my_common import get_base_dir, autodetect_device_type\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "from contextlib import nullcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3860e927-8740-4d7f-b988-2e3ab274e0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac495a3-e877-495d-b066-1af8a5054f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39795aa5-fde4-4a1c-91ae-b2adac24cbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n"
     ]
    }
   ],
   "source": [
    "device_type = autodetect_device_type()\n",
    "device = device_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e093329f-3b67-4eb6-a77a-d7661572bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture\n",
    "depth = 4\n",
    "max_seq_len = 128\n",
    "\n",
    "# training horizon\n",
    "num_iterations = 1000\n",
    "\n",
    "# optimization (not sure why this section is called that yet)\n",
    "device_batch_size = 1\n",
    "total_batch_size = 128 # (device_batch_size x max_seq_len)\n",
    "\n",
    "# these next 4 are for the optimizers and we already saw them in setup_optimizers()\n",
    "embedding_lr = 0.2\n",
    "unembedding_lr = 0.004\n",
    "weight_decay = 0.0\n",
    "matrix_lr = 0.02\n",
    "\n",
    "grad_clip = 1.0\n",
    "\n",
    "# LR scheduler\n",
    "warmup_ratio = 0.0\n",
    "warmdown_ratio = 0.2\n",
    "final_lr_fraction = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa574717-4624-4113-9e7b-09ba15dbd367",
   "metadata": {},
   "outputs": [],
   "source": [
    "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "054930e0-0064-4716-87a0-47dbad221dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65537"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = my_nanochat.my_tokenizer.get_tokenizer()\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d6cdadd-1ac2-4f06-9a80-9e537af37cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 256, 2, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model kwargs are derived from desired depth of model\n",
    "num_layers = depth\n",
    "model_dim = depth * 64 # so for example in the default in GPTConfig it's 12 * 64 = 768)\n",
    "num_heads = max(1, (model_dim + 127) // 128)\n",
    "num_kv_heads = num_heads\n",
    "num_layers, model_dim, num_heads, num_kv_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1c32fb3-7e8c-41aa-9c6d-5752184af4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# figure out the needed gradient accumulation to reach the desired total batch size\n",
    "tokens_per_fwdbwd = device_batch_size * max_seq_len\n",
    "grad_accum_steps = total_batch_size // tokens_per_fwdbwd\n",
    "tokens_per_fwdbwd, grad_accum_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d05ad7ee-b60a-4ff3-b211-75589efdc510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(65537, 256)\n",
       "    (h): ModuleList(\n",
       "      (0-3): 4 x Block(\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config_kwargs = dict(\n",
    "    sequence_len=max_seq_len,\n",
    "    vocab_size=vocab_size, \n",
    "    n_layer=num_layers,\n",
    "    n_head=num_heads,\n",
    "    n_kv_head=num_kv_heads,\n",
    "    n_embd=model_dim,\n",
    ")\n",
    "with torch.device(\"meta\"):\n",
    "    model_config = GPTConfig(**model_config_kwargs)\n",
    "    model = GPT(model_config)\n",
    "model.to_empty(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c8e26d9-80cd-416b-8019-0ee441ad0e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "807f534c-c7d3-4c6a-a2f7-9fdd8245e139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b533ab91-21f6-47cf-a866-581122e84184",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_model = model # original, uncompiled model -- looks like even in this minimal notebook we might use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "504dd9a9-33eb-4443-ae6d-912297bf29ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): GPT(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(65537, 256)\n",
       "      (h): ModuleList(\n",
       "        (0-3): 4 x Block(\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          )\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
       "            (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.compile(model, dynamic=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eefdb34-e1b4-429e-8422-a6d2fd68af51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36700672"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_params = sum([param.numel() for param in model.parameters()])\n",
    "num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a15caa09-d893-4c76-a052-b76d11ca5402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = total_batch_size * num_iterations\n",
    "total_tokens # total number of training tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "add089d9-3c14-44ac-a643-a24362dd75b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n"
     ]
    }
   ],
   "source": [
    "# initialize optimizer\n",
    "optimizers = model.setup_optimizers(\n",
    "    unembedding_lr=unembedding_lr,\n",
    "    embedding_lr=embedding_lr,\n",
    "    matrix_lr=matrix_lr,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "adamw_optimizer, muon_optimizer = optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcb4aa47-d3a1-449c-bbd0-275c46f01fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128]), torch.Size([1, 128]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize DataLoader\n",
    "train_loader = tokenizing_distributed_data_loader(device_batch_size, max_seq_len, split=\"train\", device=device)\n",
    "x, y = next(train_loader)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a240c2a-40be-4dbe-83fc-fef60e87e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyperparameter scheulders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ad43dab-0257-453c-9844-03c6eea6f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate scheduler\n",
    "def get_lr_multiplier(it):\n",
    "    warmup_iters = round(warmup_ratio * num_iterations)\n",
    "    warmdown_iters = round(warmdown_ratio * num_iterations)\n",
    "    if it < warmup_iters:\n",
    "        return (it + 1) / warmup_iters\n",
    "    elif it <= num_iterations - warmdown_iters:\n",
    "        return 1.0\n",
    "    else:\n",
    "        progress = (num_iterations - it) / warmdown_iters\n",
    "        return progress * 1.0 + (1 - progress) * final_lr_fraction\n",
    "\n",
    "def get_muon_momentum(it):\n",
    "    frac = min(it / 300, 1)\n",
    "    momentum = (1 - frac) * 0.85  + frac * 0.95\n",
    "    return momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ff1f7-393e-4605-aef4-7c366c9ba390",
   "metadata": {},
   "source": [
    "### the training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3866ca86-9232-4e6b-9d77-75acb9c5e3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return torch._C._get_cublas_allow_tf32()\n",
      "/tmp/tmpirt7xef6/cuda_utils.c:6:10: fatal error: Python.h: No such file or directory\n",
      "    6 | #include <Python.h>\n",
      "      |          ^~~~~~~~~~\n",
      "compilation terminated.\n",
      "/tmp/tmp4admk3vq/cuda_utils.c:6:10: fatal error: Python.h: No such file or directory\n",
      "    6 | #include <Python.h>\n",
      "      |          ^~~~~~~~~~\n",
      "compilation terminated.\n",
      "W1101 15:18:53.863000 2010 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 11.090370178222656\n",
      "step: 10, loss: 7.807490825653076\n",
      "step: 20, loss: 7.521103858947754\n",
      "step: 30, loss: 8.662120819091797\n",
      "step: 40, loss: 6.418573379516602\n",
      "step: 50, loss: 6.389586448669434\n",
      "step: 60, loss: 8.384309768676758\n",
      "step: 70, loss: 8.795391082763672\n",
      "step: 80, loss: 8.155033111572266\n",
      "step: 90, loss: 9.78102970123291\n",
      "step: 100, loss: 5.909819602966309\n",
      "step: 110, loss: 7.7843122482299805\n",
      "step: 120, loss: 8.592817306518555\n",
      "step: 130, loss: 8.440689086914062\n",
      "step: 140, loss: 6.654191493988037\n",
      "step: 150, loss: 7.856637001037598\n",
      "step: 160, loss: 4.278852462768555\n",
      "step: 170, loss: 8.341126441955566\n",
      "step: 180, loss: 7.483657360076904\n",
      "step: 190, loss: 7.4629693031311035\n",
      "step: 200, loss: 8.194406509399414\n",
      "step: 210, loss: 7.528143405914307\n",
      "step: 220, loss: 7.579435348510742\n",
      "step: 230, loss: 7.5358381271362305\n",
      "step: 240, loss: 8.956267356872559\n",
      "step: 250, loss: 7.652376174926758\n",
      "step: 260, loss: 8.130544662475586\n",
      "step: 270, loss: 8.446453094482422\n",
      "step: 280, loss: 8.149810791015625\n",
      "step: 290, loss: 6.801801681518555\n",
      "step: 300, loss: 8.333602905273438\n",
      "step: 310, loss: 7.707497596740723\n",
      "step: 320, loss: 7.675753593444824\n",
      "step: 330, loss: 7.895292282104492\n",
      "step: 340, loss: 5.915315628051758\n",
      "step: 350, loss: 7.3078436851501465\n",
      "step: 360, loss: 7.813971042633057\n",
      "step: 370, loss: 7.190056324005127\n",
      "step: 380, loss: 7.370737075805664\n",
      "step: 390, loss: 7.926802635192871\n",
      "step: 400, loss: 7.872453689575195\n",
      "step: 410, loss: 8.277034759521484\n",
      "step: 420, loss: 8.176742553710938\n",
      "step: 430, loss: 7.629913330078125\n",
      "step: 440, loss: 9.264177322387695\n",
      "step: 450, loss: 9.01002311706543\n",
      "step: 460, loss: 7.987677574157715\n",
      "step: 470, loss: 7.007749557495117\n",
      "step: 480, loss: 7.5974836349487305\n",
      "step: 490, loss: 8.372331619262695\n",
      "step: 500, loss: 7.6648945808410645\n",
      "step: 510, loss: 8.329666137695312\n",
      "step: 520, loss: 7.513060569763184\n",
      "step: 530, loss: 7.516664505004883\n",
      "step: 540, loss: 7.656635284423828\n",
      "step: 550, loss: 7.104179382324219\n",
      "step: 560, loss: 7.436071872711182\n",
      "step: 570, loss: 7.052762031555176\n",
      "step: 580, loss: 8.940483093261719\n",
      "step: 590, loss: 7.175833702087402\n",
      "step: 600, loss: 7.8472394943237305\n",
      "step: 610, loss: 6.711062431335449\n",
      "step: 620, loss: 8.175430297851562\n",
      "step: 630, loss: 7.970753192901611\n",
      "step: 640, loss: 8.011968612670898\n",
      "step: 650, loss: 8.184711456298828\n",
      "step: 660, loss: 8.670510292053223\n",
      "step: 670, loss: 7.040701389312744\n",
      "step: 680, loss: 7.225078105926514\n",
      "step: 690, loss: 8.234890937805176\n",
      "step: 700, loss: 8.010204315185547\n",
      "step: 710, loss: 7.969862937927246\n",
      "step: 720, loss: 7.794208526611328\n",
      "step: 730, loss: 7.611539840698242\n",
      "step: 740, loss: 7.841681957244873\n",
      "step: 750, loss: 7.067933082580566\n",
      "step: 760, loss: 7.452524185180664\n",
      "step: 770, loss: 7.96704626083374\n",
      "step: 780, loss: 7.9556074142456055\n",
      "step: 790, loss: 7.608177185058594\n",
      "step: 800, loss: 6.783914566040039\n",
      "step: 810, loss: 7.679929733276367\n",
      "step: 820, loss: 7.3102641105651855\n",
      "step: 830, loss: 7.388186454772949\n",
      "step: 840, loss: 6.707064628601074\n",
      "step: 850, loss: 7.4295148849487305\n",
      "step: 860, loss: 7.289990425109863\n",
      "step: 870, loss: 6.858983993530273\n",
      "step: 880, loss: 7.207043647766113\n",
      "step: 890, loss: 8.709287643432617\n",
      "step: 900, loss: 6.947996139526367\n",
      "step: 910, loss: 7.282291412353516\n",
      "step: 920, loss: 7.266407489776611\n",
      "step: 930, loss: 6.991096496582031\n",
      "step: 940, loss: 7.8241400718688965\n",
      "step: 950, loss: 6.9323835372924805\n",
      "step: 960, loss: 7.236832618713379\n",
      "step: 970, loss: 8.357470512390137\n",
      "step: 980, loss: 8.13962173461914\n",
      "step: 990, loss: 6.864916801452637\n"
     ]
    }
   ],
   "source": [
    "for step in range(num_iterations):\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        with autocast_ctx: # before I added this in was getting BackendCompilerFailed: backend='inductor' raised: RuntimeError: expected mat1 and mat2 to have the same dtype, but got: c10::BFloat16 != float\n",
    "            loss = model(x, y)\n",
    "        train_loss = loss.detach()\n",
    "        loss = loss / grad_accum_steps # seems import to understand, but n/a here since grad_accum_steps is 1, see his comment\n",
    "        loss.backward()\n",
    "        x, y = next(train_loader)\n",
    "    # gradient clipping\n",
    "    if grad_clip > 0.0:\n",
    "        torch.nn.utils.clip_grad_norm_(orig_model.parameters(), grad_clip) # check exactly what this does, it's not a simple cip\n",
    "    # step optimizers\n",
    "    lrm = get_lr_multiplier(step)\n",
    "    for opt in optimizers:\n",
    "        for group in opt.param_groups:\n",
    "            group[\"lr\"] = group[\"initial_lr\"] * lrm\n",
    "    muon_momentum = get_muon_momentum(step)\n",
    "    for group in muon_optimizer.param_groups:\n",
    "        group[\"momentum\"] = muon_momentum\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"step: {step}, loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78bcbb02-8641-4811-8a07-fcee91f9013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(orig_model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6c40ca0-4553-4a24-8ef3-f2ad01b329d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 paperspace paperspace 109M Nov  1 15:24 model.pth\n"
     ]
    }
   ],
   "source": [
    "!ls -lh model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dfc1227-d8e2-4de0-9928-2bd46c7ba24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The person,| of| and\n",
      "He went to the| a| \n",
      "1 + 2 = 19|3|20\n",
      "first of the| a| to\n",
      "3 cats and 2,|.| and\n",
      "mom and the| a|,\n",
      "the red,| and|.\n",
      "She the|,| a\n"
     ]
    }
   ],
   "source": [
    "# show top 3 next tokens for a few prompts\n",
    "for prompt in ['The person', 'He went to', '1 + 2 = ', 'first of', '3 cats and 2', 'mom and', 'the red', 'She']:\n",
    "    with autocast_ctx:\n",
    "        logits = orig_model(torch.tensor([tokenizer.encode(prompt)], device=device)).detach()\n",
    "    top_3_next_tokens = torch.topk(logits[0,-1,:], k=3).indices\n",
    "    print(f\"{prompt}{'|'.join([tokenizer.decode([token]) for token in top_3_next_tokens])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb2958-7fbf-4148-98e1-d954a96961cb",
   "metadata": {},
   "source": [
    "^ one of the candidates for 1 + 2 is correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb6f4e6-247c-4502-a489-83d8db2ab90c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
