{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1393b7b-957b-41b6-ae94-b43b64a98dde",
   "metadata": {},
   "source": [
    "This is starting as a copy of the `baby-pretrain` notebook from challenge 13 but I will run it with a GPU. Look at `getting-ready.ipynb` in this folder first, then once on GPU machine run `train-tokenizer.ipynb` then run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84c42a9-3a3b-4eb7-ad9c-7efb24ec124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_gpt import GPTConfig, GPT\n",
    "import my_nanochat.my_tokenizer\n",
    "from my_nanochat.my_dataset import text_iterator\n",
    "from my_nanochat.my_dataloader import tokenizing_distributed_data_loader\n",
    "from my_nanochat.my_tokenizer import MyTokenizer\n",
    "from my_nanochat.my_common import get_base_dir\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "from contextlib import nullcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3860e927-8740-4d7f-b988-2e3ab274e0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac495a3-e877-495d-b066-1af8a5054f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e093329f-3b67-4eb6-a77a-d7661572bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "# model architecture\n",
    "depth = 4\n",
    "max_seq_len = 128\n",
    "\n",
    "# training horizon\n",
    "num_iterations = 1000\n",
    "\n",
    "# optimization (not sure why this section is called that yet)\n",
    "device_batch_size = 1\n",
    "total_batch_size = 128 # (device_batch_size x max_seq_len)\n",
    "\n",
    "# these next 4 are for the optimizers and we already saw them in setup_optimizers()\n",
    "embedding_lr = 0.2\n",
    "unembedding_lr = 0.004\n",
    "weight_decay = 0.0\n",
    "matrix_lr = 0.02\n",
    "\n",
    "grad_clip = 1.0\n",
    "\n",
    "# these 3 look like they control something we haven't seen yet, some type of LR adjuster\n",
    "warmup_ratio = 0.0\n",
    "warmdown_ratio = 0.2\n",
    "final_lr_fraction = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa574717-4624-4113-9e7b-09ba15dbd367",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_type = device\n",
    "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "054930e0-0064-4716-87a0-47dbad221dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65537"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = my_nanochat.my_tokenizer.get_tokenizer()\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d6cdadd-1ac2-4f06-9a80-9e537af37cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 256, 2, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model kwargs are derived from desired depth of model\n",
    "num_layers = depth\n",
    "model_dim = depth * 64 # so for example in the default in GPTConfig it's 12 * 64 = 768)\n",
    "num_heads = max(1, (model_dim + 127) // 128)\n",
    "num_kv_heads = num_heads\n",
    "num_layers, model_dim, num_heads, num_kv_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1c32fb3-7e8c-41aa-9c6d-5752184af4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# figure out the needed gradient accumulation to reach the desired total batch size\n",
    "tokens_per_fwdbwd = device_batch_size * max_seq_len\n",
    "grad_accum_steps = total_batch_size // tokens_per_fwdbwd\n",
    "tokens_per_fwdbwd, grad_accum_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d05ad7ee-b60a-4ff3-b211-75589efdc510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(65537, 256)\n",
       "    (h): ModuleList(\n",
       "      (0-3): 4 x Block(\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config_kwargs = dict(\n",
    "    sequence_len=max_seq_len,\n",
    "    vocab_size=vocab_size, \n",
    "    n_layer=num_layers,\n",
    "    n_head=num_heads,\n",
    "    n_kv_head=num_kv_heads,\n",
    "    n_embd=model_dim,\n",
    ")\n",
    "with torch.device(\"meta\"):\n",
    "    model_config = GPTConfig(**model_config_kwargs)\n",
    "    model = GPT(model_config)\n",
    "model.to_empty(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c8e26d9-80cd-416b-8019-0ee441ad0e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "807f534c-c7d3-4c6a-a2f7-9fdd8245e139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b533ab91-21f6-47cf-a866-581122e84184",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_model = model # original, uncompiled model -- looks like even in this minimal notebook we might use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "504dd9a9-33eb-4443-ae6d-912297bf29ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): GPT(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(65537, 256)\n",
       "      (h): ModuleList(\n",
       "        (0-3): 4 x Block(\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_q): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (c_k): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (c_v): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (c_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          )\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=256, out_features=1024, bias=False)\n",
       "            (c_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=256, out_features=65537, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.compile(model, dynamic=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eefdb34-e1b4-429e-8422-a6d2fd68af51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36700672"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_params = sum([param.numel() for param in model.parameters()])\n",
    "num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a15caa09-d893-4c76-a052-b76d11ca5402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = total_batch_size * num_iterations\n",
    "total_tokens # total number of training tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "add089d9-3c14-44ac-a643-a24362dd75b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling the LR for the AdamW parameters proportional to 1/sqrt(256/768) = 1.7320508075688774\n"
     ]
    }
   ],
   "source": [
    "# initialize optimizer\n",
    "optimizers = model.setup_optimizers(\n",
    "    unembedding_lr=unembedding_lr,\n",
    "    embedding_lr=embedding_lr,\n",
    "    matrix_lr=matrix_lr,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "adamw_optimizer, muon_optimizer = optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcb4aa47-d3a1-449c-bbd0-275c46f01fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128]), torch.Size([1, 128]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize DataLoader\n",
    "train_loader = tokenizing_distributed_data_loader(device_batch_size, max_seq_len, split=\"train\", device=device)\n",
    "x, y = next(train_loader)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a240c2a-40be-4dbe-83fc-fef60e87e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyperparameter scheulders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ad43dab-0257-453c-9844-03c6eea6f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate scheduler\n",
    "def get_lr_multiplier(it):\n",
    "    warmup_iters = round(warmup_ratio * num_iterations)\n",
    "    warmdown_iters = round(warmdown_ratio * num_iterations)\n",
    "    if it < warmup_iters:\n",
    "        return (it + 1) / warmup_iters\n",
    "    elif it <= num_iterations - warmdown_iters:\n",
    "        return 1.0\n",
    "    else:\n",
    "        progress = (num_iterations - it) / warmdown_iters\n",
    "        return progress * 1.0 + (1 - progress) * final_lr_fraction\n",
    "\n",
    "def get_muon_momentum(it):\n",
    "    frac = min(it / 300, 1)\n",
    "    momentum = (1 - frac) * 0.85  + frac * 0.95\n",
    "    return momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ff1f7-393e-4605-aef4-7c366c9ba390",
   "metadata": {},
   "source": [
    "### the training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3866ca86-9232-4e6b-9d77-75acb9c5e3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return torch._C._get_cublas_allow_tf32()\n",
      "/tmp/tmpnn2muj26/cuda_utils.c:6:10: fatal error: Python.h: No such file or directory\n",
      "    6 | #include <Python.h>\n",
      "      |          ^~~~~~~~~~\n",
      "compilation terminated.\n",
      "/tmp/tmp8927zd2g/cuda_utils.c:6:10: fatal error: Python.h: No such file or directory\n",
      "    6 | #include <Python.h>\n",
      "      |          ^~~~~~~~~~\n",
      "compilation terminated.\n",
      "W1101 13:18:45.481000 34730 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/paperspace/nanogpt-learning/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Quadro RTX 4000 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 11.090370178222656\n",
      "step: 10, loss: 7.793054103851318\n",
      "step: 20, loss: 7.423770427703857\n",
      "step: 30, loss: 8.758064270019531\n",
      "step: 40, loss: 6.388361930847168\n",
      "step: 50, loss: 6.340607643127441\n",
      "step: 60, loss: 8.319405555725098\n",
      "step: 70, loss: 8.77592658996582\n",
      "step: 80, loss: 8.11382007598877\n",
      "step: 90, loss: 9.916312217712402\n",
      "step: 100, loss: 5.970464706420898\n",
      "step: 110, loss: 7.699770927429199\n",
      "step: 120, loss: 8.744855880737305\n",
      "step: 130, loss: 8.410645484924316\n",
      "step: 140, loss: 6.649989128112793\n",
      "step: 150, loss: 7.96115255355835\n",
      "step: 160, loss: 4.203775405883789\n",
      "step: 170, loss: 8.52189826965332\n",
      "step: 180, loss: 7.574532508850098\n",
      "step: 190, loss: 7.510982990264893\n",
      "step: 200, loss: 8.312272071838379\n",
      "step: 210, loss: 7.491954326629639\n",
      "step: 220, loss: 7.4583659172058105\n",
      "step: 230, loss: 7.543668746948242\n",
      "step: 240, loss: 8.850008010864258\n",
      "step: 250, loss: 7.53331184387207\n",
      "step: 260, loss: 7.910400390625\n",
      "step: 270, loss: 8.707399368286133\n",
      "step: 280, loss: 7.886048316955566\n",
      "step: 290, loss: 6.750995635986328\n",
      "step: 300, loss: 8.240318298339844\n",
      "step: 310, loss: 8.006376266479492\n",
      "step: 320, loss: 7.638274192810059\n",
      "step: 330, loss: 7.8786234855651855\n",
      "step: 340, loss: 5.976543426513672\n",
      "step: 350, loss: 7.2490153312683105\n",
      "step: 360, loss: 7.690004348754883\n",
      "step: 370, loss: 7.184162616729736\n",
      "step: 380, loss: 7.603250503540039\n",
      "step: 390, loss: 8.007540702819824\n",
      "step: 400, loss: 7.712697505950928\n",
      "step: 410, loss: 8.24711799621582\n",
      "step: 420, loss: 7.912960529327393\n",
      "step: 430, loss: 7.465568542480469\n",
      "step: 440, loss: 9.1602144241333\n",
      "step: 450, loss: 9.04698657989502\n",
      "step: 460, loss: 8.013360977172852\n",
      "step: 470, loss: 6.813485145568848\n",
      "step: 480, loss: 7.451337814331055\n",
      "step: 490, loss: 8.41695499420166\n",
      "step: 500, loss: 7.618486404418945\n",
      "step: 510, loss: 8.358055114746094\n",
      "step: 520, loss: 7.593711853027344\n",
      "step: 530, loss: 7.5260419845581055\n",
      "step: 540, loss: 7.674736499786377\n",
      "step: 550, loss: 6.978667736053467\n",
      "step: 560, loss: 7.403815269470215\n",
      "step: 570, loss: 7.034603118896484\n",
      "step: 580, loss: 8.64175033569336\n",
      "step: 590, loss: 7.220578193664551\n",
      "step: 600, loss: 7.418738842010498\n",
      "step: 610, loss: 6.182453155517578\n",
      "step: 620, loss: 8.198529243469238\n",
      "step: 630, loss: 7.957447528839111\n",
      "step: 640, loss: 8.149293899536133\n",
      "step: 650, loss: 8.307926177978516\n",
      "step: 660, loss: 8.868487358093262\n",
      "step: 670, loss: 7.187427997589111\n",
      "step: 680, loss: 7.318117618560791\n",
      "step: 690, loss: 8.76368522644043\n",
      "step: 700, loss: 8.10586929321289\n",
      "step: 710, loss: 8.097406387329102\n",
      "step: 720, loss: 8.033432006835938\n",
      "step: 730, loss: 7.645416259765625\n",
      "step: 740, loss: 8.069672584533691\n",
      "step: 750, loss: 7.283357620239258\n",
      "step: 760, loss: 7.449538230895996\n",
      "step: 770, loss: 8.088075637817383\n",
      "step: 780, loss: 8.179011344909668\n",
      "step: 790, loss: 7.662439346313477\n",
      "step: 800, loss: 6.776966571807861\n",
      "step: 810, loss: 7.78969669342041\n",
      "step: 820, loss: 7.349331855773926\n",
      "step: 830, loss: 7.290156364440918\n",
      "step: 840, loss: 6.857539176940918\n",
      "step: 850, loss: 7.312412261962891\n",
      "step: 860, loss: 7.180580139160156\n",
      "step: 870, loss: 6.967726707458496\n",
      "step: 880, loss: 7.2274909019470215\n",
      "step: 890, loss: 8.731403350830078\n",
      "step: 900, loss: 7.006368160247803\n",
      "step: 910, loss: 7.240816593170166\n",
      "step: 920, loss: 7.34249210357666\n",
      "step: 930, loss: 6.951703071594238\n",
      "step: 940, loss: 7.68406867980957\n",
      "step: 950, loss: 6.985398292541504\n",
      "step: 960, loss: 7.208500385284424\n",
      "step: 970, loss: 8.288951873779297\n",
      "step: 980, loss: 8.135839462280273\n",
      "step: 990, loss: 7.014163970947266\n"
     ]
    }
   ],
   "source": [
    "for step in range(num_iterations):\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        with autocast_ctx: # before I added this in was getting BackendCompilerFailed: backend='inductor' raised: RuntimeError: expected mat1 and mat2 to have the same dtype, but got: c10::BFloat16 != float\n",
    "            loss = model(x, y)\n",
    "        train_loss = loss.detach()\n",
    "        loss = loss / grad_accum_steps # seems import to understand, but n/a here since grad_accum_steps is 1, see his comment\n",
    "        loss.backward()\n",
    "        x, y = next(train_loader)\n",
    "    # gradient clipping\n",
    "    if grad_clip > 0.0:\n",
    "        torch.nn.utils.clip_grad_norm_(orig_model.parameters(), grad_clip) # check exactly what this does, it's not a simple cip\n",
    "    # step optimizers\n",
    "    lrm = get_lr_multiplier(step)\n",
    "    for opt in optimizers:\n",
    "        for group in opt.param_groups:\n",
    "            group[\"lr\"] = group[\"initial_lr\"] * lrm\n",
    "    muon_momentum = get_muon_momentum(step)\n",
    "    for group in muon_optimizer.param_groups:\n",
    "        group[\"momentum\"] = muon_momentum\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"step: {step}, loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78bcbb02-8641-4811-8a07-fcee91f9013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(orig_model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6c40ca0-4553-4a24-8ef3-f2ad01b329d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 paperspace paperspace 109M Nov  1 13:23 model.pth\n"
     ]
    }
   ],
   "source": [
    "!ls -lh model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dfc1227-d8e2-4de0-9928-2bd46c7ba24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The person,|.| and\n",
      "He went to the| be| a\n",
      "1 + 2 = 19|20|16\n",
      "first of the| a|,\n",
      "3 cats and 2,|.| and\n",
      "mom and the| a| \n",
      "the red,| and|.\n",
      "She,| of|.\n"
     ]
    }
   ],
   "source": [
    "# show top 3 next tokens for a few prompts\n",
    "for prompt in ['The person', 'He went to', '1 + 2 = ', 'first of', '3 cats and 2', 'mom and', 'the red', 'She']:\n",
    "    with autocast_ctx:\n",
    "        logits = orig_model(torch.tensor([tokenizer.encode(prompt)], device=device)).detach()\n",
    "    top_3_next_tokens = torch.topk(logits[0,-1,:], k=3).indices\n",
    "    print(f\"{prompt}{'|'.join([tokenizer.decode([token]) for token in top_3_next_tokens])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b890f9-4732-4789-8e58-4270c441bd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
