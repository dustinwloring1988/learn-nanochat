{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1393b7b-957b-41b6-ae94-b43b64a98dde",
   "metadata": {},
   "source": [
    "This is starting as a copy of the `baby-pretrain` notebook from challenge 13 but I will run it with a GPU. Look at `getting-ready.ipynb` in this folder first, then once on GPU machine run `train-tokenizer.ipynb` then run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c42a9-3a3b-4eb7-ad9c-7efb24ec124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_gpt import GPTConfig, GPT\n",
    "import my_nanochat.my_tokenizer\n",
    "from my_nanochat.my_dataset import text_iterator\n",
    "from my_nanochat.my_dataloader import tokenizing_distributed_data_loader\n",
    "from my_nanochat.my_tokenizer import MyTokenizer\n",
    "from my_nanochat.my_common import get_base_dir, autodetect_device_type\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "from contextlib import nullcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3860e927-8740-4d7f-b988-2e3ab274e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac495a3-e877-495d-b066-1af8a5054f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39795aa5-fde4-4a1c-91ae-b2adac24cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_type = autodetect_device_type()\n",
    "device = device_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093329f-3b67-4eb6-a77a-d7661572bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture\n",
    "depth = 4\n",
    "max_seq_len = 128\n",
    "\n",
    "# training horizon\n",
    "num_iterations = 1000\n",
    "\n",
    "# optimization (not sure why this section is called that yet)\n",
    "device_batch_size = 1\n",
    "total_batch_size = 128 # (device_batch_size x max_seq_len)\n",
    "\n",
    "# these next 4 are for the optimizers and we already saw them in setup_optimizers()\n",
    "embedding_lr = 0.2\n",
    "unembedding_lr = 0.004\n",
    "weight_decay = 0.0\n",
    "matrix_lr = 0.02\n",
    "\n",
    "grad_clip = 1.0\n",
    "\n",
    "# LR scheduler\n",
    "warmup_ratio = 0.0\n",
    "warmdown_ratio = 0.2\n",
    "final_lr_fraction = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa574717-4624-4113-9e7b-09ba15dbd367",
   "metadata": {},
   "outputs": [],
   "source": [
    "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054930e0-0064-4716-87a0-47dbad221dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = my_nanochat.my_tokenizer.get_tokenizer()\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6cdadd-1ac2-4f06-9a80-9e537af37cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model kwargs are derived from desired depth of model\n",
    "num_layers = depth\n",
    "model_dim = depth * 64 # so for example in the default in GPTConfig it's 12 * 64 = 768)\n",
    "num_heads = max(1, (model_dim + 127) // 128)\n",
    "num_kv_heads = num_heads\n",
    "num_layers, model_dim, num_heads, num_kv_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c32fb3-7e8c-41aa-9c6d-5752184af4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out the needed gradient accumulation to reach the desired total batch size\n",
    "tokens_per_fwdbwd = device_batch_size * max_seq_len\n",
    "grad_accum_steps = total_batch_size // tokens_per_fwdbwd\n",
    "tokens_per_fwdbwd, grad_accum_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ad7ee-b60a-4ff3-b211-75589efdc510",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_kwargs = dict(\n",
    "    sequence_len=max_seq_len,\n",
    "    vocab_size=vocab_size, \n",
    "    n_layer=num_layers,\n",
    "    n_head=num_heads,\n",
    "    n_kv_head=num_kv_heads,\n",
    "    n_embd=model_dim,\n",
    ")\n",
    "with torch.device(\"meta\"):\n",
    "    model_config = GPTConfig(**model_config_kwargs)\n",
    "    model = GPT(model_config)\n",
    "model.to_empty(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8e26d9-80cd-416b-8019-0ee441ad0e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f534c-c7d3-4c6a-a2f7-9fdd8245e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b533ab91-21f6-47cf-a866-581122e84184",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_model = model # original, uncompiled model -- looks like even in this minimal notebook we might use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504dd9a9-33eb-4443-ae6d-912297bf29ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.compile(model, dynamic=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eefdb34-e1b4-429e-8422-a6d2fd68af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum([param.numel() for param in model.parameters()])\n",
    "num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15caa09-d893-4c76-a052-b76d11ca5402",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = total_batch_size * num_iterations\n",
    "total_tokens # total number of training tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add089d9-3c14-44ac-a643-a24362dd75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize optimizer\n",
    "optimizers = model.setup_optimizers(\n",
    "    unembedding_lr=unembedding_lr,\n",
    "    embedding_lr=embedding_lr,\n",
    "    matrix_lr=matrix_lr,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "adamw_optimizer, muon_optimizer = optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb4aa47-d3a1-449c-bbd0-275c46f01fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize DataLoader\n",
    "train_loader = tokenizing_distributed_data_loader(device_batch_size, max_seq_len, split=\"train\", device=device)\n",
    "x, y = next(train_loader)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a240c2a-40be-4dbe-83fc-fef60e87e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyperparameter scheulders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad43dab-0257-453c-9844-03c6eea6f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate scheduler\n",
    "def get_lr_multiplier(it):\n",
    "    warmup_iters = round(warmup_ratio * num_iterations)\n",
    "    warmdown_iters = round(warmdown_ratio * num_iterations)\n",
    "    if it < warmup_iters:\n",
    "        return (it + 1) / warmup_iters\n",
    "    elif it <= num_iterations - warmdown_iters:\n",
    "        return 1.0\n",
    "    else:\n",
    "        progress = (num_iterations - it) / warmdown_iters\n",
    "        return progress * 1.0 + (1 - progress) * final_lr_fraction\n",
    "\n",
    "def get_muon_momentum(it):\n",
    "    frac = min(it / 300, 1)\n",
    "    momentum = (1 - frac) * 0.85  + frac * 0.95\n",
    "    return momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ff1f7-393e-4605-aef4-7c366c9ba390",
   "metadata": {},
   "source": [
    "### the training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866ca86-9232-4e6b-9d77-75acb9c5e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(num_iterations):\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        with autocast_ctx: # before I added this in was getting BackendCompilerFailed: backend='inductor' raised: RuntimeError: expected mat1 and mat2 to have the same dtype, but got: c10::BFloat16 != float\n",
    "            loss = model(x, y)\n",
    "        train_loss = loss.detach()\n",
    "        loss = loss / grad_accum_steps # seems import to understand, but n/a here since grad_accum_steps is 1, see his comment\n",
    "        loss.backward()\n",
    "        x, y = next(train_loader)\n",
    "    # gradient clipping\n",
    "    if grad_clip > 0.0:\n",
    "        torch.nn.utils.clip_grad_norm_(orig_model.parameters(), grad_clip) # check exactly what this does, it's not a simple cip\n",
    "    # step optimizers\n",
    "    lrm = get_lr_multiplier(step)\n",
    "    for opt in optimizers:\n",
    "        for group in opt.param_groups:\n",
    "            group[\"lr\"] = group[\"initial_lr\"] * lrm\n",
    "    muon_momentum = get_muon_momentum(step)\n",
    "    for group in muon_optimizer.param_groups:\n",
    "        group[\"momentum\"] = muon_momentum\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"step: {step}, loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bcbb02-8641-4811-8a07-fcee91f9013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(orig_model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c40ca0-4553-4a24-8ef3-f2ad01b329d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc1227-d8e2-4de0-9928-2bd46c7ba24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show top 3 next tokens for a few prompts\n",
    "for prompt in ['The person', 'He went to', '1 + 2 = ', 'first of', '3 cats and 2', 'mom and', 'the red', 'She']:\n",
    "    with autocast_ctx:\n",
    "        logits = orig_model(torch.tensor([tokenizer.encode(prompt)], device=device)).detach()\n",
    "    top_3_next_tokens = torch.topk(logits[0,-1,:], k=3).indices\n",
    "    print(f\"{prompt}{'|'.join([tokenizer.decode([token]) for token in top_3_next_tokens])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b890f9-4732-4789-8e58-4270c441bd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
