{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46b90203-4700-4fd4-bb8e-f12cfcad6654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../my_nanochat')\n",
    "from my_nanochat.my_tokenizer import MyTokenizer\n",
    "from my_nanochat.my_dataset import text_iterator\n",
    "from my_nanochat.my_common import get_base_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbfd7c3-958e-499e-a1d4-c72b7923084d",
   "metadata": {},
   "source": [
    "### train tokenizer\n",
    "\n",
    "I didn't write a script to do this yet so for now just train it right here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca31225-e385-4095-ae09-a6ba25b567e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Shipment & Transport-Sea, Air, Rail, Road, Pipelin\n",
      "1: 12. Definition â€” In this Part, unless the context \n",
      "2: GÃºthwinÃ« was the sword that belonged to Ã‰omer.\\nIt \n",
      "3: The robot in the picture above is called YOLO, whi\n",
      "4: Metal additive manufacturing (AM) is growing at a \n",
      "5: The region Bergisches Land is located in southern \n",
      "6: The investigation of past cultures of the modern n\n",
      "7: Agreement On Food Safety\\nInternational trade rules\n",
      "8: Many good novels in the past have had films produc\n",
      "9: In January we began a survey of the history of Ame\n"
     ]
    }
   ],
   "source": [
    "# confirm files are in the right place\n",
    "for i, doc in enumerate(text_iterator(max_chars=500, doc_cap=50)):\n",
    "    doc = doc.replace('\\n','\\\\n')\n",
    "    print(f\"{i}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3b9d7a3-8e76-4463-8a75-36867e5464ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffers filled: 1\n",
      "buffers filled: 2\n",
      "buffers filled: 3\n",
      "buffers filled: 4\n",
      "buffers filled: 5\n",
      "buffers filled: 6\n",
      "buffers filled: 7\n",
      "buffers filled: 8\n",
      "buffers filled: 9\n",
      "buffers filled: 10\n",
      "buffers filled: 11\n",
      "buffers filled: 12\n",
      "buffers filled: 13\n",
      "buffers filled: 14\n",
      "buffers filled: 15\n",
      "buffers filled: 16\n",
      "buffers filled: 17\n",
      "buffers filled: 18\n",
      "buffers filled: 19\n",
      "buffers filled: 20\n",
      "buffers filled: 21\n",
      "buffers filled: 22\n",
      "buffers filled: 23\n",
      "buffers filled: 24\n",
      "buffers filled: 25\n",
      "buffers filled: 26\n",
      "buffers filled: 27\n",
      "buffers filled: 28\n",
      "buffers filled: 29\n",
      "buffers filled: 30\n",
      "buffers filled: 31\n",
      "buffers filled: 32\n",
      "buffers filled: 33\n",
      "buffers filled: 34\n",
      "buffers filled: 35\n",
      "buffers filled: 36\n",
      "buffers filled: 37\n",
      "buffers filled: 38\n",
      "buffers filled: 39\n",
      "buffers filled: 40\n",
      "buffers filled: 41\n",
      "buffers filled: 42\n",
      "buffers filled: 43\n",
      "buffers filled: 44\n",
      "buffers filled: 45\n",
      "buffers filled: 46\n",
      "buffers filled: 47\n",
      "buffers filled: 48\n",
      "buffers filled: 49\n",
      "buffers filled: 50\n",
      "buffers filled: 51\n",
      "buffers filled: 52\n",
      "buffers filled: 53\n",
      "buffers filled: 54\n",
      "buffers filled: 55\n",
      "buffers filled: 56\n",
      "buffers filled: 57\n",
      "buffers filled: 58\n",
      "buffers filled: 59\n",
      "buffers filled: 60\n",
      "buffers filled: 61\n",
      "buffers filled: 62\n",
      "buffers filled: 63\n",
      "buffers filled: 64\n",
      "buffers filled: 65\n",
      "buffers filled: 66\n",
      "Merges done: 1000\n",
      "Merges done: 2000\n",
      "Merges done: 3000\n",
      "Merges done: 4000\n",
      "Merges done: 5000\n",
      "Merges done: 6000\n",
      "Merges done: 7000\n",
      "Merges done: 8000\n",
      "Merges done: 9000\n",
      "Merges done: 10000\n",
      "Merges done: 11000\n",
      "Merges done: 12000\n",
      "Merges done: 13000\n",
      "Merges done: 14000\n",
      "Merges done: 15000\n",
      "Merges done: 16000\n",
      "Merges done: 17000\n",
      "Merges done: 18000\n",
      "Merges done: 19000\n",
      "Merges done: 20000\n",
      "Merges done: 21000\n",
      "Merges done: 22000\n",
      "Merges done: 23000\n",
      "Merges done: 24000\n",
      "Merges done: 25000\n",
      "Merges done: 26000\n",
      "Merges done: 27000\n",
      "Merges done: 28000\n",
      "Merges done: 29000\n",
      "Merges done: 30000\n",
      "Merges done: 31000\n",
      "Merges done: 32000\n",
      "Merges done: 33000\n",
      "Merges done: 34000\n",
      "Merges done: 35000\n",
      "Merges done: 36000\n",
      "Merges done: 37000\n",
      "Merges done: 38000\n",
      "Merges done: 39000\n",
      "Merges done: 40000\n",
      "Merges done: 41000\n",
      "Merges done: 42000\n",
      "Merges done: 43000\n",
      "Merges done: 44000\n",
      "Merges done: 45000\n",
      "Merges done: 46000\n",
      "Merges done: 47000\n",
      "Merges done: 48000\n",
      "Merges done: 49000\n",
      "Merges done: 50000\n",
      "Merges done: 51000\n",
      "Merges done: 52000\n",
      "Merges done: 53000\n",
      "Merges done: 54000\n",
      "Merges done: 55000\n",
      "Merges done: 56000\n",
      "Merges done: 57000\n",
      "Merges done: 58000\n",
      "Merges done: 59000\n",
      "Merges done: 60000\n",
      "Merges done: 61000\n",
      "Merges done: 62000\n",
      "Merges done: 63000\n",
      "Merges done: 64000\n",
      "Merges done: 65000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MyTokenizer.train_from_iterator(text_iterator(max_chars=2_000_000_000), vocab_size=65536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b18045e-4cf2-4ace-a108-eb26c04d24c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65537"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.enc.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "256536ba-c0b4-499e-b570-63aa8852f790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1137, 309, 257, 1049, 46]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"This is a test.\"); tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0a577b5-0454-4c8c-98bb-40df1f3621b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2da5fb92-983d-452a-ac83-0d43b6516271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35947, 160, 65116, 189, 21354, 145, 139, 32, 1035, 51, 3744, 32, 3043, 46]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"ä½ å¥½ ðŸ‘‹ 123 + 45.\"); tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9c9233a-acab-4a26-ad02-0efd06c2d0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ å¥½ ðŸ‘‹ 123 + 45.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90fce8eb-72bc-4e67-9288-a9be62da2d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenizer to /home/paperspace/.cache/my_nanochat/my-tokenizer.pkl\n"
     ]
    }
   ],
   "source": [
    "tokenizer.save(os.path.join(get_base_dir(), \"my-tokenizer.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "782f5549-5a4e-4a89-86c0-19425c18b142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.1G\n",
      "-rw-rw-r-- 1 paperspace paperspace 827K Nov  1 12:26 my-tokenizer.pkl\n",
      "-rw-rw-r-- 1 paperspace paperspace  91M Nov  1 11:48 shard_00000.parquet\n",
      "-rw-rw-r-- 1 paperspace paperspace  90M Nov  1 11:48 shard_00001.parquet\n",
      "-rw-rw-r-- 1 paperspace paperspace  90M Nov  1 11:48 shard_00002.parquet\n",
      "-rw-rw-r-- 1 paperspace paperspace  90M Nov  1 11:48 shard_00003.parquet\n",
      "-rw-rw-r-- 1 paperspace paperspace  91M Nov  1 11:48 shard_00004.parquet\n",
      "-rw-rw-r-- 1 paperspace paperspace  90M Nov  1 11:48 shard_00005.parquet\n",
      "-rw-rw-r-- 1 paperspace paperspace  90M Nov  1 11:48 shard_00006.parquet\n",
      "-rw-rw-r-- 1 paperspace paperspace  90M Nov  1 11:48 shard_00007.parquet\n",
      "-rw-rw-r-- 1 paperspace paperspace  89M Nov  1 11:48 shard_00008.parquet\n",
      "-rw-rw-r-- 1 paperspace paperspace  90M Nov  1 11:48 shard_00009.parquet\n",
      "-rw-rw-r-- 1 paperspace paperspace  91M Nov  1 11:48 shard_00010.parquet\n",
      "-rw-rw-r-- 1 paperspace paperspace  91M Nov  1 11:48 shard_00011.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {get_base_dir()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
